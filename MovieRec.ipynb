{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "T_5w6XOxBuuf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wxhxbt\\Anaconda3\\envs\\tf11\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\wxhxbt\\Anaconda3\\envs\\tf11\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\wxhxbt\\Anaconda3\\envs\\tf11\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\wxhxbt\\Anaconda3\\envs\\tf11\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\wxhxbt\\Anaconda3\\envs\\tf11\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\wxhxbt\\Anaconda3\\envs\\tf11\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# import warnings\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from tensorflow.python.ops import math_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vvMwnJIQCDDe"
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "import hashlib\n",
    "from bert_serving.client import BertClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "n3DzshZA621y"
   },
   "outputs": [],
   "source": [
    "# bc = BertClient(ip='192.168.86.41')\n",
    "# bc.encode(['First do it', 'then do it right', 'then do it better'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fNpGdfcE_Mp"
   },
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "gk7VRNaVCpC0"
   },
   "outputs": [],
   "source": [
    "def _unzip(save_path, _, database_name, data_path):\n",
    "    \"\"\"\n",
    "    Unzip wrapper with the same interface as _ungzip\n",
    "    :param save_path: The path of the gzip files\n",
    "    :param database_name: Name of database\n",
    "    :param data_path: Path to extract to\n",
    "    :param _: HACK - Used to have to same interface as _ungzip\n",
    "    \"\"\"\n",
    "    print('Extracting {}...'.format(database_name))\n",
    "    with zipfile.ZipFile(save_path) as zf:\n",
    "        zf.extractall(data_path)\n",
    "\n",
    "def download_extract(database_name, data_path):\n",
    "    \"\"\"\n",
    "    Download and extract database\n",
    "    :param database_name: Database name\n",
    "    \"\"\"\n",
    "    DATASET_ML1M = 'ml-1m'\n",
    "\n",
    "    if database_name == DATASET_ML1M:\n",
    "        url = 'http://files.grouplens.org/datasets/movielens/ml-1m.zip'\n",
    "        hash_code = 'c4d9eecfca2ab87c1945afe126590906'\n",
    "        extract_path = os.path.join(data_path, 'ml-1m')\n",
    "        save_path = os.path.join(data_path, 'ml-1m.zip')\n",
    "        extract_fn = _unzip\n",
    "\n",
    "    if os.path.exists(extract_path):\n",
    "        print('Found {} Data'.format(database_name))\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        with DLProgress(unit='B', unit_scale=True, miniters=1, desc='Downloading {}'.format(database_name)) as pbar:\n",
    "            urlretrieve(\n",
    "                url,\n",
    "                save_path,\n",
    "                pbar.hook)\n",
    "\n",
    "    assert hashlib.md5(open(save_path, 'rb').read()).hexdigest() == hash_code, \\\n",
    "        '{} file is corrupted.  Remove the file and try again.'.format(save_path)\n",
    "\n",
    "    os.makedirs(extract_path)\n",
    "    try:\n",
    "        extract_fn(save_path, extract_path, database_name, data_path)\n",
    "    except Exception as err:\n",
    "        shutil.rmtree(extract_path)  # Remove extraction folder if there is an error\n",
    "        raise err\n",
    "\n",
    "    print('Done.')\n",
    "    # Remove compressed data\n",
    "#     os.remove(save_path)\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    \"\"\"\n",
    "    Handle Progress Bar while Downloading\n",
    "    \"\"\"\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        \"\"\"\n",
    "        A hook function that will be called once on establishment of the network connection and\n",
    "        once after each block read thereafter.\n",
    "        :param block_num: A count of blocks transferred so far\n",
    "        :param block_size: Block size in bytes\n",
    "        :param total_size: The total size of the file. This may be -1 on older FTP servers which do not return\n",
    "                            a file size in response to a retrieval request.\n",
    "        \"\"\"\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RnVpYF28CxtL",
    "outputId": "724d4e58-62a5-4c48-d531-4b79d36d7f24"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading ml-1m: 5.92MB [00:01, 5.65MB/s]                                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ml-1m...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "data_dir = './'\n",
    "download_extract('ml-1m', data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "gmzh6S3AC-YS",
    "outputId": "698fdd22-a32f-4a2c-e4b5-f3db9d7668a7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>OccupationID</th>\n",
       "      <th>Zip-code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>56</td>\n",
       "      <td>16</td>\n",
       "      <td>70072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>55117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>02460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>55455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID Gender  Age  OccupationID Zip-code\n",
       "0       1      F    1            10    48067\n",
       "1       2      M   56            16    70072\n",
       "2       3      M   25            15    55117\n",
       "3       4      M   45             7    02460\n",
       "4       5      M   25            20    55455"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_title = ['UserID', 'Gender', 'Age', 'OccupationID', 'Zip-code']\n",
    "users = pd.read_table('./ml-1m/users.dat', sep='::', header=None, names=users_title, engine = 'python')\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "723u-UiwDpPc",
    "outputId": "e33d32c6-7ad1-4220-cebe-ff929aeba8f2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Animation|Children's|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children's|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MovieID                               Title                        Genres\n",
       "0        1                    Toy Story (1995)   Animation|Children's|Comedy\n",
       "1        2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
       "2        3             Grumpier Old Men (1995)                Comedy|Romance\n",
       "3        4            Waiting to Exhale (1995)                  Comedy|Drama\n",
       "4        5  Father of the Bride Part II (1995)                        Comedy"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_title = ['MovieID', 'Title', 'Genres']\n",
    "movies = pd.read_table('./ml-1m/movies.dat', sep='::', header=None, names=movies_title, engine = 'python')\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "DFVtdqDeDr1S",
    "outputId": "7bdd2519-6727-4036-e235-66c1abfa759f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>timestamps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  MovieID  Rating  timestamps\n",
       "0       1     1193       5   978300760\n",
       "1       1      661       3   978302109\n",
       "2       1      914       3   978301968\n",
       "3       1     3408       4   978300275\n",
       "4       1     2355       5   978824291"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_title = ['UserID','MovieID', 'Rating', 'timestamps']\n",
    "ratings = pd.read_table('./ml-1m/ratings.dat', sep='::', header=None, names=ratings_title, engine = 'python')\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AlEEMKiMDuca"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JNOc9HRnE8Zc"
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load Dataset from File\n",
    "    \"\"\"\n",
    "    #读取User数据\n",
    "    users_title = ['UserID', 'Gender', 'Age', 'JobID', 'Zip-code']\n",
    "    users = pd.read_table('./ml-1m/users.dat', sep='::', header=None, names=users_title, engine = 'python')\n",
    "    users = users.filter(regex='UserID|Gender|Age|JobID')\n",
    "    users_orig = users.values\n",
    "    #改变User数据中性别和年龄\n",
    "    gender_map = {'F':0, 'M':1}\n",
    "    users['Gender'] = users['Gender'].map(gender_map)\n",
    "\n",
    "    age_map = {val:ii for ii,val in enumerate(set(users['Age']))}\n",
    "    users['Age'] = users['Age'].map(age_map)\n",
    "\n",
    "    #读取Movie数据集\n",
    "    movies_title = ['MovieID', 'Title', 'Genres']\n",
    "    movies = pd.read_table('./ml-1m/movies.dat', sep='::', header=None, names=movies_title, engine = 'python')\n",
    "    movies_orig = movies.values\n",
    "    #将Title中的年份去掉\n",
    "    pattern = re.compile(r'^(.*)\\((\\d+)\\)$')\n",
    "\n",
    "    title_map = {val:pattern.match(val).group(1) for ii,val in enumerate(set(movies['Title']))}\n",
    "    movies['Title'] = movies['Title'].map(title_map)\n",
    "\n",
    "    #电影类型转数字字典\n",
    "    genres_set = set()\n",
    "    for val in movies['Genres'].str.split('|'):\n",
    "        genres_set.update(val)\n",
    "\n",
    "    genres_set.add('<PAD>')\n",
    "    genres2int = {val:ii for ii, val in enumerate(genres_set)}\n",
    "\n",
    "    #将电影类型转成等长数字列表，长度是18\n",
    "    genres_map = {val:[genres2int[row] for row in val.split('|')] for ii,val in enumerate(set(movies['Genres']))}\n",
    "\n",
    "    for key in genres_map:\n",
    "        for cnt in range(max(genres2int.values()) - len(genres_map[key])):\n",
    "            genres_map[key].insert(len(genres_map[key]) + cnt,genres2int['<PAD>'])\n",
    "    \n",
    "    movies['Genres'] = movies['Genres'].map(genres_map)\n",
    "\n",
    "    #电影Title转数字字典\n",
    "    title_set = set()\n",
    "    for val in movies['Title'].str.split():\n",
    "        title_set.update(val)\n",
    "    \n",
    "    title_set.add('<PAD>')\n",
    "    title2int = {val:ii for ii, val in enumerate(title_set)}\n",
    "\n",
    "    #将电影Title转成等长数字列表，长度是15\n",
    "    title_count = 15\n",
    "    title_map = {val:[title2int[row] for row in val.split()] for ii,val in enumerate(set(movies['Title']))}\n",
    "    \n",
    "    for key in title_map:\n",
    "        for cnt in range(title_count - len(title_map[key])):\n",
    "            title_map[key].insert(len(title_map[key]) + cnt,title2int['<PAD>'])\n",
    "    \n",
    "    movies['Title'] = movies['Title'].map(title_map)\n",
    "\n",
    "    #读取评分数据集\n",
    "    ratings_title = ['UserID','MovieID', 'ratings', 'timestamps']\n",
    "    ratings = pd.read_table('./ml-1m/ratings.dat', sep='::', header=None, names=ratings_title, engine = 'python')\n",
    "    ratings = ratings.filter(regex='UserID|MovieID|ratings')\n",
    "\n",
    "    #合并三个表\n",
    "    data = pd.merge(pd.merge(ratings, users), movies)\n",
    "    \n",
    "    #将数据分成X和y两张表\n",
    "    target_fields = ['ratings']\n",
    "    features_pd, targets_pd = data.drop(target_fields, axis=1), data[target_fields]\n",
    "    \n",
    "    features = features_pd.values\n",
    "    targets_values = targets_pd.values\n",
    "    \n",
    "    return title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "QzjGYIt1ODN2"
   },
   "outputs": [],
   "source": [
    "title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig = load_data()\n",
    "\n",
    "pickle.dump((title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig), open('preprocess.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "BuLcPs1cOG2s",
    "outputId": "16652fc9-8ac9-4489-aa7b-bd56d0069254"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>JobID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  Gender  Age  JobID\n",
       "0       1       0    0     10\n",
       "1       2       1    5     16\n",
       "2       3       1    6     15\n",
       "3       4       1    2      7\n",
       "4       5       1    6     20"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "PKkyq_XNObyz",
    "outputId": "ad734e99-da1b-4c36-817c-a55048df4c4a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[3865, 1339, 640, 640, 640, 640, 640, 640, 640...</td>\n",
       "      <td>[13, 10, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[131, 640, 640, 640, 640, 640, 640, 640, 640, ...</td>\n",
       "      <td>[9, 10, 14, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[2306, 2443, 1393, 640, 640, 640, 640, 640, 64...</td>\n",
       "      <td>[2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[853, 2849, 4695, 640, 640, 640, 640, 640, 640...</td>\n",
       "      <td>[2, 16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[1116, 2921, 3604, 607, 4325, 3447, 640, 640, ...</td>\n",
       "      <td>[2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MovieID                                              Title  \\\n",
       "0        1  [3865, 1339, 640, 640, 640, 640, 640, 640, 640...   \n",
       "1        2  [131, 640, 640, 640, 640, 640, 640, 640, 640, ...   \n",
       "2        3  [2306, 2443, 1393, 640, 640, 640, 640, 640, 64...   \n",
       "3        4  [853, 2849, 4695, 640, 640, 640, 640, 640, 640...   \n",
       "4        5  [1116, 2921, 3604, 607, 4325, 3447, 640, 640, ...   \n",
       "\n",
       "                                              Genres  \n",
       "0  [13, 10, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...  \n",
       "1  [9, 10, 14, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...  \n",
       "2  [2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3  [2, 16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...  \n",
       "4  [2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "QUAkoboqOtyT"
   },
   "outputs": [],
   "source": [
    "title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig = pickle.load(open('preprocess.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ZoL5khmrOwK7"
   },
   "outputs": [],
   "source": [
    "def save_params(params):\n",
    "    \"\"\"\n",
    "    Save parameters to file\n",
    "    \"\"\"\n",
    "    pickle.dump(params, open('params.p', 'wb'))\n",
    "\n",
    "\n",
    "def load_params():\n",
    "    \"\"\"\n",
    "    Load parameters from file\n",
    "    \"\"\"\n",
    "    return pickle.load(open('params.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "iStTncQMO9DY"
   },
   "outputs": [],
   "source": [
    "#嵌入矩阵的维度\n",
    "embed_dim = 32\n",
    "#用户ID个数\n",
    "uid_max = max(features.take(0,1)) + 1 # 6040\n",
    "#性别个数\n",
    "gender_max = max(features.take(2,1)) + 1 # 1 + 1 = 2\n",
    "#年龄类别个数\n",
    "age_max = max(features.take(3,1)) + 1 # 6 + 1 = 7\n",
    "#职业个数\n",
    "job_max = max(features.take(4,1)) + 1# 20 + 1 = 21\n",
    "\n",
    "#电影ID个数\n",
    "movie_id_max = max(features.take(1,1)) + 1 # 3952\n",
    "#电影类型个数\n",
    "movie_categories_max = max(genres2int.values()) + 1 # 18 + 1 = 19\n",
    "#电影名单词个数\n",
    "movie_title_max = len(title_set) # 5216\n",
    "\n",
    "#对电影类型嵌入向量做加和操作的标志，考虑过使用mean做平均，但是没实现mean\n",
    "combiner = \"sum\"\n",
    "\n",
    "#电影名长度\n",
    "sentences_size = title_count # = 15\n",
    "#文本卷积滑动窗口，分别滑动2, 3, 4, 5个单词\n",
    "window_sizes = {2, 3, 4, 5}\n",
    "#文本卷积核数量\n",
    "filter_num = 8\n",
    "\n",
    "#电影ID转下标的字典，数据集中电影ID跟下标不一致，比如第5行的数据电影ID不一定是5\n",
    "movieid2idx = {val[0]:i for i, val in enumerate(movies.values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "PA4zKCV4PAK7"
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 5\n",
    "# Batch Size\n",
    "batch_size = 256\n",
    "\n",
    "dropout_keep = 0.5\n",
    "# Learning Rate\n",
    "learning_rate = 0.0001\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 20\n",
    "\n",
    "save_dir = './save/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ALEKmeH-PCYT"
   },
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    uid = tf.placeholder(tf.int32, [None, 1], name=\"uid\")\n",
    "    user_gender = tf.placeholder(tf.int32, [None, 1], name=\"user_gender\")\n",
    "    user_age = tf.placeholder(tf.int32, [None, 1], name=\"user_age\")\n",
    "    user_job = tf.placeholder(tf.int32, [None, 1], name=\"user_job\")\n",
    "    \n",
    "    movie_id = tf.placeholder(tf.int32, [None, 1], name=\"movie_id\")\n",
    "    movie_categories = tf.placeholder(tf.int32, [None, 18], name=\"movie_categories\")\n",
    "    movie_titles = tf.placeholder(tf.int32, [None, 15], name=\"movie_titles\")\n",
    "    targets = tf.placeholder(tf.int32, [None, 1], name=\"targets\")\n",
    "    LearningRate = tf.placeholder(tf.float32, name = \"LearningRate\")\n",
    "    dropout_keep_prob = tf.placeholder(tf.float32, name = \"dropout_keep_prob\")\n",
    "    return uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, LearningRate, dropout_keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Dn3V2lZjPFN8"
   },
   "outputs": [],
   "source": [
    "def get_user_embedding(uid, user_gender, user_age, user_job):\n",
    "    with tf.name_scope(\"user_embedding\"):\n",
    "        uid_embed_matrix = tf.Variable(tf.random_uniform([uid_max, embed_dim], -1, 1), name = \"uid_embed_matrix\")\n",
    "        uid_embed_layer = tf.nn.embedding_lookup(uid_embed_matrix, uid, name = \"uid_embed_layer\")\n",
    "    \n",
    "        gender_embed_matrix = tf.Variable(tf.random_uniform([gender_max, embed_dim // 2], -1, 1), name= \"gender_embed_matrix\")\n",
    "        gender_embed_layer = tf.nn.embedding_lookup(gender_embed_matrix, user_gender, name = \"gender_embed_layer\")\n",
    "        \n",
    "        age_embed_matrix = tf.Variable(tf.random_uniform([age_max, embed_dim // 2], -1, 1), name=\"age_embed_matrix\")\n",
    "        age_embed_layer = tf.nn.embedding_lookup(age_embed_matrix, user_age, name=\"age_embed_layer\")\n",
    "        \n",
    "        job_embed_matrix = tf.Variable(tf.random_uniform([job_max, embed_dim // 2], -1, 1), name = \"job_embed_matrix\")\n",
    "        job_embed_layer = tf.nn.embedding_lookup(job_embed_matrix, user_job, name = \"job_embed_layer\")\n",
    "    return uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "GkBJ_Iu7PKcL"
   },
   "outputs": [],
   "source": [
    "def get_user_feature_layer(uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer):\n",
    "    with tf.name_scope(\"user_fc\"):\n",
    "        #第一层全连接\n",
    "        uid_fc_layer = tf.layers.dense(uid_embed_layer, embed_dim, name = \"uid_fc_layer\", activation=tf.nn.relu)\n",
    "        gender_fc_layer = tf.layers.dense(gender_embed_layer, embed_dim, name = \"gender_fc_layer\", activation=tf.nn.relu)\n",
    "        age_fc_layer = tf.layers.dense(age_embed_layer, embed_dim, name =\"age_fc_layer\", activation=tf.nn.relu)\n",
    "        job_fc_layer = tf.layers.dense(job_embed_layer, embed_dim, name = \"job_fc_layer\", activation=tf.nn.relu)\n",
    "        \n",
    "        #第二层全连接\n",
    "        user_combine_layer = tf.concat([uid_fc_layer, gender_fc_layer, age_fc_layer, job_fc_layer], 2)  #(?, 1, 128)\n",
    "        user_combine_layer = tf.contrib.layers.fully_connected(user_combine_layer, 200, tf.tanh)  #(?, 1, 200)\n",
    "    \n",
    "        user_combine_layer_flat = tf.reshape(user_combine_layer, [-1, 200])\n",
    "    return user_combine_layer, user_combine_layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "SXOMi5faPNS8"
   },
   "outputs": [],
   "source": [
    "def get_movie_id_embed_layer(movie_id):\n",
    "    with tf.name_scope(\"movie_embedding\"):\n",
    "        movie_id_embed_matrix = tf.Variable(tf.random_uniform([movie_id_max, embed_dim], -1, 1), name = \"movie_id_embed_matrix\")\n",
    "        movie_id_embed_layer = tf.nn.embedding_lookup(movie_id_embed_matrix, movie_id, name = \"movie_id_embed_layer\")\n",
    "    return movie_id_embed_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "orh9gLNoPQI0"
   },
   "outputs": [],
   "source": [
    "def get_movie_categories_layers(movie_categories):\n",
    "    with tf.name_scope(\"movie_categories_layers\"):\n",
    "        movie_categories_embed_matrix = tf.Variable(tf.random_uniform([movie_categories_max, embed_dim], -1, 1), name = \"movie_categories_embed_matrix\")\n",
    "        movie_categories_embed_layer = tf.nn.embedding_lookup(movie_categories_embed_matrix, movie_categories, name = \"movie_categories_embed_layer\")\n",
    "        if combiner == \"sum\":\n",
    "            movie_categories_embed_layer = tf.reduce_sum(movie_categories_embed_layer, axis=1, keep_dims=True)\n",
    "    #     elif combiner == \"mean\":\n",
    "\n",
    "    return movie_categories_embed_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "_woxTamnPSqj"
   },
   "outputs": [],
   "source": [
    "def get_movie_cnn_layer(movie_titles):\n",
    "    #从嵌入矩阵中得到电影名对应的各个单词的嵌入向量\n",
    "    with tf.name_scope(\"movie_embedding\"):\n",
    "        movie_title_embed_matrix = tf.Variable(tf.random_uniform([movie_title_max, embed_dim], -1, 1), name = \"movie_title_embed_matrix\")\n",
    "        movie_title_embed_layer = tf.nn.embedding_lookup(movie_title_embed_matrix, movie_titles, name = \"movie_title_embed_layer\")\n",
    "        movie_title_embed_layer_expand = tf.expand_dims(movie_title_embed_layer, -1)\n",
    "    \n",
    "    #对文本嵌入层使用不同尺寸的卷积核做卷积和最大池化\n",
    "    pool_layer_lst = []\n",
    "    for window_size in window_sizes:\n",
    "        with tf.name_scope(\"movie_txt_conv_maxpool_{}\".format(window_size)):\n",
    "            filter_weights = tf.Variable(tf.truncated_normal([window_size, embed_dim, 1, filter_num],stddev=0.1),name = \"filter_weights\")\n",
    "            filter_bias = tf.Variable(tf.constant(0.1, shape=[filter_num]), name=\"filter_bias\")\n",
    "            \n",
    "            conv_layer = tf.nn.conv2d(movie_title_embed_layer_expand, filter_weights, [1,1,1,1], padding=\"VALID\", name=\"conv_layer\")\n",
    "            relu_layer = tf.nn.relu(tf.nn.bias_add(conv_layer,filter_bias), name =\"relu_layer\")\n",
    "            \n",
    "            maxpool_layer = tf.nn.max_pool(relu_layer, [1,sentences_size - window_size + 1 ,1,1], [1,1,1,1], padding=\"VALID\", name=\"maxpool_layer\")\n",
    "            pool_layer_lst.append(maxpool_layer)\n",
    "\n",
    "    #Dropout层\n",
    "    with tf.name_scope(\"pool_dropout\"):\n",
    "        pool_layer = tf.concat(pool_layer_lst, 3, name =\"pool_layer\")\n",
    "        max_num = len(window_sizes) * filter_num\n",
    "        pool_layer_flat = tf.reshape(pool_layer , [-1, 1, max_num], name = \"pool_layer_flat\")\n",
    "    \n",
    "        dropout_layer = tf.nn.dropout(pool_layer_flat, dropout_keep_prob, name = \"dropout_layer\")\n",
    "    return pool_layer_flat, dropout_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "wlKoHM--PVtU"
   },
   "outputs": [],
   "source": [
    "def get_movie_feature_layer(movie_id_embed_layer, movie_categories_embed_layer, dropout_layer):\n",
    "    with tf.name_scope(\"movie_fc\"):\n",
    "        #第一层全连接\n",
    "        movie_id_fc_layer = tf.layers.dense(movie_id_embed_layer, embed_dim, name = \"movie_id_fc_layer\", activation=tf.nn.relu)\n",
    "        movie_categories_fc_layer = tf.layers.dense(movie_categories_embed_layer, embed_dim, name = \"movie_categories_fc_layer\", activation=tf.nn.relu)\n",
    "    \n",
    "        #第二层全连接\n",
    "        movie_combine_layer = tf.concat([movie_id_fc_layer, movie_categories_fc_layer, dropout_layer], 2)  #(?, 1, 96)\n",
    "        movie_combine_layer = tf.contrib.layers.fully_connected(movie_combine_layer, 200, tf.tanh)  #(?, 1, 200)\n",
    "    \n",
    "        movie_combine_layer_flat = tf.reshape(movie_combine_layer, [-1, 200])\n",
    "    return movie_combine_layer, movie_combine_layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ffdh0S_PY7b",
    "outputId": "b9525f8a-437e-47e9-e9c2-4de91d0192bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-559a1ee9ce9e>:6: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    #获取输入占位符\n",
    "    uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob = get_inputs()\n",
    "    #获取User的4个嵌入向量\n",
    "    uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer = get_user_embedding(uid, user_gender, user_age, user_job)\n",
    "    #得到用户特征\n",
    "    user_combine_layer, user_combine_layer_flat = get_user_feature_layer(uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer)\n",
    "    #获取电影ID的嵌入向量\n",
    "    movie_id_embed_layer = get_movie_id_embed_layer(movie_id)\n",
    "    #获取电影类型的嵌入向量\n",
    "    movie_categories_embed_layer = get_movie_categories_layers(movie_categories)\n",
    "    #获取电影名的特征向量\n",
    "    pool_layer_flat, dropout_layer = get_movie_cnn_layer(movie_titles)\n",
    "    #得到电影特征\n",
    "    movie_combine_layer, movie_combine_layer_flat = get_movie_feature_layer(movie_id_embed_layer, \n",
    "                                                                                movie_categories_embed_layer, \n",
    "                                                                                dropout_layer)\n",
    "    #计算出评分，要注意两个不同的方案，inference的名字（name值）是不一样的，后面做推荐时要根据name取得tensor\n",
    "    with tf.name_scope(\"inference\"):\n",
    "        #将用户特征和电影特征作为输入，经过全连接，输出一个值的方案\n",
    "#         inference_layer = tf.concat([user_combine_layer_flat, movie_combine_layer_flat], 1)  #(?, 200)\n",
    "#         inference = tf.layers.dense(inference_layer, 1,\n",
    "#                                     kernel_initializer=tf.truncated_normal_initializer(stddev=0.01), \n",
    "#                                     kernel_regularizer=tf.nn.l2_loss, name=\"inference\")\n",
    "        #简单的将用户特征和电影特征做矩阵乘法得到一个预测评分\n",
    "#        inference = tf.matmul(user_combine_layer_flat, tf.transpose(movie_combine_layer_flat))\n",
    "        inference = tf.reduce_sum(user_combine_layer_flat * movie_combine_layer_flat, axis=1)\n",
    "        inference = tf.expand_dims(inference, axis=1)\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        # MSE损失，将计算值回归到评分\n",
    "        cost = tf.losses.mean_squared_error(targets, inference )\n",
    "        loss = tf.reduce_mean(cost)\n",
    "    # 优化损失 \n",
    "#     train_op = tf.train.AdamOptimizer(lr).minimize(loss)  #cost\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "    gradients = optimizer.compute_gradients(loss)  #cost\n",
    "    train_op = optimizer.apply_gradients(gradients, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "AXa_tQLYPe5D"
   },
   "outputs": [],
   "source": [
    "def get_batches(Xs, ys, batch_size):\n",
    "    for start in range(0, len(Xs), batch_size):\n",
    "        end = min(start + batch_size, len(Xs))\n",
    "        yield Xs[start:end], ys[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vzmEZAKSQ5oq",
    "outputId": "58ba3dfc-7bf1-4ea7-bca2-3f8b3257a106"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to C:\\Users\\wxhxbt\\jupyter\\MovieRec\\runs\\1619144047\n",
      "\n",
      "2021-04-22T22:14:12.358653: Epoch   0 Batch    0/3125   train_loss = 27.999\n",
      "2021-04-22T22:14:12.888357: Epoch   0 Batch   20/3125   train_loss = 5.675\n",
      "2021-04-22T22:14:13.415045: Epoch   0 Batch   40/3125   train_loss = 3.196\n",
      "2021-04-22T22:14:13.935744: Epoch   0 Batch   60/3125   train_loss = 2.356\n",
      "2021-04-22T22:14:14.452448: Epoch   0 Batch   80/3125   train_loss = 2.215\n",
      "2021-04-22T22:14:14.974146: Epoch   0 Batch  100/3125   train_loss = 1.997\n",
      "2021-04-22T22:14:15.505842: Epoch   0 Batch  120/3125   train_loss = 2.113\n",
      "2021-04-22T22:14:16.028539: Epoch   0 Batch  140/3125   train_loss = 2.084\n",
      "2021-04-22T22:14:16.562234: Epoch   0 Batch  160/3125   train_loss = 1.630\n",
      "2021-04-22T22:14:17.098923: Epoch   0 Batch  180/3125   train_loss = 1.746\n",
      "2021-04-22T22:14:17.624633: Epoch   0 Batch  200/3125   train_loss = 1.883\n",
      "2021-04-22T22:14:18.156328: Epoch   0 Batch  220/3125   train_loss = 1.606\n",
      "2021-04-22T22:14:18.691024: Epoch   0 Batch  240/3125   train_loss = 1.586\n",
      "2021-04-22T22:14:19.217704: Epoch   0 Batch  260/3125   train_loss = 1.586\n",
      "2021-04-22T22:14:19.743413: Epoch   0 Batch  280/3125   train_loss = 1.860\n",
      "2021-04-22T22:14:20.275093: Epoch   0 Batch  300/3125   train_loss = 1.665\n",
      "2021-04-22T22:14:20.804806: Epoch   0 Batch  320/3125   train_loss = 1.573\n",
      "2021-04-22T22:14:21.332486: Epoch   0 Batch  340/3125   train_loss = 1.425\n",
      "2021-04-22T22:14:21.863179: Epoch   0 Batch  360/3125   train_loss = 1.445\n",
      "2021-04-22T22:14:22.383880: Epoch   0 Batch  380/3125   train_loss = 1.479\n",
      "2021-04-22T22:14:22.915574: Epoch   0 Batch  400/3125   train_loss = 1.215\n",
      "2021-04-22T22:14:23.445268: Epoch   0 Batch  420/3125   train_loss = 1.310\n",
      "2021-04-22T22:14:23.981960: Epoch   0 Batch  440/3125   train_loss = 1.469\n",
      "2021-04-22T22:14:24.510655: Epoch   0 Batch  460/3125   train_loss = 1.461\n",
      "2021-04-22T22:14:25.039259: Epoch   0 Batch  480/3125   train_loss = 1.524\n",
      "2021-04-22T22:14:25.560959: Epoch   0 Batch  500/3125   train_loss = 1.136\n",
      "2021-04-22T22:14:26.088308: Epoch   0 Batch  520/3125   train_loss = 1.393\n",
      "2021-04-22T22:14:26.614015: Epoch   0 Batch  540/3125   train_loss = 1.352\n",
      "2021-04-22T22:14:27.165687: Epoch   0 Batch  560/3125   train_loss = 1.434\n",
      "2021-04-22T22:14:27.699950: Epoch   0 Batch  580/3125   train_loss = 1.585\n",
      "2021-04-22T22:14:28.235642: Epoch   0 Batch  600/3125   train_loss = 1.514\n",
      "2021-04-22T22:14:28.763336: Epoch   0 Batch  620/3125   train_loss = 1.443\n",
      "2021-04-22T22:14:29.283051: Epoch   0 Batch  640/3125   train_loss = 1.436\n",
      "2021-04-22T22:14:29.816740: Epoch   0 Batch  660/3125   train_loss = 1.291\n",
      "2021-04-22T22:14:30.335432: Epoch   0 Batch  680/3125   train_loss = 1.225\n",
      "2021-04-22T22:14:30.862129: Epoch   0 Batch  700/3125   train_loss = 1.378\n",
      "2021-04-22T22:14:31.383837: Epoch   0 Batch  720/3125   train_loss = 1.212\n",
      "2021-04-22T22:14:31.914541: Epoch   0 Batch  740/3125   train_loss = 1.262\n",
      "2021-04-22T22:14:32.441218: Epoch   0 Batch  760/3125   train_loss = 1.407\n",
      "2021-04-22T22:14:32.974920: Epoch   0 Batch  780/3125   train_loss = 1.405\n",
      "2021-04-22T22:14:33.492628: Epoch   0 Batch  800/3125   train_loss = 1.305\n",
      "2021-04-22T22:14:34.020319: Epoch   0 Batch  820/3125   train_loss = 1.290\n",
      "2021-04-22T22:14:34.544023: Epoch   0 Batch  840/3125   train_loss = 1.318\n",
      "2021-04-22T22:14:35.074704: Epoch   0 Batch  860/3125   train_loss = 1.146\n",
      "2021-04-22T22:14:35.603398: Epoch   0 Batch  880/3125   train_loss = 1.245\n",
      "2021-04-22T22:14:36.131105: Epoch   0 Batch  900/3125   train_loss = 1.312\n",
      "2021-04-22T22:14:36.657800: Epoch   0 Batch  920/3125   train_loss = 1.233\n",
      "2021-04-22T22:14:37.189498: Epoch   0 Batch  940/3125   train_loss = 1.387\n",
      "2021-04-22T22:14:37.718180: Epoch   0 Batch  960/3125   train_loss = 1.285\n",
      "2021-04-22T22:14:38.246875: Epoch   0 Batch  980/3125   train_loss = 1.367\n",
      "2021-04-22T22:14:38.771583: Epoch   0 Batch 1000/3125   train_loss = 1.371\n",
      "2021-04-22T22:14:39.299280: Epoch   0 Batch 1020/3125   train_loss = 1.348\n",
      "2021-04-22T22:14:39.822982: Epoch   0 Batch 1040/3125   train_loss = 1.220\n",
      "2021-04-22T22:14:40.348665: Epoch   0 Batch 1060/3125   train_loss = 1.436\n",
      "2021-04-22T22:14:40.875361: Epoch   0 Batch 1080/3125   train_loss = 1.271\n",
      "2021-04-22T22:14:41.401059: Epoch   0 Batch 1100/3125   train_loss = 1.294\n",
      "2021-04-22T22:14:41.925757: Epoch   0 Batch 1120/3125   train_loss = 1.335\n",
      "2021-04-22T22:14:42.460463: Epoch   0 Batch 1140/3125   train_loss = 1.382\n",
      "2021-04-22T22:14:42.987145: Epoch   0 Batch 1160/3125   train_loss = 1.294\n",
      "2021-04-22T22:14:43.510846: Epoch   0 Batch 1180/3125   train_loss = 1.246\n",
      "2021-04-22T22:14:44.036557: Epoch   0 Batch 1200/3125   train_loss = 1.257\n",
      "2021-04-22T22:14:44.566238: Epoch   0 Batch 1220/3125   train_loss = 1.169\n",
      "2021-04-22T22:14:45.092952: Epoch   0 Batch 1240/3125   train_loss = 1.214\n",
      "2021-04-22T22:14:45.615633: Epoch   0 Batch 1260/3125   train_loss = 1.302\n",
      "2021-04-22T22:14:46.144328: Epoch   0 Batch 1280/3125   train_loss = 1.258\n",
      "2021-04-22T22:14:46.682021: Epoch   0 Batch 1300/3125   train_loss = 1.224\n",
      "2021-04-22T22:14:47.205198: Epoch   0 Batch 1320/3125   train_loss = 1.186\n",
      "2021-04-22T22:14:47.737885: Epoch   0 Batch 1340/3125   train_loss = 1.103\n",
      "2021-04-22T22:14:48.262581: Epoch   0 Batch 1360/3125   train_loss = 1.194\n",
      "2021-04-22T22:14:48.793265: Epoch   0 Batch 1380/3125   train_loss = 1.013\n",
      "2021-04-22T22:14:49.313979: Epoch   0 Batch 1400/3125   train_loss = 1.316\n",
      "2021-04-22T22:14:49.841663: Epoch   0 Batch 1420/3125   train_loss = 1.233\n",
      "2021-04-22T22:14:50.369359: Epoch   0 Batch 1440/3125   train_loss = 1.211\n",
      "2021-04-22T22:14:50.895056: Epoch   0 Batch 1460/3125   train_loss = 1.232\n",
      "2021-04-22T22:14:51.419754: Epoch   0 Batch 1480/3125   train_loss = 1.261\n",
      "2021-04-22T22:14:51.947449: Epoch   0 Batch 1500/3125   train_loss = 1.371\n",
      "2021-04-22T22:14:52.474146: Epoch   0 Batch 1520/3125   train_loss = 1.262\n",
      "2021-04-22T22:14:52.996845: Epoch   0 Batch 1540/3125   train_loss = 1.270\n",
      "2021-04-22T22:14:53.523543: Epoch   0 Batch 1560/3125   train_loss = 1.194\n",
      "2021-04-22T22:14:54.049248: Epoch   0 Batch 1580/3125   train_loss = 1.238\n",
      "2021-04-22T22:14:54.572938: Epoch   0 Batch 1600/3125   train_loss = 1.248\n",
      "2021-04-22T22:14:55.102632: Epoch   0 Batch 1620/3125   train_loss = 1.205\n",
      "2021-04-22T22:14:55.626332: Epoch   0 Batch 1640/3125   train_loss = 1.327\n",
      "2021-04-22T22:14:56.152027: Epoch   0 Batch 1660/3125   train_loss = 1.240\n",
      "2021-04-22T22:14:56.674727: Epoch   0 Batch 1680/3125   train_loss = 1.236\n",
      "2021-04-22T22:14:57.199425: Epoch   0 Batch 1700/3125   train_loss = 1.145\n",
      "2021-04-22T22:14:57.723123: Epoch   0 Batch 1720/3125   train_loss = 1.231\n",
      "2021-04-22T22:14:58.254831: Epoch   0 Batch 1740/3125   train_loss = 1.222\n",
      "2021-04-22T22:14:58.772519: Epoch   0 Batch 1760/3125   train_loss = 1.314\n",
      "2021-04-22T22:14:59.301230: Epoch   0 Batch 1780/3125   train_loss = 1.087\n",
      "2021-04-22T22:14:59.825913: Epoch   0 Batch 1800/3125   train_loss = 1.161\n",
      "2021-04-22T22:15:00.354622: Epoch   0 Batch 1820/3125   train_loss = 1.155\n",
      "2021-04-22T22:15:00.880306: Epoch   0 Batch 1840/3125   train_loss = 1.321\n",
      "2021-04-22T22:15:01.407726: Epoch   0 Batch 1860/3125   train_loss = 1.264\n",
      "2021-04-22T22:15:01.931424: Epoch   0 Batch 1880/3125   train_loss = 1.300\n",
      "2021-04-22T22:15:02.461104: Epoch   0 Batch 1900/3125   train_loss = 1.005\n",
      "2021-04-22T22:15:02.993813: Epoch   0 Batch 1920/3125   train_loss = 1.232\n",
      "2021-04-22T22:15:03.533269: Epoch   0 Batch 1940/3125   train_loss = 1.147\n",
      "2021-04-22T22:15:04.062970: Epoch   0 Batch 1960/3125   train_loss = 1.172\n",
      "2021-04-22T22:15:04.593675: Epoch   0 Batch 1980/3125   train_loss = 1.127\n",
      "2021-04-22T22:15:05.117371: Epoch   0 Batch 2000/3125   train_loss = 1.432\n",
      "2021-04-22T22:15:05.641071: Epoch   0 Batch 2020/3125   train_loss = 1.219\n",
      "2021-04-22T22:15:06.171764: Epoch   0 Batch 2040/3125   train_loss = 1.151\n",
      "2021-04-22T22:15:06.697447: Epoch   0 Batch 2060/3125   train_loss = 1.004\n",
      "2021-04-22T22:15:07.227142: Epoch   0 Batch 2080/3125   train_loss = 1.370\n",
      "2021-04-22T22:15:08.912171: Epoch   0 Batch 2100/3125   train_loss = 1.228\n",
      "2021-04-22T22:15:09.445879: Epoch   0 Batch 2120/3125   train_loss = 1.124\n",
      "2021-04-22T22:15:09.968573: Epoch   0 Batch 2140/3125   train_loss = 1.228\n",
      "2021-04-22T22:15:10.493271: Epoch   0 Batch 2160/3125   train_loss = 1.168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-22T22:15:11.022956: Epoch   0 Batch 2180/3125   train_loss = 1.148\n",
      "2021-04-22T22:15:11.552661: Epoch   0 Batch 2200/3125   train_loss = 1.150\n",
      "2021-04-22T22:15:12.080022: Epoch   0 Batch 2220/3125   train_loss = 1.120\n",
      "2021-04-22T22:15:12.613715: Epoch   0 Batch 2240/3125   train_loss = 1.043\n",
      "2021-04-22T22:15:13.142411: Epoch   0 Batch 2260/3125   train_loss = 1.156\n",
      "2021-04-22T22:15:13.670107: Epoch   0 Batch 2280/3125   train_loss = 1.182\n",
      "2021-04-22T22:15:14.197575: Epoch   0 Batch 2300/3125   train_loss = 1.254\n",
      "2021-04-22T22:15:14.726272: Epoch   0 Batch 2320/3125   train_loss = 1.323\n",
      "2021-04-22T22:15:15.252969: Epoch   0 Batch 2340/3125   train_loss = 1.321\n",
      "2021-04-22T22:15:15.787661: Epoch   0 Batch 2360/3125   train_loss = 1.159\n",
      "2021-04-22T22:15:16.310360: Epoch   0 Batch 2380/3125   train_loss = 1.200\n",
      "2021-04-22T22:15:17.330775: Epoch   0 Batch 2400/3125   train_loss = 1.274\n",
      "2021-04-22T22:15:18.505095: Epoch   0 Batch 2420/3125   train_loss = 1.143\n",
      "2021-04-22T22:15:19.029793: Epoch   0 Batch 2440/3125   train_loss = 1.203\n",
      "2021-04-22T22:15:19.547495: Epoch   0 Batch 2460/3125   train_loss = 1.141\n",
      "2021-04-22T22:15:20.071193: Epoch   0 Batch 2480/3125   train_loss = 1.217\n",
      "2021-04-22T22:15:20.594892: Epoch   0 Batch 2500/3125   train_loss = 1.171\n",
      "2021-04-22T22:15:21.120590: Epoch   0 Batch 2520/3125   train_loss = 1.122\n",
      "2021-04-22T22:15:22.468817: Epoch   0 Batch 2540/3125   train_loss = 1.118\n",
      "2021-04-22T22:15:23.334316: Epoch   0 Batch 2560/3125   train_loss = 0.996\n",
      "2021-04-22T22:15:23.861010: Epoch   0 Batch 2580/3125   train_loss = 1.132\n",
      "2021-04-22T22:15:24.388708: Epoch   0 Batch 2600/3125   train_loss = 1.149\n",
      "2021-04-22T22:15:24.916417: Epoch   0 Batch 2620/3125   train_loss = 1.119\n",
      "2021-04-22T22:15:25.441115: Epoch   0 Batch 2640/3125   train_loss = 1.120\n",
      "2021-04-22T22:15:25.959804: Epoch   0 Batch 2660/3125   train_loss = 1.199\n",
      "2021-04-22T22:15:26.493496: Epoch   0 Batch 2680/3125   train_loss = 1.074\n",
      "2021-04-22T22:15:27.019356: Epoch   0 Batch 2700/3125   train_loss = 1.241\n",
      "2021-04-22T22:15:27.547068: Epoch   0 Batch 2720/3125   train_loss = 1.209\n",
      "2021-04-22T22:15:28.075747: Epoch   0 Batch 2740/3125   train_loss = 1.168\n",
      "2021-04-22T22:15:28.607458: Epoch   0 Batch 2760/3125   train_loss = 1.184\n",
      "2021-04-22T22:15:29.135152: Epoch   0 Batch 2780/3125   train_loss = 1.140\n",
      "2021-04-22T22:15:29.668845: Epoch   0 Batch 2800/3125   train_loss = 1.415\n",
      "2021-04-22T22:15:30.189545: Epoch   0 Batch 2820/3125   train_loss = 1.428\n",
      "2021-04-22T22:15:30.711245: Epoch   0 Batch 2840/3125   train_loss = 1.216\n",
      "2021-04-22T22:15:32.377287: Epoch   0 Batch 2860/3125   train_loss = 1.112\n",
      "2021-04-22T22:15:32.912977: Epoch   0 Batch 2880/3125   train_loss = 1.145\n",
      "2021-04-22T22:15:33.446670: Epoch   0 Batch 2900/3125   train_loss = 1.192\n",
      "2021-04-22T22:15:33.970353: Epoch   0 Batch 2920/3125   train_loss = 1.168\n",
      "2021-04-22T22:15:34.496066: Epoch   0 Batch 2940/3125   train_loss = 1.191\n",
      "2021-04-22T22:15:35.028746: Epoch   0 Batch 2960/3125   train_loss = 1.182\n",
      "2021-04-22T22:15:35.975201: Epoch   0 Batch 2980/3125   train_loss = 1.171\n",
      "2021-04-22T22:15:37.229477: Epoch   0 Batch 3000/3125   train_loss = 1.156\n",
      "2021-04-22T22:15:37.759187: Epoch   0 Batch 3020/3125   train_loss = 1.208\n",
      "2021-04-22T22:15:38.295862: Epoch   0 Batch 3040/3125   train_loss = 1.123\n",
      "2021-04-22T22:15:38.827185: Epoch   0 Batch 3060/3125   train_loss = 1.219\n",
      "2021-04-22T22:15:39.353894: Epoch   0 Batch 3080/3125   train_loss = 1.264\n",
      "2021-04-22T22:15:39.869584: Epoch   0 Batch 3100/3125   train_loss = 1.199\n",
      "2021-04-22T22:15:40.403291: Epoch   0 Batch 3120/3125   train_loss = 1.072\n",
      "2021-04-22T22:15:40.664141: Epoch   0 Batch    0/781   test_loss = 1.007\n",
      "2021-04-22T22:15:40.812042: Epoch   0 Batch   20/781   test_loss = 1.133\n",
      "2021-04-22T22:15:40.958957: Epoch   0 Batch   40/781   test_loss = 1.076\n",
      "2021-04-22T22:15:41.106871: Epoch   0 Batch   60/781   test_loss = 1.312\n",
      "2021-04-22T22:15:41.258785: Epoch   0 Batch   80/781   test_loss = 1.355\n",
      "2021-04-22T22:15:41.405699: Epoch   0 Batch  100/781   test_loss = 1.392\n",
      "2021-04-22T22:15:41.551815: Epoch   0 Batch  120/781   test_loss = 1.219\n",
      "2021-04-22T22:15:41.696732: Epoch   0 Batch  140/781   test_loss = 1.193\n",
      "2021-04-22T22:15:41.843663: Epoch   0 Batch  160/781   test_loss = 1.400\n",
      "2021-04-22T22:15:41.988563: Epoch   0 Batch  180/781   test_loss = 1.258\n",
      "2021-04-22T22:15:42.138477: Epoch   0 Batch  200/781   test_loss = 1.174\n",
      "2021-04-22T22:15:42.289404: Epoch   0 Batch  220/781   test_loss = 0.979\n",
      "2021-04-22T22:15:42.439304: Epoch   0 Batch  240/781   test_loss = 1.198\n",
      "2021-04-22T22:15:42.584220: Epoch   0 Batch  260/781   test_loss = 1.265\n",
      "2021-04-22T22:15:42.730146: Epoch   0 Batch  280/781   test_loss = 1.430\n",
      "2021-04-22T22:15:42.873378: Epoch   0 Batch  300/781   test_loss = 1.220\n",
      "2021-04-22T22:15:43.021305: Epoch   0 Batch  320/781   test_loss = 1.357\n",
      "2021-04-22T22:15:43.166209: Epoch   0 Batch  340/781   test_loss = 0.922\n",
      "2021-04-22T22:15:43.322118: Epoch   0 Batch  360/781   test_loss = 1.300\n",
      "2021-04-22T22:15:43.470034: Epoch   0 Batch  380/781   test_loss = 1.169\n",
      "2021-04-22T22:15:43.616949: Epoch   0 Batch  400/781   test_loss = 1.109\n",
      "2021-04-22T22:15:43.762359: Epoch   0 Batch  420/781   test_loss = 1.066\n",
      "2021-04-22T22:15:43.911265: Epoch   0 Batch  440/781   test_loss = 1.231\n",
      "2021-04-22T22:15:44.056180: Epoch   0 Batch  460/781   test_loss = 1.083\n",
      "2021-04-22T22:15:44.211092: Epoch   0 Batch  480/781   test_loss = 1.201\n",
      "2021-04-22T22:15:44.362003: Epoch   0 Batch  500/781   test_loss = 0.968\n",
      "2021-04-22T22:15:44.511932: Epoch   0 Batch  520/781   test_loss = 1.227\n",
      "2021-04-22T22:15:44.656835: Epoch   0 Batch  540/781   test_loss = 1.022\n",
      "2021-04-22T22:15:44.809747: Epoch   0 Batch  560/781   test_loss = 1.291\n",
      "2021-04-22T22:15:44.955672: Epoch   0 Batch  580/781   test_loss = 1.171\n",
      "2021-04-22T22:15:45.107574: Epoch   0 Batch  600/781   test_loss = 1.208\n",
      "2021-04-22T22:15:45.257488: Epoch   0 Batch  620/781   test_loss = 1.196\n",
      "2021-04-22T22:15:45.415398: Epoch   0 Batch  640/781   test_loss = 1.302\n",
      "2021-04-22T22:15:45.565312: Epoch   0 Batch  660/781   test_loss = 1.222\n",
      "2021-04-22T22:15:45.713226: Epoch   0 Batch  680/781   test_loss = 1.472\n",
      "2021-04-22T22:15:45.859141: Epoch   0 Batch  700/781   test_loss = 1.178\n",
      "2021-04-22T22:15:46.012053: Epoch   0 Batch  720/781   test_loss = 1.344\n",
      "2021-04-22T22:15:46.162968: Epoch   0 Batch  740/781   test_loss = 1.241\n",
      "2021-04-22T22:15:46.311882: Epoch   0 Batch  760/781   test_loss = 1.182\n",
      "2021-04-22T22:15:46.466805: Epoch   0 Batch  780/781   test_loss = 1.221\n",
      "2021-04-22T22:15:47.695099: Epoch   1 Batch   15/3125   train_loss = 1.294\n",
      "2021-04-22T22:15:48.219796: Epoch   1 Batch   35/3125   train_loss = 1.160\n",
      "2021-04-22T22:15:48.746479: Epoch   1 Batch   55/3125   train_loss = 1.297\n",
      "2021-04-22T22:15:49.270021: Epoch   1 Batch   75/3125   train_loss = 1.137\n",
      "2021-04-22T22:15:49.801704: Epoch   1 Batch   95/3125   train_loss = 1.009\n",
      "2021-04-22T22:15:50.455329: Epoch   1 Batch  115/3125   train_loss = 1.181\n",
      "2021-04-22T22:15:50.983024: Epoch   1 Batch  135/3125   train_loss = 1.007\n",
      "2021-04-22T22:15:51.506723: Epoch   1 Batch  155/3125   train_loss = 1.102\n",
      "2021-04-22T22:15:52.033434: Epoch   1 Batch  175/3125   train_loss = 1.061\n",
      "2021-04-22T22:15:52.561116: Epoch   1 Batch  195/3125   train_loss = 1.254\n",
      "2021-04-22T22:15:54.249159: Epoch   1 Batch  215/3125   train_loss = 1.098\n",
      "2021-04-22T22:15:54.773841: Epoch   1 Batch  235/3125   train_loss = 1.086\n",
      "2021-04-22T22:15:55.299538: Epoch   1 Batch  255/3125   train_loss = 1.329\n",
      "2021-04-22T22:15:55.830233: Epoch   1 Batch  275/3125   train_loss = 1.081\n",
      "2021-04-22T22:15:56.368923: Epoch   1 Batch  295/3125   train_loss = 1.014\n",
      "2021-04-22T22:15:56.897633: Epoch   1 Batch  315/3125   train_loss = 1.142\n",
      "2021-04-22T22:15:57.854069: Epoch   1 Batch  335/3125   train_loss = 1.020\n",
      "2021-04-22T22:15:59.097363: Epoch   1 Batch  355/3125   train_loss = 1.126\n",
      "2021-04-22T22:15:59.631059: Epoch   1 Batch  375/3125   train_loss = 1.212\n",
      "2021-04-22T22:16:00.187723: Epoch   1 Batch  395/3125   train_loss = 1.065\n",
      "2021-04-22T22:16:01.891746: Epoch   1 Batch  415/3125   train_loss = 1.218\n",
      "2021-04-22T22:16:02.459281: Epoch   1 Batch  435/3125   train_loss = 1.179\n",
      "2021-04-22T22:16:02.986991: Epoch   1 Batch  455/3125   train_loss = 1.126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-22T22:16:03.521684: Epoch   1 Batch  475/3125   train_loss = 1.182\n",
      "2021-04-22T22:16:04.058360: Epoch   1 Batch  495/3125   train_loss = 1.096\n",
      "2021-04-22T22:16:04.587056: Epoch   1 Batch  515/3125   train_loss = 1.228\n",
      "2021-04-22T22:16:05.112753: Epoch   1 Batch  535/3125   train_loss = 1.192\n",
      "2021-04-22T22:16:05.655441: Epoch   1 Batch  555/3125   train_loss = 1.191\n",
      "2021-04-22T22:16:06.181751: Epoch   1 Batch  575/3125   train_loss = 1.121\n",
      "2021-04-22T22:16:06.792388: Epoch   1 Batch  595/3125   train_loss = 1.301\n",
      "2021-04-22T22:16:08.419984: Epoch   1 Batch  615/3125   train_loss = 1.068\n",
      "2021-04-22T22:16:08.946695: Epoch   1 Batch  635/3125   train_loss = 1.187\n",
      "2021-04-22T22:16:09.474393: Epoch   1 Batch  655/3125   train_loss = 1.008\n",
      "2021-04-22T22:16:10.010069: Epoch   1 Batch  675/3125   train_loss = 0.894\n",
      "2021-04-22T22:16:10.543023: Epoch   1 Batch  695/3125   train_loss = 1.057\n",
      "2021-04-22T22:16:11.076716: Epoch   1 Batch  715/3125   train_loss = 1.096\n",
      "2021-04-22T22:16:12.273063: Epoch   1 Batch  735/3125   train_loss = 0.958\n",
      "2021-04-22T22:16:13.290439: Epoch   1 Batch  755/3125   train_loss = 1.104\n",
      "2021-04-22T22:16:13.813148: Epoch   1 Batch  775/3125   train_loss = 1.036\n",
      "2021-04-22T22:16:14.344835: Epoch   1 Batch  795/3125   train_loss = 1.246\n",
      "2021-04-22T22:16:14.874527: Epoch   1 Batch  815/3125   train_loss = 1.153\n",
      "2021-04-22T22:16:15.514159: Epoch   1 Batch  835/3125   train_loss = 1.128\n",
      "2021-04-22T22:16:16.046854: Epoch   1 Batch  855/3125   train_loss = 1.287\n",
      "2021-04-22T22:16:16.575549: Epoch   1 Batch  875/3125   train_loss = 1.214\n",
      "2021-04-22T22:16:17.104244: Epoch   1 Batch  895/3125   train_loss = 1.058\n",
      "2021-04-22T22:16:17.647946: Epoch   1 Batch  915/3125   train_loss = 1.182\n",
      "2021-04-22T22:16:18.179625: Epoch   1 Batch  935/3125   train_loss = 1.229\n",
      "2021-04-22T22:16:18.713331: Epoch   1 Batch  955/3125   train_loss = 1.183\n",
      "2021-04-22T22:16:19.406922: Epoch   1 Batch  975/3125   train_loss = 1.140\n",
      "2021-04-22T22:16:20.925053: Epoch   1 Batch  995/3125   train_loss = 0.934\n",
      "2021-04-22T22:16:21.454738: Epoch   1 Batch 1015/3125   train_loss = 1.136\n",
      "2021-04-22T22:16:21.988431: Epoch   1 Batch 1035/3125   train_loss = 1.144\n",
      "2021-04-22T22:16:22.515141: Epoch   1 Batch 1055/3125   train_loss = 1.095\n",
      "2021-04-22T22:16:23.047834: Epoch   1 Batch 1075/3125   train_loss = 1.063\n",
      "2021-04-22T22:16:23.582513: Epoch   1 Batch 1095/3125   train_loss = 1.118\n",
      "2021-04-22T22:16:24.793817: Epoch   1 Batch 1115/3125   train_loss = 1.142\n",
      "2021-04-22T22:16:25.791251: Epoch   1 Batch 1135/3125   train_loss = 1.043\n",
      "2021-04-22T22:16:26.320949: Epoch   1 Batch 1155/3125   train_loss = 1.134\n",
      "2021-04-22T22:16:26.854629: Epoch   1 Batch 1175/3125   train_loss = 1.193\n",
      "2021-04-22T22:16:28.471704: Epoch   1 Batch 1195/3125   train_loss = 1.249\n",
      "2021-04-22T22:16:29.060372: Epoch   1 Batch 1215/3125   train_loss = 1.029\n",
      "2021-04-22T22:16:29.599063: Epoch   1 Batch 1235/3125   train_loss = 1.122\n",
      "2021-04-22T22:16:30.159734: Epoch   1 Batch 1255/3125   train_loss = 1.050\n",
      "2021-04-22T22:16:31.827778: Epoch   1 Batch 1275/3125   train_loss = 1.025\n",
      "2021-04-22T22:16:32.356475: Epoch   1 Batch 1295/3125   train_loss = 1.152\n",
      "2021-04-22T22:16:32.884159: Epoch   1 Batch 1315/3125   train_loss = 1.190\n",
      "2021-04-22T22:16:33.423851: Epoch   1 Batch 1335/3125   train_loss = 1.059\n",
      "2021-04-22T22:16:33.956530: Epoch   1 Batch 1355/3125   train_loss = 1.034\n",
      "2021-04-22T22:16:34.481244: Epoch   1 Batch 1375/3125   train_loss = 1.127\n",
      "2021-04-22T22:16:35.415728: Epoch   1 Batch 1395/3125   train_loss = 1.057\n",
      "2021-04-22T22:16:36.688967: Epoch   1 Batch 1415/3125   train_loss = 1.144\n",
      "2021-04-22T22:16:37.220650: Epoch   1 Batch 1435/3125   train_loss = 1.261\n",
      "2021-04-22T22:16:37.745349: Epoch   1 Batch 1455/3125   train_loss = 1.247\n",
      "2021-04-22T22:16:39.007624: Epoch   1 Batch 1475/3125   train_loss = 1.191\n",
      "2021-04-22T22:16:39.949080: Epoch   1 Batch 1495/3125   train_loss = 1.032\n",
      "2021-04-22T22:16:40.475791: Epoch   1 Batch 1515/3125   train_loss = 1.011\n",
      "2021-04-22T22:16:41.008482: Epoch   1 Batch 1535/3125   train_loss = 0.883\n",
      "2021-04-22T22:16:41.540180: Epoch   1 Batch 1555/3125   train_loss = 1.091\n",
      "2021-04-22T22:16:42.063862: Epoch   1 Batch 1575/3125   train_loss = 1.050\n",
      "2021-04-22T22:16:42.594413: Epoch   1 Batch 1595/3125   train_loss = 1.188\n",
      "2021-04-22T22:16:44.211504: Epoch   1 Batch 1615/3125   train_loss = 1.133\n",
      "2021-04-22T22:16:44.807125: Epoch   1 Batch 1635/3125   train_loss = 1.061\n",
      "2021-04-22T22:16:45.335834: Epoch   1 Batch 1655/3125   train_loss = 1.195\n",
      "2021-04-22T22:16:45.873511: Epoch   1 Batch 1675/3125   train_loss = 1.027\n",
      "2021-04-22T22:16:46.419196: Epoch   1 Batch 1695/3125   train_loss = 1.073\n",
      "2021-04-22T22:16:46.946891: Epoch   1 Batch 1715/3125   train_loss = 1.069\n",
      "2021-04-22T22:16:47.638498: Epoch   1 Batch 1735/3125   train_loss = 1.242\n",
      "2021-04-22T22:16:49.180605: Epoch   1 Batch 1755/3125   train_loss = 1.001\n",
      "2021-04-22T22:16:49.712299: Epoch   1 Batch 1775/3125   train_loss = 1.117\n",
      "2021-04-22T22:16:50.240010: Epoch   1 Batch 1795/3125   train_loss = 1.103\n",
      "2021-04-22T22:16:50.762695: Epoch   1 Batch 1815/3125   train_loss = 1.048\n",
      "2021-04-22T22:16:51.288392: Epoch   1 Batch 1835/3125   train_loss = 1.188\n",
      "2021-04-22T22:16:51.822099: Epoch   1 Batch 1855/3125   train_loss = 1.042\n",
      "2021-04-22T22:16:53.001407: Epoch   1 Batch 1875/3125   train_loss = 1.198\n",
      "2021-04-22T22:16:54.029813: Epoch   1 Batch 1895/3125   train_loss = 1.006\n",
      "2021-04-22T22:16:54.554511: Epoch   1 Batch 1915/3125   train_loss = 0.879\n",
      "2021-04-22T22:16:55.084216: Epoch   1 Batch 1935/3125   train_loss = 1.038\n",
      "2021-04-22T22:16:55.614914: Epoch   1 Batch 1955/3125   train_loss = 0.984\n",
      "2021-04-22T22:16:56.153604: Epoch   1 Batch 1975/3125   train_loss = 1.125\n",
      "2021-04-22T22:16:56.698291: Epoch   1 Batch 1995/3125   train_loss = 1.202\n",
      "2021-04-22T22:16:58.402295: Epoch   1 Batch 2015/3125   train_loss = 1.203\n",
      "2021-04-22T22:16:58.931992: Epoch   1 Batch 2035/3125   train_loss = 1.160\n",
      "2021-04-22T22:16:59.465685: Epoch   1 Batch 2055/3125   train_loss = 1.082\n",
      "2021-04-22T22:17:00.128305: Epoch   1 Batch 2075/3125   train_loss = 1.162\n",
      "2021-04-22T22:17:01.690402: Epoch   1 Batch 2095/3125   train_loss = 1.057\n",
      "2021-04-22T22:17:02.213110: Epoch   1 Batch 2115/3125   train_loss = 1.122\n",
      "2021-04-22T22:17:02.745808: Epoch   1 Batch 2135/3125   train_loss = 1.019\n",
      "2021-04-22T22:17:03.287497: Epoch   1 Batch 2155/3125   train_loss = 1.045\n",
      "2021-04-22T22:17:03.822174: Epoch   1 Batch 2175/3125   train_loss = 1.085\n",
      "2021-04-22T22:17:04.355882: Epoch   1 Batch 2195/3125   train_loss = 1.097\n",
      "2021-04-22T22:17:05.508207: Epoch   1 Batch 2215/3125   train_loss = 1.062\n",
      "2021-04-22T22:17:06.577601: Epoch   1 Batch 2235/3125   train_loss = 1.198\n",
      "2021-04-22T22:17:07.105294: Epoch   1 Batch 2255/3125   train_loss = 1.152\n",
      "2021-04-22T22:17:07.637976: Epoch   1 Batch 2275/3125   train_loss = 0.926\n",
      "2021-04-22T22:17:08.175678: Epoch   1 Batch 2295/3125   train_loss = 1.185\n",
      "2021-04-22T22:17:08.702378: Epoch   1 Batch 2315/3125   train_loss = 1.166\n",
      "2021-04-22T22:17:09.236066: Epoch   1 Batch 2335/3125   train_loss = 1.037\n",
      "2021-04-22T22:17:10.844966: Epoch   1 Batch 2355/3125   train_loss = 1.067\n",
      "2021-04-22T22:17:11.435624: Epoch   1 Batch 2375/3125   train_loss = 1.300\n",
      "2021-04-22T22:17:11.956760: Epoch   1 Batch 2395/3125   train_loss = 1.030\n",
      "2021-04-22T22:17:12.498449: Epoch   1 Batch 2415/3125   train_loss = 1.159\n",
      "2021-04-22T22:17:13.027856: Epoch   1 Batch 2435/3125   train_loss = 0.985\n",
      "2021-04-22T22:17:13.560559: Epoch   1 Batch 2455/3125   train_loss = 1.153\n",
      "2021-04-22T22:17:14.177196: Epoch   1 Batch 2475/3125   train_loss = 0.989\n",
      "2021-04-22T22:17:15.767279: Epoch   1 Batch 2495/3125   train_loss = 1.022\n",
      "2021-04-22T22:17:16.292978: Epoch   1 Batch 2515/3125   train_loss = 1.091\n",
      "2021-04-22T22:17:16.816675: Epoch   1 Batch 2535/3125   train_loss = 1.125\n",
      "2021-04-22T22:17:17.793900: Epoch   1 Batch 2555/3125   train_loss = 0.922\n",
      "2021-04-22T22:17:19.041945: Epoch   1 Batch 2575/3125   train_loss = 0.965\n",
      "2021-04-22T22:17:19.562644: Epoch   1 Batch 2595/3125   train_loss = 0.989\n",
      "2021-04-22T22:17:20.087356: Epoch   1 Batch 2615/3125   train_loss = 1.159\n",
      "2021-04-22T22:17:20.620050: Epoch   1 Batch 2635/3125   train_loss = 1.027\n",
      "2021-04-22T22:17:21.145733: Epoch   1 Batch 2655/3125   train_loss = 1.112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-22T22:17:21.684201: Epoch   1 Batch 2675/3125   train_loss = 0.996\n",
      "2021-04-22T22:17:23.049403: Epoch   1 Batch 2695/3125   train_loss = 1.089\n",
      "2021-04-22T22:17:23.890932: Epoch   1 Batch 2715/3125   train_loss = 0.963\n",
      "2021-04-22T22:17:24.423372: Epoch   1 Batch 2735/3125   train_loss = 0.900\n",
      "2021-04-22T22:17:24.958868: Epoch   1 Batch 2755/3125   train_loss = 1.101\n",
      "2021-04-22T22:17:25.492547: Epoch   1 Batch 2775/3125   train_loss = 1.096\n",
      "2021-04-22T22:17:26.018259: Epoch   1 Batch 2795/3125   train_loss = 1.072\n",
      "2021-04-22T22:17:26.547939: Epoch   1 Batch 2815/3125   train_loss = 0.985\n",
      "2021-04-22T22:17:28.222975: Epoch   1 Batch 2835/3125   train_loss = 1.154\n",
      "2021-04-22T22:17:28.752684: Epoch   1 Batch 2855/3125   train_loss = 1.081\n",
      "2021-04-22T22:17:29.275370: Epoch   1 Batch 2875/3125   train_loss = 1.087\n",
      "2021-04-22T22:17:29.803075: Epoch   1 Batch 2895/3125   train_loss = 1.110\n",
      "2021-04-22T22:17:30.331318: Epoch   1 Batch 2915/3125   train_loss = 0.978\n",
      "2021-04-22T22:17:30.859030: Epoch   1 Batch 2935/3125   train_loss = 1.083\n",
      "2021-04-22T22:17:31.635568: Epoch   1 Batch 2955/3125   train_loss = 1.111\n",
      "2021-04-22T22:17:33.072738: Epoch   1 Batch 2975/3125   train_loss = 1.051\n",
      "2021-04-22T22:17:33.593454: Epoch   1 Batch 2995/3125   train_loss = 0.976\n",
      "2021-04-22T22:17:34.127134: Epoch   1 Batch 3015/3125   train_loss = 1.010\n",
      "2021-04-22T22:17:34.653842: Epoch   1 Batch 3035/3125   train_loss = 1.021\n",
      "2021-04-22T22:17:35.185523: Epoch   1 Batch 3055/3125   train_loss = 1.117\n",
      "2021-04-22T22:17:35.722225: Epoch   1 Batch 3075/3125   train_loss = 0.991\n",
      "2021-04-22T22:17:36.920526: Epoch   1 Batch 3095/3125   train_loss = 1.072\n",
      "2021-04-22T22:17:37.914952: Epoch   1 Batch 3115/3125   train_loss = 1.006\n",
      "2021-04-22T22:17:38.300729: Epoch   1 Batch   19/781   test_loss = 1.053\n",
      "2021-04-22T22:17:38.447646: Epoch   1 Batch   39/781   test_loss = 0.899\n",
      "2021-04-22T22:17:38.597557: Epoch   1 Batch   59/781   test_loss = 0.941\n",
      "2021-04-22T22:17:38.743473: Epoch   1 Batch   79/781   test_loss = 1.063\n",
      "2021-04-22T22:17:38.890388: Epoch   1 Batch   99/781   test_loss = 1.035\n",
      "2021-04-22T22:17:39.035315: Epoch   1 Batch  119/781   test_loss = 0.953\n",
      "2021-04-22T22:17:39.183220: Epoch   1 Batch  139/781   test_loss = 1.087\n",
      "2021-04-22T22:17:39.331135: Epoch   1 Batch  159/781   test_loss = 1.018\n",
      "2021-04-22T22:17:39.480060: Epoch   1 Batch  179/781   test_loss = 0.958\n",
      "2021-04-22T22:17:39.624965: Epoch   1 Batch  199/781   test_loss = 0.906\n",
      "2021-04-22T22:17:39.769882: Epoch   1 Batch  219/781   test_loss = 1.050\n",
      "2021-04-22T22:17:39.915296: Epoch   1 Batch  239/781   test_loss = 1.143\n",
      "2021-04-22T22:17:40.065208: Epoch   1 Batch  259/781   test_loss = 1.016\n",
      "2021-04-22T22:17:40.212124: Epoch   1 Batch  279/781   test_loss = 1.063\n",
      "2021-04-22T22:17:40.361048: Epoch   1 Batch  299/781   test_loss = 1.243\n",
      "2021-04-22T22:17:40.504955: Epoch   1 Batch  319/781   test_loss = 1.037\n",
      "2021-04-22T22:17:40.652870: Epoch   1 Batch  339/781   test_loss = 0.899\n",
      "2021-04-22T22:17:40.796422: Epoch   1 Batch  359/781   test_loss = 0.956\n",
      "2021-04-22T22:17:40.947331: Epoch   1 Batch  379/781   test_loss = 1.025\n",
      "2021-04-22T22:17:41.096936: Epoch   1 Batch  399/781   test_loss = 0.839\n",
      "2021-04-22T22:17:41.243852: Epoch   1 Batch  419/781   test_loss = 0.955\n",
      "2021-04-22T22:17:41.392775: Epoch   1 Batch  439/781   test_loss = 1.024\n",
      "2021-04-22T22:17:41.543688: Epoch   1 Batch  459/781   test_loss = 1.070\n",
      "2021-04-22T22:17:41.687605: Epoch   1 Batch  479/781   test_loss = 1.042\n",
      "2021-04-22T22:17:41.832511: Epoch   1 Batch  499/781   test_loss = 0.951\n",
      "2021-04-22T22:17:41.980823: Epoch   1 Batch  519/781   test_loss = 1.097\n",
      "2021-04-22T22:17:42.129737: Epoch   1 Batch  539/781   test_loss = 0.872\n",
      "2021-04-22T22:17:42.274656: Epoch   1 Batch  559/781   test_loss = 1.205\n",
      "2021-04-22T22:17:42.425582: Epoch   1 Batch  579/781   test_loss = 1.020\n",
      "2021-04-22T22:17:42.575491: Epoch   1 Batch  599/781   test_loss = 1.022\n",
      "2021-04-22T22:17:42.722412: Epoch   1 Batch  619/781   test_loss = 1.203\n",
      "2021-04-22T22:17:42.867312: Epoch   1 Batch  639/781   test_loss = 0.908\n",
      "2021-04-22T22:17:43.015242: Epoch   1 Batch  659/781   test_loss = 1.151\n",
      "2021-04-22T22:17:43.165154: Epoch   1 Batch  679/781   test_loss = 1.170\n",
      "2021-04-22T22:17:43.313056: Epoch   1 Batch  699/781   test_loss = 0.874\n",
      "2021-04-22T22:17:43.470897: Epoch   1 Batch  719/781   test_loss = 1.028\n",
      "2021-04-22T22:17:43.617812: Epoch   1 Batch  739/781   test_loss = 0.957\n",
      "2021-04-22T22:17:43.765729: Epoch   1 Batch  759/781   test_loss = 0.954\n",
      "2021-04-22T22:17:43.910645: Epoch   1 Batch  779/781   test_loss = 0.827\n",
      "2021-04-22T22:17:45.005930: Epoch   2 Batch   10/3125   train_loss = 0.996\n",
      "2021-04-22T22:17:45.530626: Epoch   2 Batch   30/3125   train_loss = 1.056\n",
      "2021-04-22T22:17:46.060331: Epoch   2 Batch   50/3125   train_loss = 1.055\n",
      "2021-04-22T22:17:46.593014: Epoch   2 Batch   70/3125   train_loss = 1.108\n",
      "2021-04-22T22:17:47.648408: Epoch   2 Batch   90/3125   train_loss = 1.052\n",
      "2021-04-22T22:17:48.876699: Epoch   2 Batch  110/3125   train_loss = 0.973\n",
      "2021-04-22T22:17:49.437819: Epoch   2 Batch  130/3125   train_loss = 1.021\n",
      "2021-04-22T22:17:50.011486: Epoch   2 Batch  150/3125   train_loss = 1.156\n",
      "2021-04-22T22:17:51.705461: Epoch   2 Batch  170/3125   train_loss = 1.062\n",
      "2021-04-22T22:17:52.233157: Epoch   2 Batch  190/3125   train_loss = 1.123\n",
      "2021-04-22T22:17:52.763851: Epoch   2 Batch  210/3125   train_loss = 0.935\n",
      "2021-04-22T22:17:53.639965: Epoch   2 Batch  230/3125   train_loss = 1.103\n",
      "2021-04-22T22:17:55.016182: Epoch   2 Batch  250/3125   train_loss = 0.969\n",
      "2021-04-22T22:17:55.571852: Epoch   2 Batch  270/3125   train_loss = 0.907\n",
      "2021-04-22T22:17:56.127546: Epoch   2 Batch  290/3125   train_loss = 1.012\n",
      "2021-04-22T22:17:57.632670: Epoch   2 Batch  310/3125   train_loss = 0.966\n",
      "2021-04-22T22:17:58.400218: Epoch   2 Batch  330/3125   train_loss = 1.042\n",
      "2021-04-22T22:17:58.958899: Epoch   2 Batch  350/3125   train_loss = 0.966\n",
      "2021-04-22T22:17:59.510432: Epoch   2 Batch  370/3125   train_loss = 1.175\n",
      "2021-04-22T22:18:01.238437: Epoch   2 Batch  390/3125   train_loss = 1.231\n",
      "2021-04-22T22:18:01.761136: Epoch   2 Batch  410/3125   train_loss = 0.946\n",
      "2021-04-22T22:18:02.305823: Epoch   2 Batch  430/3125   train_loss = 1.169\n",
      "2021-04-22T22:18:03.302258: Epoch   2 Batch  450/3125   train_loss = 1.014\n",
      "2021-04-22T22:18:04.542517: Epoch   2 Batch  470/3125   train_loss = 0.935\n",
      "2021-04-22T22:18:05.080207: Epoch   2 Batch  490/3125   train_loss = 1.007\n",
      "2021-04-22T22:18:05.600908: Epoch   2 Batch  510/3125   train_loss = 1.146\n",
      "2021-04-22T22:18:06.897163: Epoch   2 Batch  530/3125   train_loss = 1.008\n",
      "2021-04-22T22:18:07.817641: Epoch   2 Batch  550/3125   train_loss = 1.041\n",
      "2021-04-22T22:18:08.374312: Epoch   2 Batch  570/3125   train_loss = 1.106\n",
      "2021-04-22T22:18:08.903843: Epoch   2 Batch  590/3125   train_loss = 1.075\n",
      "2021-04-22T22:18:10.608860: Epoch   2 Batch  610/3125   train_loss = 0.966\n",
      "2021-04-22T22:18:11.184530: Epoch   2 Batch  630/3125   train_loss = 1.055\n",
      "2021-04-22T22:18:11.859140: Epoch   2 Batch  650/3125   train_loss = 1.057\n",
      "2021-04-22T22:18:13.355283: Epoch   2 Batch  670/3125   train_loss = 0.914\n",
      "2021-04-22T22:18:14.190811: Epoch   2 Batch  690/3125   train_loss = 0.978\n",
      "2021-04-22T22:18:14.720493: Epoch   2 Batch  710/3125   train_loss = 0.892\n",
      "2021-04-22T22:18:15.251186: Epoch   2 Batch  730/3125   train_loss = 0.884\n",
      "2021-04-22T22:18:15.791877: Epoch   2 Batch  750/3125   train_loss = 0.972\n",
      "2021-04-22T22:18:16.330579: Epoch   2 Batch  770/3125   train_loss = 0.920\n",
      "2021-04-22T22:18:16.855277: Epoch   2 Batch  790/3125   train_loss = 0.909\n",
      "2021-04-22T22:18:18.541292: Epoch   2 Batch  810/3125   train_loss = 0.862\n",
      "2021-04-22T22:18:19.077001: Epoch   2 Batch  830/3125   train_loss = 0.800\n",
      "2021-04-22T22:18:19.610676: Epoch   2 Batch  850/3125   train_loss = 1.061\n",
      "2021-04-22T22:18:20.288296: Epoch   2 Batch  870/3125   train_loss = 0.904\n",
      "2021-04-22T22:18:21.814407: Epoch   2 Batch  890/3125   train_loss = 0.858\n",
      "2021-04-22T22:18:22.338106: Epoch   2 Batch  910/3125   train_loss = 0.995\n",
      "2021-04-22T22:18:22.903780: Epoch   2 Batch  930/3125   train_loss = 1.031\n",
      "2021-04-22T22:18:24.714740: Epoch   2 Batch  950/3125   train_loss = 0.933\n",
      "2021-04-22T22:18:25.359367: Epoch   2 Batch  970/3125   train_loss = 1.073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-22T22:18:26.009993: Epoch   2 Batch  990/3125   train_loss = 0.888\n",
      "2021-04-22T22:18:27.215299: Epoch   2 Batch 1010/3125   train_loss = 1.184\n",
      "2021-04-22T22:18:28.362635: Epoch   2 Batch 1030/3125   train_loss = 0.979\n",
      "2021-04-22T22:18:28.951299: Epoch   2 Batch 1050/3125   train_loss = 0.950\n",
      "2021-04-22T22:18:29.491986: Epoch   2 Batch 1070/3125   train_loss = 1.020\n",
      "2021-04-22T22:18:31.192008: Epoch   2 Batch 1090/3125   train_loss = 1.149\n",
      "2021-04-22T22:18:31.722712: Epoch   2 Batch 1110/3125   train_loss = 1.082\n",
      "2021-04-22T22:18:32.254404: Epoch   2 Batch 1130/3125   train_loss = 0.997\n",
      "2021-04-22T22:18:33.022962: Epoch   2 Batch 1150/3125   train_loss = 0.986\n",
      "2021-04-22T22:18:34.503114: Epoch   2 Batch 1170/3125   train_loss = 0.944\n",
      "2021-04-22T22:18:35.032814: Epoch   2 Batch 1190/3125   train_loss = 1.042\n",
      "2021-04-22T22:18:35.562504: Epoch   2 Batch 1210/3125   train_loss = 0.931\n",
      "2021-04-22T22:18:36.102181: Epoch   2 Batch 1230/3125   train_loss = 0.876\n",
      "2021-04-22T22:18:36.648866: Epoch   2 Batch 1250/3125   train_loss = 0.938\n",
      "2021-04-22T22:18:37.230530: Epoch   2 Batch 1270/3125   train_loss = 0.995\n",
      "2021-04-22T22:18:38.728668: Epoch   2 Batch 1290/3125   train_loss = 0.973\n",
      "2021-04-22T22:18:39.503222: Epoch   2 Batch 1310/3125   train_loss = 0.989\n",
      "2021-04-22T22:18:40.078890: Epoch   2 Batch 1330/3125   train_loss = 1.082\n",
      "2021-04-22T22:18:40.607585: Epoch   2 Batch 1350/3125   train_loss = 0.947\n",
      "2021-04-22T22:18:42.289616: Epoch   2 Batch 1370/3125   train_loss = 0.859\n",
      "2021-04-22T22:18:42.813315: Epoch   2 Batch 1390/3125   train_loss = 1.062\n",
      "2021-04-22T22:18:43.341010: Epoch   2 Batch 1410/3125   train_loss = 1.035\n",
      "2021-04-22T22:18:44.223514: Epoch   2 Batch 1430/3125   train_loss = 1.011\n",
      "2021-04-22T22:18:45.567729: Epoch   2 Batch 1450/3125   train_loss = 1.025\n",
      "2021-04-22T22:18:46.098424: Epoch   2 Batch 1470/3125   train_loss = 1.016\n",
      "2021-04-22T22:18:46.640125: Epoch   2 Batch 1490/3125   train_loss = 1.049\n",
      "2021-04-22T22:18:47.957361: Epoch   2 Batch 1510/3125   train_loss = 0.954\n",
      "2021-04-22T22:18:48.885828: Epoch   2 Batch 1530/3125   train_loss = 1.093\n",
      "2021-04-22T22:18:49.410517: Epoch   2 Batch 1550/3125   train_loss = 0.976\n",
      "2021-04-22T22:18:49.949205: Epoch   2 Batch 1570/3125   train_loss = 0.936\n",
      "2021-04-22T22:18:50.495905: Epoch   2 Batch 1590/3125   train_loss = 0.960\n",
      "2021-04-22T22:18:51.039578: Epoch   2 Batch 1610/3125   train_loss = 1.008\n",
      "2021-04-22T22:18:51.575280: Epoch   2 Batch 1630/3125   train_loss = 1.050\n",
      "2021-04-22T22:18:53.262298: Epoch   2 Batch 1650/3125   train_loss = 0.928\n",
      "2021-04-22T22:18:53.779000: Epoch   2 Batch 1670/3125   train_loss = 0.835\n",
      "2021-04-22T22:18:54.301700: Epoch   2 Batch 1690/3125   train_loss = 1.073\n",
      "2021-04-22T22:18:54.896361: Epoch   2 Batch 1710/3125   train_loss = 1.082\n",
      "2021-04-22T22:18:56.496435: Epoch   2 Batch 1730/3125   train_loss = 1.005\n",
      "2021-04-22T22:18:57.021134: Epoch   2 Batch 1750/3125   train_loss = 0.815\n",
      "2021-04-22T22:18:57.548843: Epoch   2 Batch 1770/3125   train_loss = 1.105\n",
      "2021-04-22T22:18:58.490289: Epoch   2 Batch 1790/3125   train_loss = 1.017\n",
      "2021-04-22T22:18:59.765568: Epoch   2 Batch 1810/3125   train_loss = 1.046\n",
      "2021-04-22T22:19:00.293264: Epoch   2 Batch 1830/3125   train_loss = 1.003\n",
      "2021-04-22T22:19:00.825941: Epoch   2 Batch 1850/3125   train_loss = 0.938\n",
      "2021-04-22T22:19:02.064233: Epoch   2 Batch 1870/3125   train_loss = 1.028\n",
      "2021-04-22T22:19:03.040964: Epoch   2 Batch 1890/3125   train_loss = 0.861\n",
      "2021-04-22T22:19:03.568231: Epoch   2 Batch 1910/3125   train_loss = 0.920\n",
      "2021-04-22T22:19:04.107904: Epoch   2 Batch 1930/3125   train_loss = 0.986\n",
      "2021-04-22T22:19:05.651018: Epoch   2 Batch 1950/3125   train_loss = 0.919\n",
      "2021-04-22T22:19:06.324184: Epoch   2 Batch 1970/3125   train_loss = 0.993\n",
      "2021-04-22T22:19:06.851875: Epoch   2 Batch 1990/3125   train_loss = 0.868\n",
      "2021-04-22T22:19:07.379579: Epoch   2 Batch 2010/3125   train_loss = 0.823\n",
      "2021-04-22T22:19:09.075585: Epoch   2 Batch 2030/3125   train_loss = 0.977\n",
      "2021-04-22T22:19:09.603296: Epoch   2 Batch 2050/3125   train_loss = 0.933\n",
      "2021-04-22T22:19:10.128978: Epoch   2 Batch 2070/3125   train_loss = 0.937\n",
      "2021-04-22T22:19:10.842573: Epoch   2 Batch 2090/3125   train_loss = 0.839\n",
      "2021-04-22T22:19:12.356695: Epoch   2 Batch 2110/3125   train_loss = 1.108\n",
      "2021-04-22T22:19:12.886405: Epoch   2 Batch 2130/3125   train_loss = 1.021\n",
      "2021-04-22T22:19:13.418099: Epoch   2 Batch 2150/3125   train_loss = 1.001\n",
      "2021-04-22T22:19:14.459488: Epoch   2 Batch 2170/3125   train_loss = 0.901\n",
      "2021-04-22T22:19:15.641819: Epoch   2 Batch 2190/3125   train_loss = 0.986\n",
      "2021-04-22T22:19:16.171509: Epoch   2 Batch 2210/3125   train_loss = 1.016\n",
      "2021-04-22T22:19:16.698195: Epoch   2 Batch 2230/3125   train_loss = 0.870\n",
      "2021-04-22T22:19:18.085401: Epoch   2 Batch 2250/3125   train_loss = 1.091\n",
      "2021-04-22T22:19:18.926923: Epoch   2 Batch 2270/3125   train_loss = 0.971\n",
      "2021-04-22T22:19:19.455609: Epoch   2 Batch 2290/3125   train_loss = 0.870\n",
      "2021-04-22T22:19:19.988218: Epoch   2 Batch 2310/3125   train_loss = 0.898\n",
      "2021-04-22T22:19:20.525907: Epoch   2 Batch 2330/3125   train_loss = 1.029\n",
      "2021-04-22T22:19:21.055588: Epoch   2 Batch 2350/3125   train_loss = 1.008\n",
      "2021-04-22T22:19:21.593279: Epoch   2 Batch 2370/3125   train_loss = 0.933\n",
      "2021-04-22T22:19:23.288303: Epoch   2 Batch 2390/3125   train_loss = 1.030\n",
      "2021-04-22T22:19:23.807018: Epoch   2 Batch 2410/3125   train_loss = 1.075\n",
      "2021-04-22T22:19:24.336698: Epoch   2 Batch 2430/3125   train_loss = 0.933\n",
      "2021-04-22T22:19:24.958343: Epoch   2 Batch 2450/3125   train_loss = 1.034\n",
      "2021-04-22T22:19:26.542443: Epoch   2 Batch 2470/3125   train_loss = 0.995\n",
      "2021-04-22T22:19:27.072138: Epoch   2 Batch 2490/3125   train_loss = 1.052\n",
      "2021-04-22T22:19:27.597821: Epoch   2 Batch 2510/3125   train_loss = 1.095\n",
      "2021-04-22T22:19:28.611239: Epoch   2 Batch 2530/3125   train_loss = 0.858\n",
      "2021-04-22T22:19:29.826319: Epoch   2 Batch 2550/3125   train_loss = 1.064\n",
      "2021-04-22T22:19:30.345020: Epoch   2 Batch 2570/3125   train_loss = 0.982\n",
      "2021-04-22T22:19:30.876713: Epoch   2 Batch 2590/3125   train_loss = 1.012\n",
      "2021-04-22T22:19:31.405918: Epoch   2 Batch 2610/3125   train_loss = 1.088\n",
      "2021-04-22T22:19:31.931602: Epoch   2 Batch 2630/3125   train_loss = 0.694\n",
      "2021-04-22T22:19:32.466307: Epoch   2 Batch 2650/3125   train_loss = 0.993\n",
      "2021-04-22T22:19:33.757587: Epoch   2 Batch 2670/3125   train_loss = 1.031\n",
      "2021-04-22T22:19:34.682032: Epoch   2 Batch 2690/3125   train_loss = 1.027\n",
      "2021-04-22T22:19:35.215725: Epoch   2 Batch 2710/3125   train_loss = 0.892\n",
      "2021-04-22T22:19:35.744406: Epoch   2 Batch 2730/3125   train_loss = 1.085\n",
      "2021-04-22T22:19:37.417447: Epoch   2 Batch 2750/3125   train_loss = 0.981\n",
      "2021-04-22T22:19:37.943154: Epoch   2 Batch 2770/3125   train_loss = 0.952\n",
      "2021-04-22T22:19:38.472849: Epoch   2 Batch 2790/3125   train_loss = 0.865\n",
      "2021-04-22T22:19:39.018521: Epoch   2 Batch 2810/3125   train_loss = 0.978\n",
      "2021-04-22T22:19:40.698054: Epoch   2 Batch 2830/3125   train_loss = 0.883\n",
      "2021-04-22T22:19:41.220768: Epoch   2 Batch 2850/3125   train_loss = 0.979\n",
      "2021-04-22T22:19:41.752461: Epoch   2 Batch 2870/3125   train_loss = 0.876\n",
      "2021-04-22T22:19:42.288137: Epoch   2 Batch 2890/3125   train_loss = 0.812\n",
      "2021-04-22T22:19:42.836831: Epoch   2 Batch 2910/3125   train_loss = 0.951\n",
      "2021-04-22T22:19:43.374520: Epoch   2 Batch 2930/3125   train_loss = 0.818\n",
      "2021-04-22T22:19:44.355952: Epoch   2 Batch 2950/3125   train_loss = 1.078\n",
      "2021-04-22T22:19:45.592234: Epoch   2 Batch 2970/3125   train_loss = 0.855\n",
      "2021-04-22T22:19:46.113946: Epoch   2 Batch 2990/3125   train_loss = 0.911\n",
      "2021-04-22T22:19:46.650626: Epoch   2 Batch 3010/3125   train_loss = 0.939\n",
      "2021-04-22T22:19:47.996852: Epoch   2 Batch 3030/3125   train_loss = 0.961\n",
      "2021-04-22T22:19:48.860820: Epoch   2 Batch 3050/3125   train_loss = 0.958\n",
      "2021-04-22T22:19:49.391840: Epoch   2 Batch 3070/3125   train_loss = 0.856\n",
      "2021-04-22T22:19:49.922530: Epoch   2 Batch 3090/3125   train_loss = 0.850\n",
      "2021-04-22T22:19:51.616544: Epoch   2 Batch 3110/3125   train_loss = 0.855\n",
      "2021-04-22T22:19:52.131878: Epoch   2 Batch   18/781   test_loss = 0.792\n",
      "2021-04-22T22:19:52.274793: Epoch   2 Batch   38/781   test_loss = 0.913\n",
      "2021-04-22T22:19:52.421710: Epoch   2 Batch   58/781   test_loss = 0.844\n",
      "2021-04-22T22:19:52.565626: Epoch   2 Batch   78/781   test_loss = 0.925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-22T22:19:52.718548: Epoch   2 Batch   98/781   test_loss = 0.979\n",
      "2021-04-22T22:19:52.863467: Epoch   2 Batch  118/781   test_loss = 0.854\n",
      "2021-04-22T22:19:53.010384: Epoch   2 Batch  138/781   test_loss = 0.979\n",
      "2021-04-22T22:19:53.159285: Epoch   2 Batch  158/781   test_loss = 0.878\n",
      "2021-04-22T22:19:53.305200: Epoch   2 Batch  178/781   test_loss = 0.832\n",
      "2021-04-22T22:19:53.452116: Epoch   2 Batch  198/781   test_loss = 0.965\n",
      "2021-04-22T22:19:53.600033: Epoch   2 Batch  218/781   test_loss = 1.066\n",
      "2021-04-22T22:19:53.748955: Epoch   2 Batch  238/781   test_loss = 0.967\n",
      "2021-04-22T22:19:53.893862: Epoch   2 Batch  258/781   test_loss = 0.998\n",
      "2021-04-22T22:19:54.040439: Epoch   2 Batch  278/781   test_loss = 1.065\n",
      "2021-04-22T22:19:54.188343: Epoch   2 Batch  298/781   test_loss = 0.892\n",
      "2021-04-22T22:19:54.337254: Epoch   2 Batch  318/781   test_loss = 0.877\n",
      "2021-04-22T22:19:54.486985: Epoch   2 Batch  338/781   test_loss = 0.922\n",
      "2021-04-22T22:19:54.631911: Epoch   2 Batch  358/781   test_loss = 0.911\n",
      "2021-04-22T22:19:54.783812: Epoch   2 Batch  378/781   test_loss = 0.879\n",
      "2021-04-22T22:19:54.927730: Epoch   2 Batch  398/781   test_loss = 0.826\n",
      "2021-04-22T22:19:55.077643: Epoch   2 Batch  418/781   test_loss = 0.985\n",
      "2021-04-22T22:19:55.223561: Epoch   2 Batch  438/781   test_loss = 1.037\n",
      "2021-04-22T22:19:55.371079: Epoch   2 Batch  458/781   test_loss = 0.932\n",
      "2021-04-22T22:19:55.516996: Epoch   2 Batch  478/781   test_loss = 0.930\n",
      "2021-04-22T22:19:55.665908: Epoch   2 Batch  498/781   test_loss = 0.831\n",
      "2021-04-22T22:19:55.817821: Epoch   2 Batch  518/781   test_loss = 0.921\n",
      "2021-04-22T22:19:55.967735: Epoch   2 Batch  538/781   test_loss = 0.809\n",
      "2021-04-22T22:19:56.118648: Epoch   2 Batch  558/781   test_loss = 0.895\n",
      "2021-04-22T22:19:56.265564: Epoch   2 Batch  578/781   test_loss = 0.961\n",
      "2021-04-22T22:19:56.414480: Epoch   2 Batch  598/781   test_loss = 1.038\n",
      "2021-04-22T22:19:56.567404: Epoch   2 Batch  618/781   test_loss = 0.868\n",
      "2021-04-22T22:19:56.716318: Epoch   2 Batch  638/781   test_loss = 0.919\n",
      "2021-04-22T22:19:56.865366: Epoch   2 Batch  658/781   test_loss = 0.988\n",
      "2021-04-22T22:19:57.014788: Epoch   2 Batch  678/781   test_loss = 0.949\n",
      "2021-04-22T22:19:57.160706: Epoch   2 Batch  698/781   test_loss = 0.950\n",
      "2021-04-22T22:19:57.306621: Epoch   2 Batch  718/781   test_loss = 1.036\n",
      "2021-04-22T22:19:57.455544: Epoch   2 Batch  738/781   test_loss = 0.892\n",
      "2021-04-22T22:19:57.600451: Epoch   2 Batch  758/781   test_loss = 0.958\n",
      "2021-04-22T22:19:57.748366: Epoch   2 Batch  778/781   test_loss = 0.930\n",
      "2021-04-22T22:19:58.725339: Epoch   3 Batch    5/3125   train_loss = 0.950\n",
      "2021-04-22T22:19:59.256014: Epoch   3 Batch   25/3125   train_loss = 0.966\n",
      "2021-04-22T22:19:59.781169: Epoch   3 Batch   45/3125   train_loss = 0.840\n",
      "2021-04-22T22:20:00.325853: Epoch   3 Batch   65/3125   train_loss = 0.993\n",
      "2021-04-22T22:20:00.856535: Epoch   3 Batch   85/3125   train_loss = 0.849\n",
      "2021-04-22T22:20:02.389654: Epoch   3 Batch  105/3125   train_loss = 0.785\n",
      "2021-04-22T22:20:03.074261: Epoch   3 Batch  125/3125   train_loss = 0.901\n",
      "2021-04-22T22:20:03.602507: Epoch   3 Batch  145/3125   train_loss = 0.949\n",
      "2021-04-22T22:20:04.133201: Epoch   3 Batch  165/3125   train_loss = 0.955\n",
      "2021-04-22T22:20:05.812234: Epoch   3 Batch  185/3125   train_loss = 0.877\n",
      "2021-04-22T22:20:06.334933: Epoch   3 Batch  205/3125   train_loss = 0.871\n",
      "2021-04-22T22:20:06.862610: Epoch   3 Batch  225/3125   train_loss = 0.825\n",
      "2021-04-22T22:20:07.678134: Epoch   3 Batch  245/3125   train_loss = 1.080\n",
      "2021-04-22T22:20:09.091326: Epoch   3 Batch  265/3125   train_loss = 0.914\n",
      "2021-04-22T22:20:09.620022: Epoch   3 Batch  285/3125   train_loss = 0.952\n",
      "2021-04-22T22:20:10.150716: Epoch   3 Batch  305/3125   train_loss = 0.883\n",
      "2021-04-22T22:20:11.313034: Epoch   3 Batch  325/3125   train_loss = 0.896\n",
      "2021-04-22T22:20:12.365427: Epoch   3 Batch  345/3125   train_loss = 1.011\n",
      "2021-04-22T22:20:12.890125: Epoch   3 Batch  365/3125   train_loss = 0.861\n",
      "2021-04-22T22:20:13.420833: Epoch   3 Batch  385/3125   train_loss = 0.899\n",
      "2021-04-22T22:20:14.893972: Epoch   3 Batch  405/3125   train_loss = 0.868\n",
      "2021-04-22T22:20:15.628562: Epoch   3 Batch  425/3125   train_loss = 0.985\n",
      "2021-04-22T22:20:16.156258: Epoch   3 Batch  445/3125   train_loss = 0.971\n",
      "2021-04-22T22:20:16.696951: Epoch   3 Batch  465/3125   train_loss = 0.884\n",
      "2021-04-22T22:20:18.384961: Epoch   3 Batch  485/3125   train_loss = 1.077\n",
      "2021-04-22T22:20:18.912658: Epoch   3 Batch  505/3125   train_loss = 0.815\n",
      "2021-04-22T22:20:19.436371: Epoch   3 Batch  525/3125   train_loss = 0.994\n",
      "2021-04-22T22:20:20.159940: Epoch   3 Batch  545/3125   train_loss = 0.879\n",
      "2021-04-22T22:20:21.664074: Epoch   3 Batch  565/3125   train_loss = 1.076\n",
      "2021-04-22T22:20:22.190779: Epoch   3 Batch  585/3125   train_loss = 0.888\n",
      "2021-04-22T22:20:22.723476: Epoch   3 Batch  605/3125   train_loss = 0.877\n",
      "2021-04-22T22:20:23.759875: Epoch   3 Batch  625/3125   train_loss = 0.936\n",
      "2021-04-22T22:20:24.947181: Epoch   3 Batch  645/3125   train_loss = 0.945\n",
      "2021-04-22T22:20:25.471893: Epoch   3 Batch  665/3125   train_loss = 1.065\n",
      "2021-04-22T22:20:26.003588: Epoch   3 Batch  685/3125   train_loss = 0.891\n",
      "2021-04-22T22:20:27.342804: Epoch   3 Batch  705/3125   train_loss = 1.054\n",
      "2021-04-22T22:20:28.226303: Epoch   3 Batch  725/3125   train_loss = 0.908\n",
      "2021-04-22T22:20:28.748007: Epoch   3 Batch  745/3125   train_loss = 0.916\n",
      "2021-04-22T22:20:29.290682: Epoch   3 Batch  765/3125   train_loss = 0.849\n",
      "2021-04-22T22:20:30.995700: Epoch   3 Batch  785/3125   train_loss = 1.050\n",
      "2021-04-22T22:20:31.520396: Epoch   3 Batch  805/3125   train_loss = 0.828\n",
      "2021-04-22T22:20:32.042110: Epoch   3 Batch  825/3125   train_loss = 0.936\n",
      "2021-04-22T22:20:32.639789: Epoch   3 Batch  845/3125   train_loss = 0.945\n",
      "2021-04-22T22:20:34.272813: Epoch   3 Batch  865/3125   train_loss = 1.033\n",
      "2021-04-22T22:20:34.799509: Epoch   3 Batch  885/3125   train_loss = 0.928\n",
      "2021-04-22T22:20:35.324216: Epoch   3 Batch  905/3125   train_loss = 1.062\n",
      "2021-04-22T22:20:36.258676: Epoch   3 Batch  925/3125   train_loss = 0.918\n",
      "2021-04-22T22:20:37.554935: Epoch   3 Batch  945/3125   train_loss = 0.983\n",
      "2021-04-22T22:20:38.086616: Epoch   3 Batch  965/3125   train_loss = 0.827\n",
      "2021-04-22T22:20:38.614326: Epoch   3 Batch  985/3125   train_loss = 1.039\n",
      "2021-04-22T22:20:39.881589: Epoch   3 Batch 1005/3125   train_loss = 0.870\n",
      "2021-04-22T22:20:40.845046: Epoch   3 Batch 1025/3125   train_loss = 0.878\n",
      "2021-04-22T22:20:41.374722: Epoch   3 Batch 1045/3125   train_loss = 1.099\n",
      "2021-04-22T22:20:41.903417: Epoch   3 Batch 1065/3125   train_loss = 0.894\n",
      "2021-04-22T22:20:43.542475: Epoch   3 Batch 1085/3125   train_loss = 0.848\n",
      "2021-04-22T22:20:44.127659: Epoch   3 Batch 1105/3125   train_loss = 0.800\n",
      "2021-04-22T22:20:44.659351: Epoch   3 Batch 1125/3125   train_loss = 0.890\n",
      "2021-04-22T22:20:45.191046: Epoch   3 Batch 1145/3125   train_loss = 0.848\n",
      "2021-04-22T22:20:46.903044: Epoch   3 Batch 1165/3125   train_loss = 1.012\n",
      "2021-04-22T22:20:47.426743: Epoch   3 Batch 1185/3125   train_loss = 0.849\n",
      "2021-04-22T22:20:47.955384: Epoch   3 Batch 1205/3125   train_loss = 0.911\n",
      "2021-04-22T22:20:48.862865: Epoch   3 Batch 1225/3125   train_loss = 1.007\n",
      "2021-04-22T22:20:50.176105: Epoch   3 Batch 1245/3125   train_loss = 1.052\n",
      "2021-04-22T22:20:50.690808: Epoch   3 Batch 1265/3125   train_loss = 0.914\n",
      "2021-04-22T22:20:51.239846: Epoch   3 Batch 1285/3125   train_loss = 0.996\n",
      "2021-04-22T22:20:52.447152: Epoch   3 Batch 1305/3125   train_loss = 0.817\n",
      "2021-04-22T22:20:53.454571: Epoch   3 Batch 1325/3125   train_loss = 0.861\n",
      "2021-04-22T22:20:53.980282: Epoch   3 Batch 1345/3125   train_loss = 0.950\n",
      "2021-04-22T22:20:54.524964: Epoch   3 Batch 1365/3125   train_loss = 0.824\n",
      "2021-04-22T22:20:56.035094: Epoch   3 Batch 1385/3125   train_loss = 0.815\n",
      "2021-04-22T22:20:56.731683: Epoch   3 Batch 1405/3125   train_loss = 0.871\n",
      "2021-04-22T22:20:57.255392: Epoch   3 Batch 1425/3125   train_loss = 1.069\n",
      "2021-04-22T22:20:57.796071: Epoch   3 Batch 1445/3125   train_loss = 1.078\n",
      "2021-04-22T22:20:59.484098: Epoch   3 Batch 1465/3125   train_loss = 0.932\n",
      "2021-04-22T22:21:00.010810: Epoch   3 Batch 1485/3125   train_loss = 0.982\n",
      "2021-04-22T22:21:00.532512: Epoch   3 Batch 1505/3125   train_loss = 0.813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-22T22:21:01.208108: Epoch   3 Batch 1525/3125   train_loss = 0.845\n",
      "2021-04-22T22:21:02.756214: Epoch   3 Batch 1545/3125   train_loss = 0.911\n",
      "2021-04-22T22:21:03.285909: Epoch   3 Batch 1565/3125   train_loss = 0.936\n",
      "2021-04-22T22:21:03.810621: Epoch   3 Batch 1585/3125   train_loss = 0.884\n",
      "2021-04-22T22:21:04.797043: Epoch   3 Batch 1605/3125   train_loss = 0.918\n",
      "2021-04-22T22:21:06.033341: Epoch   3 Batch 1625/3125   train_loss = 0.920\n",
      "2021-04-22T22:21:06.562042: Epoch   3 Batch 1645/3125   train_loss = 0.946\n",
      "2021-04-22T22:21:07.091718: Epoch   3 Batch 1665/3125   train_loss = 0.904\n",
      "2021-04-22T22:21:08.427950: Epoch   3 Batch 1685/3125   train_loss = 1.049\n",
      "2021-04-22T22:21:09.314437: Epoch   3 Batch 1705/3125   train_loss = 0.957\n",
      "2021-04-22T22:21:09.839135: Epoch   3 Batch 1725/3125   train_loss = 0.865\n",
      "2021-04-22T22:21:10.363833: Epoch   3 Batch 1745/3125   train_loss = 0.787\n",
      "2021-04-22T22:21:11.966912: Epoch   3 Batch 1765/3125   train_loss = 0.857\n",
      "2021-04-22T22:21:12.594768: Epoch   3 Batch 1785/3125   train_loss = 1.043\n",
      "2021-04-22T22:21:13.121465: Epoch   3 Batch 1805/3125   train_loss = 0.928\n",
      "2021-04-22T22:21:13.654159: Epoch   3 Batch 1825/3125   train_loss = 1.015\n",
      "2021-04-22T22:21:15.352180: Epoch   3 Batch 1845/3125   train_loss = 0.981\n",
      "2021-04-22T22:21:15.881876: Epoch   3 Batch 1865/3125   train_loss = 0.793\n",
      "2021-04-22T22:21:16.417567: Epoch   3 Batch 1885/3125   train_loss = 0.948\n",
      "2021-04-22T22:21:17.242094: Epoch   3 Batch 1905/3125   train_loss = 0.817\n",
      "2021-04-22T22:21:18.644300: Epoch   3 Batch 1925/3125   train_loss = 0.870\n",
      "2021-04-22T22:21:19.172925: Epoch   3 Batch 1945/3125   train_loss = 0.961\n",
      "2021-04-22T22:21:19.705613: Epoch   3 Batch 1965/3125   train_loss = 0.879\n",
      "2021-04-22T22:21:20.880926: Epoch   3 Batch 1985/3125   train_loss = 0.838\n",
      "2021-04-22T22:21:21.932331: Epoch   3 Batch 2005/3125   train_loss = 0.968\n",
      "2021-04-22T22:21:22.448020: Epoch   3 Batch 2025/3125   train_loss = 0.945\n",
      "2021-04-22T22:21:22.976715: Epoch   3 Batch 2045/3125   train_loss = 0.864\n",
      "2021-04-22T22:21:24.390902: Epoch   3 Batch 2065/3125   train_loss = 0.825\n",
      "2021-04-22T22:21:25.180461: Epoch   3 Batch 2085/3125   train_loss = 1.011\n",
      "2021-04-22T22:21:25.706143: Epoch   3 Batch 2105/3125   train_loss = 0.899\n",
      "2021-04-22T22:21:26.237847: Epoch   3 Batch 2125/3125   train_loss = 0.958\n",
      "2021-04-22T22:21:27.906878: Epoch   3 Batch 2145/3125   train_loss = 0.981\n",
      "2021-04-22T22:21:28.442582: Epoch   3 Batch 2165/3125   train_loss = 0.831\n",
      "2021-04-22T22:21:28.965281: Epoch   3 Batch 2185/3125   train_loss = 0.962\n",
      "2021-04-22T22:21:29.497960: Epoch   3 Batch 2205/3125   train_loss = 0.975\n",
      "2021-04-22T22:21:31.181990: Epoch   3 Batch 2225/3125   train_loss = 0.883\n",
      "2021-04-22T22:21:31.711686: Epoch   3 Batch 2245/3125   train_loss = 0.808\n",
      "2021-04-22T22:21:32.234575: Epoch   3 Batch 2265/3125   train_loss = 0.906\n",
      "2021-04-22T22:21:33.065093: Epoch   3 Batch 2285/3125   train_loss = 1.057\n",
      "2021-04-22T22:21:34.438300: Epoch   3 Batch 2305/3125   train_loss = 0.818\n",
      "2021-04-22T22:21:34.962998: Epoch   3 Batch 2325/3125   train_loss = 0.852\n",
      "2021-04-22T22:21:35.492892: Epoch   3 Batch 2345/3125   train_loss = 0.890\n",
      "2021-04-22T22:21:36.610240: Epoch   3 Batch 2365/3125   train_loss = 0.753\n",
      "2021-04-22T22:21:37.717600: Epoch   3 Batch 2385/3125   train_loss = 0.905\n",
      "2021-04-22T22:21:38.245297: Epoch   3 Batch 2405/3125   train_loss = 0.860\n",
      "2021-04-22T22:21:38.780003: Epoch   3 Batch 2425/3125   train_loss = 0.862\n",
      "2021-04-22T22:21:39.316696: Epoch   3 Batch 2445/3125   train_loss = 0.990\n",
      "2021-04-22T22:21:39.852385: Epoch   3 Batch 2465/3125   train_loss = 0.766\n",
      "2021-04-22T22:21:40.388063: Epoch   3 Batch 2485/3125   train_loss = 0.797\n",
      "2021-04-22T22:21:41.969323: Epoch   3 Batch 2505/3125   train_loss = 0.847\n",
      "2021-04-22T22:21:42.600956: Epoch   3 Batch 2525/3125   train_loss = 0.837\n",
      "2021-04-22T22:21:43.127654: Epoch   3 Batch 2545/3125   train_loss = 1.026\n",
      "2021-04-22T22:21:43.661362: Epoch   3 Batch 2565/3125   train_loss = 0.877\n",
      "2021-04-22T22:21:45.338381: Epoch   3 Batch 2585/3125   train_loss = 0.850\n",
      "2021-04-22T22:21:45.862079: Epoch   3 Batch 2605/3125   train_loss = 0.833\n",
      "2021-04-22T22:21:46.389776: Epoch   3 Batch 2625/3125   train_loss = 1.077\n",
      "2021-04-22T22:21:47.224299: Epoch   3 Batch 2645/3125   train_loss = 0.940\n",
      "2021-04-22T22:21:48.606498: Epoch   3 Batch 2665/3125   train_loss = 0.985\n",
      "2021-04-22T22:21:49.137307: Epoch   3 Batch 2685/3125   train_loss = 0.939\n",
      "2021-04-22T22:21:49.667991: Epoch   3 Batch 2705/3125   train_loss = 0.805\n",
      "2021-04-22T22:21:50.805337: Epoch   3 Batch 2725/3125   train_loss = 1.014\n",
      "2021-04-22T22:21:51.877720: Epoch   3 Batch 2745/3125   train_loss = 0.944\n",
      "2021-04-22T22:21:52.403430: Epoch   3 Batch 2765/3125   train_loss = 0.833\n",
      "2021-04-22T22:21:52.937118: Epoch   3 Batch 2785/3125   train_loss = 0.988\n",
      "2021-04-22T22:21:54.412265: Epoch   3 Batch 2805/3125   train_loss = 0.855\n",
      "2021-04-22T22:21:55.146835: Epoch   3 Batch 2825/3125   train_loss = 0.926\n",
      "2021-04-22T22:21:55.669548: Epoch   3 Batch 2845/3125   train_loss = 0.869\n",
      "2021-04-22T22:21:56.200230: Epoch   3 Batch 2865/3125   train_loss = 0.891\n",
      "2021-04-22T22:21:57.886259: Epoch   3 Batch 2885/3125   train_loss = 0.923\n",
      "2021-04-22T22:21:58.419962: Epoch   3 Batch 2905/3125   train_loss = 0.945\n",
      "2021-04-22T22:21:58.948656: Epoch   3 Batch 2925/3125   train_loss = 0.932\n",
      "2021-04-22T22:21:59.656245: Epoch   3 Batch 2945/3125   train_loss = 0.984\n",
      "2021-04-22T22:22:01.177363: Epoch   3 Batch 2965/3125   train_loss = 1.046\n",
      "2021-04-22T22:22:01.706068: Epoch   3 Batch 2985/3125   train_loss = 0.895\n",
      "2021-04-22T22:22:02.232773: Epoch   3 Batch 3005/3125   train_loss = 0.809\n",
      "2021-04-22T22:22:03.311141: Epoch   3 Batch 3025/3125   train_loss = 0.887\n",
      "2021-04-22T22:22:04.516443: Epoch   3 Batch 3045/3125   train_loss = 0.918\n",
      "2021-04-22T22:22:05.085128: Epoch   3 Batch 3065/3125   train_loss = 0.817\n",
      "2021-04-22T22:22:05.676772: Epoch   3 Batch 3085/3125   train_loss = 0.865\n",
      "2021-04-22T22:22:07.406777: Epoch   3 Batch 3105/3125   train_loss = 0.915\n",
      "2021-04-22T22:22:08.042476: Epoch   3 Batch   17/781   test_loss = 0.900\n",
      "2021-04-22T22:22:08.189376: Epoch   3 Batch   37/781   test_loss = 0.891\n",
      "2021-04-22T22:22:08.336294: Epoch   3 Batch   57/781   test_loss = 0.985\n",
      "2021-04-22T22:22:08.482208: Epoch   3 Batch   77/781   test_loss = 0.935\n",
      "2021-04-22T22:22:08.628124: Epoch   3 Batch   97/781   test_loss = 0.781\n",
      "2021-04-22T22:22:08.773425: Epoch   3 Batch  117/781   test_loss = 0.981\n",
      "2021-04-22T22:22:08.919340: Epoch   3 Batch  137/781   test_loss = 0.913\n",
      "2021-04-22T22:22:09.067256: Epoch   3 Batch  157/781   test_loss = 0.952\n",
      "2021-04-22T22:22:09.220167: Epoch   3 Batch  177/781   test_loss = 0.942\n",
      "2021-04-22T22:22:09.366083: Epoch   3 Batch  197/781   test_loss = 0.932\n",
      "2021-04-22T22:22:09.517006: Epoch   3 Batch  217/781   test_loss = 0.777\n",
      "2021-04-22T22:22:09.662913: Epoch   3 Batch  237/781   test_loss = 0.792\n",
      "2021-04-22T22:22:09.809827: Epoch   3 Batch  257/781   test_loss = 1.057\n",
      "2021-04-22T22:22:09.954505: Epoch   3 Batch  277/781   test_loss = 0.992\n",
      "2021-04-22T22:22:10.102421: Epoch   3 Batch  297/781   test_loss = 1.007\n",
      "2021-04-22T22:22:10.253333: Epoch   3 Batch  317/781   test_loss = 1.068\n",
      "2021-04-22T22:22:10.401239: Epoch   3 Batch  337/781   test_loss = 0.901\n",
      "2021-04-22T22:22:10.546157: Epoch   3 Batch  357/781   test_loss = 0.901\n",
      "2021-04-22T22:22:10.693080: Epoch   3 Batch  377/781   test_loss = 0.937\n",
      "2021-04-22T22:22:10.836988: Epoch   3 Batch  397/781   test_loss = 0.933\n",
      "2021-04-22T22:22:10.985902: Epoch   3 Batch  417/781   test_loss = 0.853\n",
      "2021-04-22T22:22:11.136815: Epoch   3 Batch  437/781   test_loss = 0.793\n",
      "2021-04-22T22:22:11.287729: Epoch   3 Batch  457/781   test_loss = 0.746\n",
      "2021-04-22T22:22:11.432658: Epoch   3 Batch  477/781   test_loss = 0.911\n",
      "2021-04-22T22:22:11.581559: Epoch   3 Batch  497/781   test_loss = 0.828\n",
      "2021-04-22T22:22:11.726475: Epoch   3 Batch  517/781   test_loss = 0.836\n",
      "2021-04-22T22:22:11.871391: Epoch   3 Batch  537/781   test_loss = 0.902\n",
      "2021-04-22T22:22:12.016307: Epoch   3 Batch  557/781   test_loss = 1.039\n",
      "2021-04-22T22:22:12.162226: Epoch   3 Batch  577/781   test_loss = 0.930\n",
      "2021-04-22T22:22:12.310138: Epoch   3 Batch  597/781   test_loss = 0.884\n",
      "2021-04-22T22:22:12.465050: Epoch   3 Batch  617/781   test_loss = 0.875\n",
      "2021-04-22T22:22:12.610400: Epoch   3 Batch  637/781   test_loss = 0.766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-22T22:22:12.762313: Epoch   3 Batch  657/781   test_loss = 1.035\n",
      "2021-04-22T22:22:12.909243: Epoch   3 Batch  677/781   test_loss = 0.939\n",
      "2021-04-22T22:22:13.054146: Epoch   3 Batch  697/781   test_loss = 0.920\n",
      "2021-04-22T22:22:13.204059: Epoch   3 Batch  717/781   test_loss = 0.854\n",
      "2021-04-22T22:22:13.353972: Epoch   3 Batch  737/781   test_loss = 0.821\n",
      "2021-04-22T22:22:13.500082: Epoch   3 Batch  757/781   test_loss = 1.053\n",
      "2021-04-22T22:22:13.647996: Epoch   3 Batch  777/781   test_loss = 0.930\n",
      "2021-04-22T22:22:14.503504: Epoch   4 Batch    0/3125   train_loss = 0.987\n",
      "2021-04-22T22:22:15.049402: Epoch   4 Batch   20/3125   train_loss = 0.914\n",
      "2021-04-22T22:22:15.588082: Epoch   4 Batch   40/3125   train_loss = 0.894\n",
      "2021-04-22T22:22:16.128352: Epoch   4 Batch   60/3125   train_loss = 0.715\n",
      "2021-04-22T22:22:16.673051: Epoch   4 Batch   80/3125   train_loss = 0.877\n",
      "2021-04-22T22:22:18.341074: Epoch   4 Batch  100/3125   train_loss = 0.929\n",
      "2021-04-22T22:22:18.865773: Epoch   4 Batch  120/3125   train_loss = 0.960\n",
      "2021-04-22T22:22:19.388474: Epoch   4 Batch  140/3125   train_loss = 0.922\n",
      "2021-04-22T22:22:20.105068: Epoch   4 Batch  160/3125   train_loss = 0.780\n",
      "2021-04-22T22:22:21.598200: Epoch   4 Batch  180/3125   train_loss = 0.886\n",
      "2021-04-22T22:22:22.126909: Epoch   4 Batch  200/3125   train_loss = 1.096\n",
      "2021-04-22T22:22:22.659588: Epoch   4 Batch  220/3125   train_loss = 0.919\n",
      "2021-04-22T22:22:23.746965: Epoch   4 Batch  240/3125   train_loss = 0.967\n",
      "2021-04-22T22:22:24.882322: Epoch   4 Batch  260/3125   train_loss = 0.971\n",
      "2021-04-22T22:22:25.410782: Epoch   4 Batch  280/3125   train_loss = 1.002\n",
      "2021-04-22T22:22:25.947222: Epoch   4 Batch  300/3125   train_loss = 1.034\n",
      "2021-04-22T22:22:27.358396: Epoch   4 Batch  320/3125   train_loss = 1.049\n",
      "2021-04-22T22:22:28.165939: Epoch   4 Batch  340/3125   train_loss = 0.772\n",
      "2021-04-22T22:22:28.693547: Epoch   4 Batch  360/3125   train_loss = 0.843\n",
      "2021-04-22T22:22:29.227236: Epoch   4 Batch  380/3125   train_loss = 0.881\n",
      "2021-04-22T22:22:30.918268: Epoch   4 Batch  400/3125   train_loss = 0.837\n",
      "2021-04-22T22:22:31.442964: Epoch   4 Batch  420/3125   train_loss = 0.828\n",
      "2021-04-22T22:22:31.972658: Epoch   4 Batch  440/3125   train_loss = 0.883\n",
      "2021-04-22T22:22:32.497343: Epoch   4 Batch  460/3125   train_loss = 0.912\n",
      "2021-04-22T22:22:33.039030: Epoch   4 Batch  480/3125   train_loss = 1.001\n",
      "2021-04-22T22:22:33.572737: Epoch   4 Batch  500/3125   train_loss = 0.679\n",
      "2021-04-22T22:22:34.270324: Epoch   4 Batch  520/3125   train_loss = 0.897\n",
      "2021-04-22T22:22:35.786786: Epoch   4 Batch  540/3125   train_loss = 0.834\n",
      "2021-04-22T22:22:36.313483: Epoch   4 Batch  560/3125   train_loss = 1.053\n",
      "2021-04-22T22:22:36.844883: Epoch   4 Batch  580/3125   train_loss = 0.987\n",
      "2021-04-22T22:22:37.851310: Epoch   4 Batch  600/3125   train_loss = 0.902\n",
      "2021-04-22T22:22:39.042631: Epoch   4 Batch  620/3125   train_loss = 0.896\n",
      "2021-04-22T22:22:39.570323: Epoch   4 Batch  640/3125   train_loss = 0.871\n",
      "2021-04-22T22:22:40.092014: Epoch   4 Batch  660/3125   train_loss = 0.906\n",
      "2021-04-22T22:22:41.469220: Epoch   4 Batch  680/3125   train_loss = 0.943\n",
      "2021-04-22T22:22:42.313743: Epoch   4 Batch  700/3125   train_loss = 0.880\n",
      "2021-04-22T22:22:42.843428: Epoch   4 Batch  720/3125   train_loss = 0.812\n",
      "2021-04-22T22:22:43.380129: Epoch   4 Batch  740/3125   train_loss = 0.902\n",
      "2021-04-22T22:22:45.089135: Epoch   4 Batch  760/3125   train_loss = 0.836\n",
      "2021-04-22T22:22:45.621843: Epoch   4 Batch  780/3125   train_loss = 0.938\n",
      "2021-04-22T22:22:46.170525: Epoch   4 Batch  800/3125   train_loss = 0.805\n",
      "2021-04-22T22:22:47.086986: Epoch   4 Batch  820/3125   train_loss = 0.863\n",
      "2021-04-22T22:22:48.482181: Epoch   4 Batch  840/3125   train_loss = 0.810\n",
      "2021-04-22T22:22:49.006879: Epoch   4 Batch  860/3125   train_loss = 0.855\n",
      "2021-04-22T22:22:49.553563: Epoch   4 Batch  880/3125   train_loss = 0.816\n",
      "2021-04-22T22:22:51.055701: Epoch   4 Batch  900/3125   train_loss = 0.931\n",
      "2021-04-22T22:22:51.865242: Epoch   4 Batch  920/3125   train_loss = 0.893\n",
      "2021-04-22T22:22:52.387945: Epoch   4 Batch  940/3125   train_loss = 0.874\n",
      "2021-04-22T22:22:52.924624: Epoch   4 Batch  960/3125   train_loss = 0.951\n",
      "2021-04-22T22:22:54.632639: Epoch   4 Batch  980/3125   train_loss = 0.979\n",
      "2021-04-22T22:22:55.158350: Epoch   4 Batch 1000/3125   train_loss = 0.982\n",
      "2021-04-22T22:22:55.690042: Epoch   4 Batch 1020/3125   train_loss = 0.906\n",
      "2021-04-22T22:22:56.570517: Epoch   4 Batch 1040/3125   train_loss = 0.794\n",
      "2021-04-22T22:22:57.936728: Epoch   4 Batch 1060/3125   train_loss = 0.955\n",
      "2021-04-22T22:22:58.461426: Epoch   4 Batch 1080/3125   train_loss = 0.915\n",
      "2021-04-22T22:22:58.994129: Epoch   4 Batch 1100/3125   train_loss = 0.832\n",
      "2021-04-22T22:23:00.269386: Epoch   4 Batch 1120/3125   train_loss = 0.889\n",
      "2021-04-22T22:23:01.218839: Epoch   4 Batch 1140/3125   train_loss = 0.907\n",
      "2021-04-22T22:23:01.757529: Epoch   4 Batch 1160/3125   train_loss = 0.807\n",
      "2021-04-22T22:23:02.290392: Epoch   4 Batch 1180/3125   train_loss = 0.853\n",
      "2021-04-22T22:23:03.939466: Epoch   4 Batch 1200/3125   train_loss = 0.991\n",
      "2021-04-22T22:23:04.511112: Epoch   4 Batch 1220/3125   train_loss = 0.951\n",
      "2021-04-22T22:23:05.035796: Epoch   4 Batch 1240/3125   train_loss = 0.832\n",
      "2021-04-22T22:23:05.571502: Epoch   4 Batch 1260/3125   train_loss = 0.907\n",
      "2021-04-22T22:23:07.256532: Epoch   4 Batch 1280/3125   train_loss = 0.878\n",
      "2021-04-22T22:23:07.784228: Epoch   4 Batch 1300/3125   train_loss = 0.803\n",
      "2021-04-22T22:23:08.321914: Epoch   4 Batch 1320/3125   train_loss = 0.899\n",
      "2021-04-22T22:23:09.200400: Epoch   4 Batch 1340/3125   train_loss = 0.721\n",
      "2021-04-22T22:23:10.537158: Epoch   4 Batch 1360/3125   train_loss = 0.787\n",
      "2021-04-22T22:23:11.079815: Epoch   4 Batch 1380/3125   train_loss = 0.826\n",
      "2021-04-22T22:23:11.663479: Epoch   4 Batch 1400/3125   train_loss = 0.979\n",
      "2021-04-22T22:23:13.220584: Epoch   4 Batch 1420/3125   train_loss = 0.919\n",
      "2021-04-22T22:23:13.973149: Epoch   4 Batch 1440/3125   train_loss = 0.748\n",
      "2021-04-22T22:23:14.548817: Epoch   4 Batch 1460/3125   train_loss = 0.872\n",
      "2021-04-22T22:23:15.384338: Epoch   4 Batch 1480/3125   train_loss = 0.898\n",
      "2021-04-22T22:23:16.839498: Epoch   4 Batch 1500/3125   train_loss = 0.860\n",
      "2021-04-22T22:23:17.360026: Epoch   4 Batch 1520/3125   train_loss = 0.782\n",
      "2021-04-22T22:23:17.889719: Epoch   4 Batch 1540/3125   train_loss = 0.947\n",
      "2021-04-22T22:23:19.003068: Epoch   4 Batch 1560/3125   train_loss = 0.806\n",
      "2021-04-22T22:23:20.097434: Epoch   4 Batch 1580/3125   train_loss = 0.953\n",
      "2021-04-22T22:23:20.620133: Epoch   4 Batch 1600/3125   train_loss = 0.826\n",
      "2021-04-22T22:23:21.158834: Epoch   4 Batch 1620/3125   train_loss = 0.795\n",
      "2021-04-22T22:23:22.573012: Epoch   4 Batch 1640/3125   train_loss = 0.937\n",
      "2021-04-22T22:23:23.377559: Epoch   4 Batch 1660/3125   train_loss = 0.951\n",
      "2021-04-22T22:23:23.909253: Epoch   4 Batch 1680/3125   train_loss = 0.920\n",
      "2021-04-22T22:23:24.443932: Epoch   4 Batch 1700/3125   train_loss = 0.783\n",
      "2021-04-22T22:23:26.155944: Epoch   4 Batch 1720/3125   train_loss = 0.888\n",
      "2021-04-22T22:23:26.678988: Epoch   4 Batch 1740/3125   train_loss = 0.981\n",
      "2021-04-22T22:23:27.201477: Epoch   4 Batch 1760/3125   train_loss = 0.934\n",
      "2021-04-22T22:23:27.891081: Epoch   4 Batch 1780/3125   train_loss = 0.870\n",
      "2021-04-22T22:23:29.423212: Epoch   4 Batch 1800/3125   train_loss = 0.831\n",
      "2021-04-22T22:23:29.957890: Epoch   4 Batch 1820/3125   train_loss = 0.841\n",
      "2021-04-22T22:23:30.488599: Epoch   4 Batch 1840/3125   train_loss = 0.922\n",
      "2021-04-22T22:23:31.556972: Epoch   4 Batch 1860/3125   train_loss = 0.946\n",
      "2021-04-22T22:23:32.719314: Epoch   4 Batch 1880/3125   train_loss = 0.877\n",
      "2021-04-22T22:23:33.245008: Epoch   4 Batch 1900/3125   train_loss = 0.744\n",
      "2021-04-22T22:23:33.777690: Epoch   4 Batch 1920/3125   train_loss = 0.893\n",
      "2021-04-22T22:23:35.182882: Epoch   4 Batch 1940/3125   train_loss = 0.781\n",
      "2021-04-22T22:23:35.998421: Epoch   4 Batch 1960/3125   train_loss = 0.799\n",
      "2021-04-22T22:23:36.521110: Epoch   4 Batch 1980/3125   train_loss = 0.889\n",
      "2021-04-22T22:23:37.055802: Epoch   4 Batch 2000/3125   train_loss = 1.081\n",
      "2021-04-22T22:23:38.753824: Epoch   4 Batch 2020/3125   train_loss = 0.915\n",
      "2021-04-22T22:23:39.281521: Epoch   4 Batch 2040/3125   train_loss = 0.784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-22T22:23:39.809233: Epoch   4 Batch 2060/3125   train_loss = 0.827\n",
      "2021-04-22T22:23:40.406875: Epoch   4 Batch 2080/3125   train_loss = 1.039\n",
      "2021-04-22T22:23:42.026949: Epoch   4 Batch 2100/3125   train_loss = 0.794\n",
      "2021-04-22T22:23:42.558633: Epoch   4 Batch 2120/3125   train_loss = 0.791\n",
      "2021-04-22T22:23:43.097340: Epoch   4 Batch 2140/3125   train_loss = 0.877\n",
      "2021-04-22T22:23:44.043781: Epoch   4 Batch 2160/3125   train_loss = 0.846\n",
      "2021-04-22T22:23:45.329038: Epoch   4 Batch 2180/3125   train_loss = 0.911\n",
      "2021-04-22T22:23:45.859732: Epoch   4 Batch 2200/3125   train_loss = 0.846\n",
      "2021-04-22T22:23:46.392425: Epoch   4 Batch 2220/3125   train_loss = 0.841\n",
      "2021-04-22T22:23:47.688681: Epoch   4 Batch 2240/3125   train_loss = 0.827\n",
      "2021-04-22T22:23:48.605161: Epoch   4 Batch 2260/3125   train_loss = 0.937\n",
      "2021-04-22T22:23:49.129860: Epoch   4 Batch 2280/3125   train_loss = 0.928\n",
      "2021-04-22T22:23:49.671552: Epoch   4 Batch 2300/3125   train_loss = 0.904\n",
      "2021-04-22T22:23:51.290608: Epoch   4 Batch 2320/3125   train_loss = 0.957\n",
      "2021-04-22T22:23:51.880266: Epoch   4 Batch 2340/3125   train_loss = 0.885\n",
      "2021-04-22T22:23:52.400965: Epoch   4 Batch 2360/3125   train_loss = 0.876\n",
      "2021-04-22T22:23:52.939658: Epoch   4 Batch 2380/3125   train_loss = 0.863\n",
      "2021-04-22T22:23:54.616690: Epoch   4 Batch 2400/3125   train_loss = 0.963\n",
      "2021-04-22T22:23:55.141402: Epoch   4 Batch 2420/3125   train_loss = 0.839\n",
      "2021-04-22T22:23:55.676080: Epoch   4 Batch 2440/3125   train_loss = 0.818\n",
      "2021-04-22T22:23:56.445643: Epoch   4 Batch 2460/3125   train_loss = 0.864\n",
      "2021-04-22T22:23:57.899176: Epoch   4 Batch 2480/3125   train_loss = 0.971\n",
      "2021-04-22T22:23:58.425869: Epoch   4 Batch 2500/3125   train_loss = 0.796\n",
      "2021-04-22T22:23:58.957563: Epoch   4 Batch 2520/3125   train_loss = 0.903\n",
      "2021-04-22T22:24:00.026866: Epoch   4 Batch 2540/3125   train_loss = 0.844\n",
      "2021-04-22T22:24:01.179200: Epoch   4 Batch 2560/3125   train_loss = 0.677\n",
      "2021-04-22T22:24:01.700900: Epoch   4 Batch 2580/3125   train_loss = 0.875\n",
      "2021-04-22T22:24:02.232609: Epoch   4 Batch 2600/3125   train_loss = 0.870\n",
      "2021-04-22T22:24:03.655776: Epoch   4 Batch 2620/3125   train_loss = 0.819\n",
      "2021-04-22T22:24:04.458312: Epoch   4 Batch 2640/3125   train_loss = 0.841\n",
      "2021-04-22T22:24:04.979978: Epoch   4 Batch 2660/3125   train_loss = 0.971\n",
      "2021-04-22T22:24:05.517653: Epoch   4 Batch 2680/3125   train_loss = 0.762\n",
      "2021-04-22T22:24:07.220673: Epoch   4 Batch 2700/3125   train_loss = 0.882\n",
      "2021-04-22T22:24:07.748376: Epoch   4 Batch 2720/3125   train_loss = 0.745\n",
      "2021-04-22T22:24:08.281062: Epoch   4 Batch 2740/3125   train_loss = 0.885\n",
      "2021-04-22T22:24:08.943682: Epoch   4 Batch 2760/3125   train_loss = 0.781\n",
      "2021-04-22T22:24:10.510049: Epoch   4 Batch 2780/3125   train_loss = 0.788\n",
      "2021-04-22T22:24:11.033764: Epoch   4 Batch 2800/3125   train_loss = 1.062\n",
      "2021-04-22T22:24:11.563442: Epoch   4 Batch 2820/3125   train_loss = 1.070\n",
      "2021-04-22T22:24:12.605843: Epoch   4 Batch 2840/3125   train_loss = 0.897\n",
      "2021-04-22T22:24:13.797171: Epoch   4 Batch 2860/3125   train_loss = 0.788\n",
      "2021-04-22T22:24:14.322241: Epoch   4 Batch 2880/3125   train_loss = 0.851\n",
      "2021-04-22T22:24:14.917645: Epoch   4 Batch 2900/3125   train_loss = 0.901\n",
      "2021-04-22T22:24:15.455145: Epoch   4 Batch 2920/3125   train_loss = 0.847\n",
      "2021-04-22T22:24:15.993834: Epoch   4 Batch 2940/3125   train_loss = 0.914\n",
      "2021-04-22T22:24:16.534538: Epoch   4 Batch 2960/3125   train_loss = 0.890\n",
      "2021-04-22T22:24:18.157590: Epoch   4 Batch 2980/3125   train_loss = 0.842\n",
      "2021-04-22T22:24:18.736255: Epoch   4 Batch 3000/3125   train_loss = 0.969\n",
      "2021-04-22T22:24:19.263964: Epoch   4 Batch 3020/3125   train_loss = 0.964\n",
      "2021-04-22T22:24:19.804654: Epoch   4 Batch 3040/3125   train_loss = 0.882\n",
      "2021-04-22T22:24:21.495666: Epoch   4 Batch 3060/3125   train_loss = 0.793\n",
      "2021-04-22T22:24:22.022363: Epoch   4 Batch 3080/3125   train_loss = 0.992\n",
      "2021-04-22T22:24:22.556066: Epoch   4 Batch 3100/3125   train_loss = 1.039\n",
      "2021-04-22T22:24:23.535494: Epoch   4 Batch 3120/3125   train_loss = 0.836\n",
      "2021-04-22T22:24:24.450004: Epoch   4 Batch   16/781   test_loss = 0.787\n",
      "2021-04-22T22:24:24.649868: Epoch   4 Batch   36/781   test_loss = 0.925\n",
      "2021-04-22T22:24:24.799791: Epoch   4 Batch   56/781   test_loss = 0.939\n",
      "2021-04-22T22:24:24.946697: Epoch   4 Batch   76/781   test_loss = 1.050\n",
      "2021-04-22T22:24:25.095621: Epoch   4 Batch   96/781   test_loss = 0.991\n",
      "2021-04-22T22:24:25.239528: Epoch   4 Batch  116/781   test_loss = 0.869\n",
      "2021-04-22T22:24:25.389443: Epoch   4 Batch  136/781   test_loss = 0.800\n",
      "2021-04-22T22:24:25.536358: Epoch   4 Batch  156/781   test_loss = 0.855\n",
      "2021-04-22T22:24:25.681273: Epoch   4 Batch  176/781   test_loss = 0.886\n",
      "2021-04-22T22:24:25.833197: Epoch   4 Batch  196/781   test_loss = 0.816\n",
      "2021-04-22T22:24:25.982115: Epoch   4 Batch  216/781   test_loss = 0.963\n",
      "2021-04-22T22:24:26.134023: Epoch   4 Batch  236/781   test_loss = 0.824\n",
      "2021-04-22T22:24:26.282927: Epoch   4 Batch  256/781   test_loss = 0.822\n",
      "2021-04-22T22:24:26.431842: Epoch   4 Batch  276/781   test_loss = 1.095\n",
      "2021-04-22T22:24:26.580767: Epoch   4 Batch  296/781   test_loss = 0.810\n",
      "2021-04-22T22:24:26.724683: Epoch   4 Batch  316/781   test_loss = 0.859\n",
      "2021-04-22T22:24:26.874587: Epoch   4 Batch  336/781   test_loss = 0.787\n",
      "2021-04-22T22:24:27.022503: Epoch   4 Batch  356/781   test_loss = 0.881\n",
      "2021-04-22T22:24:27.171417: Epoch   4 Batch  376/781   test_loss = 0.919\n",
      "2021-04-22T22:24:27.315332: Epoch   4 Batch  396/781   test_loss = 0.860\n",
      "2021-04-22T22:24:27.467246: Epoch   4 Batch  416/781   test_loss = 0.957\n",
      "2021-04-22T22:24:27.615161: Epoch   4 Batch  436/781   test_loss = 0.914\n",
      "2021-04-22T22:24:27.763074: Epoch   4 Batch  456/781   test_loss = 0.730\n",
      "2021-04-22T22:24:27.913987: Epoch   4 Batch  476/781   test_loss = 0.933\n",
      "2021-04-22T22:24:28.065900: Epoch   4 Batch  496/781   test_loss = 0.971\n",
      "2021-04-22T22:24:28.209819: Epoch   4 Batch  516/781   test_loss = 0.764\n",
      "2021-04-22T22:24:28.358731: Epoch   4 Batch  536/781   test_loss = 0.920\n",
      "2021-04-22T22:24:28.505648: Epoch   4 Batch  556/781   test_loss = 0.872\n",
      "2021-04-22T22:24:28.654575: Epoch   4 Batch  576/781   test_loss = 0.966\n",
      "2021-04-22T22:24:28.800478: Epoch   4 Batch  596/781   test_loss = 0.952\n",
      "2021-04-22T22:24:28.954390: Epoch   4 Batch  616/781   test_loss = 1.008\n",
      "2021-04-22T22:24:29.097306: Epoch   4 Batch  636/781   test_loss = 0.833\n",
      "2021-04-22T22:24:29.246231: Epoch   4 Batch  656/781   test_loss = 0.899\n",
      "2021-04-22T22:24:29.393137: Epoch   4 Batch  676/781   test_loss = 1.046\n",
      "2021-04-22T22:24:29.542060: Epoch   4 Batch  696/781   test_loss = 0.869\n",
      "2021-04-22T22:24:29.685967: Epoch   4 Batch  716/781   test_loss = 0.890\n",
      "2021-04-22T22:24:29.834424: Epoch   4 Batch  736/781   test_loss = 1.032\n",
      "2021-04-22T22:24:29.986336: Epoch   4 Batch  756/781   test_loss = 0.841\n",
      "2021-04-22T22:24:30.133253: Epoch   4 Batch  776/781   test_loss = 0.746\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "losses = {'train':[], 'test':[]}\n",
    "\n",
    "with tf.Session(graph=train_graph, config=config) as sess:\n",
    "    \n",
    "    #搜集数据给tensorBoard用\n",
    "    # Keep track of gradient values and sparsity\n",
    "    grad_summaries = []\n",
    "    for g, v in gradients:\n",
    "        if g is not None:\n",
    "            grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name.replace(':', '_')), g)\n",
    "            sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name.replace(':', '_')), tf.nn.zero_fraction(g))\n",
    "            grad_summaries.append(grad_hist_summary)\n",
    "            grad_summaries.append(sparsity_summary)\n",
    "    grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "        \n",
    "    # Output directory for models and summaries\n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "    print(\"Writing to {}\\n\".format(out_dir))\n",
    "     \n",
    "    # Summaries for loss and accuracy\n",
    "    loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "    # Train Summaries\n",
    "    train_summary_op = tf.summary.merge([loss_summary, grad_summaries_merged])\n",
    "    train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "    train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "    # Inference summaries\n",
    "    inference_summary_op = tf.summary.merge([loss_summary])\n",
    "    inference_summary_dir = os.path.join(out_dir, \"summaries\", \"inference\")\n",
    "    inference_summary_writer = tf.summary.FileWriter(inference_summary_dir, sess.graph)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    for epoch_i in range(num_epochs):\n",
    "        \n",
    "        #将数据集分成训练集和测试集，随机种子不固定\n",
    "        train_X,test_X, train_y, test_y = train_test_split(features,  \n",
    "                                                           targets_values,  \n",
    "                                                           test_size = 0.2,  \n",
    "                                                           random_state = 0)  \n",
    "        \n",
    "        train_batches = get_batches(train_X, train_y, batch_size)\n",
    "        test_batches = get_batches(test_X, test_y, batch_size)\n",
    "    \n",
    "        #训练的迭代，保存训练损失\n",
    "        for batch_i in range(len(train_X) // batch_size):\n",
    "            x, y = next(train_batches)\n",
    "\n",
    "            categories = np.zeros([batch_size, 18])\n",
    "            for i in range(batch_size):\n",
    "                categories[i] = x.take(6,1)[i]\n",
    "\n",
    "            titles = np.zeros([batch_size, sentences_size])\n",
    "            for i in range(batch_size):\n",
    "                titles[i] = x.take(5,1)[i]\n",
    "            \n",
    "            feed = {\n",
    "                uid: np.reshape(x.take(0,1), [batch_size, 1]),\n",
    "                user_gender: np.reshape(x.take(2,1), [batch_size, 1]),\n",
    "                user_age: np.reshape(x.take(3,1), [batch_size, 1]),\n",
    "                user_job: np.reshape(x.take(4,1), [batch_size, 1]),\n",
    "                movie_id: np.reshape(x.take(1,1), [batch_size, 1]),\n",
    "                movie_categories: categories,  #x.take(6,1)\n",
    "                movie_titles: titles,  #x.take(5,1)\n",
    "                targets: np.reshape(y, [batch_size, 1]),\n",
    "                dropout_keep_prob: dropout_keep, #dropout_keep\n",
    "                lr: learning_rate}\n",
    "            \n",
    "            step, train_loss, summaries, _ = sess.run([global_step, loss, train_summary_op, train_op], feed)  #cost\n",
    "            \n",
    "            losses['train'].append(train_loss)\n",
    "            train_summary_writer.add_summary(summaries, step)  #\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * (len(train_X) // batch_size) + batch_i) % show_every_n_batches == 0:\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print('{}: Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    time_str,\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    (len(train_X) // batch_size),\n",
    "                    train_loss))\n",
    "                \n",
    "        #使用测试数据的迭代\n",
    "        for batch_i  in range(len(test_X) // batch_size):\n",
    "            x, y = next(test_batches)\n",
    "            \n",
    "            categories = np.zeros([batch_size, 18])\n",
    "            for i in range(batch_size):\n",
    "                categories[i] = x.take(6,1)[i]\n",
    "\n",
    "            titles = np.zeros([batch_size, sentences_size])\n",
    "            for i in range(batch_size):\n",
    "                titles[i] = x.take(5,1)[i]\n",
    "\n",
    "            feed = {\n",
    "                uid: np.reshape(x.take(0,1), [batch_size, 1]),\n",
    "                user_gender: np.reshape(x.take(2,1), [batch_size, 1]),\n",
    "                user_age: np.reshape(x.take(3,1), [batch_size, 1]),\n",
    "                user_job: np.reshape(x.take(4,1), [batch_size, 1]),\n",
    "                movie_id: np.reshape(x.take(1,1), [batch_size, 1]),\n",
    "                movie_categories: categories,  #x.take(6,1)\n",
    "                movie_titles: titles,  #x.take(5,1)\n",
    "                targets: np.reshape(y, [batch_size, 1]),\n",
    "                dropout_keep_prob: 1,\n",
    "                lr: learning_rate}\n",
    "            \n",
    "            step, test_loss, summaries = sess.run([global_step, loss, inference_summary_op], feed)  #cost\n",
    "\n",
    "            #保存测试损失\n",
    "            losses['test'].append(test_loss)\n",
    "            inference_summary_writer.add_summary(summaries, step)  #\n",
    "\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            if (epoch_i * (len(test_X) // batch_size) + batch_i) % show_every_n_batches == 0:\n",
    "                print('{}: Epoch {:>3} Batch {:>4}/{}   test_loss = {:.3f}'.format(\n",
    "                    time_str,\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    (len(test_X) // batch_size),\n",
    "                    test_loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver.save(sess, save_dir)  #, global_step=epoch_i\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "jZy763jFQ-dZ"
   },
   "outputs": [],
   "source": [
    "save_params((save_dir))\n",
    "\n",
    "load_dir = load_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "bOCKZe59dU10"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6317194, 0.669506)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load_dir\n",
    "# !rm -rf ml-1m\n",
    "min(losses['train']), min(losses['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "I_jjFeUdRnQp",
    "outputId": "baa09280-1ad5-4212-a38a-65a8bd8211de"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses['train'], label='Training loss')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "xKssIf8nRquY",
    "outputId": "43d8d72d-90bd-4875-a670-67d2cfa9e214"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses['test'], label='Test loss')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "0P-VRGn-Rvgg"
   },
   "outputs": [],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "\n",
    "    uid = loaded_graph.get_tensor_by_name(\"uid:0\")\n",
    "    user_gender = loaded_graph.get_tensor_by_name(\"user_gender:0\")\n",
    "    user_age = loaded_graph.get_tensor_by_name(\"user_age:0\")\n",
    "    user_job = loaded_graph.get_tensor_by_name(\"user_job:0\")\n",
    "    movie_id = loaded_graph.get_tensor_by_name(\"movie_id:0\")\n",
    "    movie_categories = loaded_graph.get_tensor_by_name(\"movie_categories:0\")\n",
    "    movie_titles = loaded_graph.get_tensor_by_name(\"movie_titles:0\")\n",
    "    targets = loaded_graph.get_tensor_by_name(\"targets:0\")\n",
    "    dropout_keep_prob = loaded_graph.get_tensor_by_name(\"dropout_keep_prob:0\")\n",
    "    lr = loaded_graph.get_tensor_by_name(\"LearningRate:0\")\n",
    "    #两种不同计算预测评分的方案使用不同的name获取tensor inference\n",
    "#     inference = loaded_graph.get_tensor_by_name(\"inference/inference/BiasAdd:0\")\n",
    "    inference = loaded_graph.get_tensor_by_name(\"inference/ExpandDims:0\") # 之前是MatMul:0 因为inference代码修改了 这里也要修改 感谢网友 @清歌 指出问题\n",
    "    movie_combine_layer_flat = loaded_graph.get_tensor_by_name(\"movie_fc/Reshape:0\")\n",
    "    user_combine_layer_flat = loaded_graph.get_tensor_by_name(\"user_fc/Reshape:0\")\n",
    "    return uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, inference, movie_combine_layer_flat, user_combine_layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Vh-Xl0c-RzK3"
   },
   "outputs": [],
   "source": [
    "def rating_movie(user_id_val, movie_id_val):\n",
    "    loaded_graph = tf.Graph()  #\n",
    "    with tf.Session(graph=loaded_graph) as sess:  #\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "    \n",
    "        # Get Tensors from loaded model\n",
    "        uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, inference,_, __ = get_tensors(loaded_graph)  #loaded_graph\n",
    "    \n",
    "        categories = np.zeros([1, 18])\n",
    "        categories[0] = movies.values[movieid2idx[movie_id_val]][2]\n",
    "    \n",
    "        titles = np.zeros([1, sentences_size])\n",
    "        titles[0] = movies.values[movieid2idx[movie_id_val]][1]\n",
    "    \n",
    "        feed = {\n",
    "              uid: np.reshape(users.values[user_id_val-1][0], [1, 1]),\n",
    "              user_gender: np.reshape(users.values[user_id_val-1][1], [1, 1]),\n",
    "              user_age: np.reshape(users.values[user_id_val-1][2], [1, 1]),\n",
    "              user_job: np.reshape(users.values[user_id_val-1][3], [1, 1]),\n",
    "              movie_id: np.reshape(movies.values[movieid2idx[movie_id_val]][0], [1, 1]),\n",
    "              movie_categories: categories,  #x.take(6,1)\n",
    "              movie_titles: titles,  #x.take(5,1)\n",
    "              dropout_keep_prob: 1}\n",
    "    \n",
    "        # Get Prediction\n",
    "        inference_val = sess.run([inference], feed)  \n",
    "    \n",
    "        return (inference_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZRE6fPkLR2xP",
    "outputId": "ec3e7de8-155a-40f3-92f2-2489a5f4c6ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[3.5354874]], dtype=float32)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_movie(234, 1401)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3K-cKqlDR453",
    "outputId": "e2134d50-731e-494a-e69b-83965404c728"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save/\n"
     ]
    }
   ],
   "source": [
    "loaded_graph = tf.Graph()  #\n",
    "movie_matrics = []\n",
    "with tf.Session(graph=loaded_graph) as sess:  #\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, _, movie_combine_layer_flat, __ = get_tensors(loaded_graph)  #loaded_graph\n",
    "\n",
    "    for item in movies.values:\n",
    "        categories = np.zeros([1, 18])\n",
    "        categories[0] = item.take(2)\n",
    "\n",
    "        titles = np.zeros([1, sentences_size])\n",
    "        titles[0] = item.take(1)\n",
    "\n",
    "        feed = {\n",
    "            movie_id: np.reshape(item.take(0), [1, 1]),\n",
    "            movie_categories: categories,  #x.take(6,1)\n",
    "            movie_titles: titles,  #x.take(5,1)\n",
    "            dropout_keep_prob: 1}\n",
    "\n",
    "        movie_combine_layer_flat_val = sess.run([movie_combine_layer_flat], feed)  \n",
    "        movie_matrics.append(movie_combine_layer_flat_val)\n",
    "\n",
    "pickle.dump((np.array(movie_matrics).reshape(-1, 200)), open('movie_matrics.p', 'wb'))\n",
    "movie_matrics = pickle.load(open('movie_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bmsXiepvSAK_",
    "outputId": "75291bfd-abc0-431f-9138-7917b77131ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save/\n"
     ]
    }
   ],
   "source": [
    "loaded_graph = tf.Graph()  #\n",
    "users_matrics = []\n",
    "with tf.Session(graph=loaded_graph) as sess:  #\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, _, __,user_combine_layer_flat = get_tensors(loaded_graph)  #loaded_graph\n",
    "\n",
    "    for item in users.values:\n",
    "\n",
    "        feed = {\n",
    "            uid: np.reshape(item.take(0), [1, 1]),\n",
    "            user_gender: np.reshape(item.take(1), [1, 1]),\n",
    "            user_age: np.reshape(item.take(2), [1, 1]),\n",
    "            user_job: np.reshape(item.take(3), [1, 1]),\n",
    "            dropout_keep_prob: 1}\n",
    "\n",
    "        user_combine_layer_flat_val = sess.run([user_combine_layer_flat], feed)  \n",
    "        users_matrics.append(user_combine_layer_flat_val)\n",
    "\n",
    "pickle.dump((np.array(users_matrics).reshape(-1, 200)), open('users_matrics.p', 'wb'))\n",
    "users_matrics = pickle.load(open('users_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "2z7lM5j0SSKn"
   },
   "outputs": [],
   "source": [
    "def recommend_same_type_movie(movie_id_val, top_k = 20):\n",
    "    \n",
    "    loaded_graph = tf.Graph()  #\n",
    "    with tf.Session(graph=loaded_graph) as sess:  #\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "        \n",
    "        norm_movie_matrics = tf.sqrt(tf.reduce_sum(tf.square(movie_matrics), 1, keep_dims=True))\n",
    "        normalized_movie_matrics = movie_matrics / norm_movie_matrics\n",
    "\n",
    "        #推荐同类型的电影\n",
    "        probs_embeddings = (movie_matrics[movieid2idx[movie_id_val]]).reshape([1, 200])\n",
    "        probs_similarity = tf.matmul(probs_embeddings, tf.transpose(normalized_movie_matrics))\n",
    "        sim = (probs_similarity.eval())\n",
    "    #     results = (-sim[0]).argsort()[0:top_k]\n",
    "    #     print(results)\n",
    "        \n",
    "        print(\"您看的电影是：{}\".format(movies_orig[movieid2idx[movie_id_val]]))\n",
    "        print(\"以下是给您的推荐：\")\n",
    "        p = np.squeeze(sim)\n",
    "        p[np.argsort(p)[:-top_k]] = 0\n",
    "        p = p / np.sum(p)\n",
    "        results = set()\n",
    "        while len(results) != 5:\n",
    "            c = np.random.choice(3883, 1, p=p)[0]\n",
    "            results.add(c)\n",
    "        for val in (results):\n",
    "            print(val)\n",
    "            print(movies_orig[val])\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8VbLBuRpSV2o",
    "outputId": "10df2820-f32e-46db-b82b-5ef4e0cf2da8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save/\n",
      "您看的电影是：[1401 'Ghosts of Mississippi (1996)' 'Drama']\n",
      "以下是给您的推荐：\n",
      "1380\n",
      "[1401 'Ghosts of Mississippi (1996)' 'Drama']\n",
      "3846\n",
      "[3916 'Remember the Titans (2000)' 'Drama']\n",
      "3346\n",
      "[3415 'Mirror, The (Zerkalo) (1975)' 'Drama']\n",
      "3029\n",
      "[3098 'Natural, The (1984)' 'Drama']\n",
      "471\n",
      "[475 'In the Name of the Father (1993)' 'Drama']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{471, 1380, 3029, 3346, 3846}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_same_type_movie(1401, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "A2WVuM7lSXv3"
   },
   "outputs": [],
   "source": [
    "def recommend_your_favorite_movie(user_id_val, top_k = 10):\n",
    "\n",
    "    loaded_graph = tf.Graph()  #\n",
    "    with tf.Session(graph=loaded_graph) as sess:  #\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "\n",
    "        #推荐您喜欢的电影\n",
    "        probs_embeddings = (users_matrics[user_id_val-1]).reshape([1, 200])\n",
    "\n",
    "        probs_similarity = tf.matmul(probs_embeddings, tf.transpose(movie_matrics))\n",
    "        sim = (probs_similarity.eval())\n",
    "    #     print(sim.shape)\n",
    "    #     results = (-sim[0]).argsort()[0:top_k]\n",
    "    #     print(results)\n",
    "        \n",
    "    #     sim_norm = probs_norm_similarity.eval()\n",
    "    #     print((-sim_norm[0]).argsort()[0:top_k])\n",
    "    \n",
    "        print(\"以下是给您的推荐：\")\n",
    "        p = np.squeeze(sim)\n",
    "        p[np.argsort(p)[:-top_k]] = 0\n",
    "        p = p / np.sum(p)\n",
    "        results = set()\n",
    "        while len(results) != 5:\n",
    "            c = np.random.choice(3883, 1, p=p)[0]\n",
    "            results.add(c)\n",
    "        for val in (results):\n",
    "            print(val)\n",
    "            print(movies_orig[val])\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zEkK6ksMSb_Y",
    "outputId": "ae1ad720-7b77-4f87-80eb-6753a5c67d0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save/\n",
      "以下是给您的推荐：\n",
      "1162\n",
      "[1178 'Paths of Glory (1957)' 'Drama|War']\n",
      "2263\n",
      "[2332 'Belly (1998)' 'Crime|Drama']\n",
      "1180\n",
      "[1198 'Raiders of the Lost Ark (1981)' 'Action|Adventure']\n",
      "1950\n",
      "[2019\n",
      " 'Seven Samurai (The Magnificent Seven) (Shichinin no samurai) (1954)'\n",
      " 'Action|Drama']\n",
      "735\n",
      "[745 'Close Shave, A (1995)' 'Animation|Comedy|Thriller']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{735, 1162, 1180, 1950, 2263}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_your_favorite_movie(234, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "TpXoYGVrSenn"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def recommend_other_favorite_movie(movie_id_val, top_k = 20):\n",
    "    loaded_graph = tf.Graph()  #\n",
    "    with tf.Session(graph=loaded_graph) as sess:  #\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "\n",
    "        probs_movie_embeddings = (movie_matrics[movieid2idx[movie_id_val]]).reshape([1, 200])\n",
    "        probs_user_favorite_similarity = tf.matmul(probs_movie_embeddings, tf.transpose(users_matrics))\n",
    "        favorite_user_id = np.argsort(probs_user_favorite_similarity.eval())[0][-top_k:]\n",
    "    #     print(normalized_users_matrics.eval().shape)\n",
    "    #     print(probs_user_favorite_similarity.eval()[0][favorite_user_id])\n",
    "    #     print(favorite_user_id.shape)\n",
    "    \n",
    "        print(\"您看的电影是：{}\".format(movies_orig[movieid2idx[movie_id_val]]))\n",
    "        \n",
    "        print(\"喜欢看这个电影的人是：{}\".format(users_orig[favorite_user_id-1]))\n",
    "        probs_users_embeddings = (users_matrics[favorite_user_id-1]).reshape([-1, 200])\n",
    "        probs_similarity = tf.matmul(probs_users_embeddings, tf.transpose(movie_matrics))\n",
    "        sim = (probs_similarity.eval())\n",
    "    #     results = (-sim[0]).argsort()[0:top_k]\n",
    "    #     print(results)\n",
    "    \n",
    "    #     print(sim.shape)\n",
    "    #     print(np.argmax(sim, 1))\n",
    "        p = np.argmax(sim, 1)\n",
    "        print(\"喜欢看这个电影的人还喜欢看：\")\n",
    "\n",
    "        results = set()\n",
    "        while len(results) != 3:\n",
    "            c = p[random.randrange(top_k)]\n",
    "            results.add(c)\n",
    "        for val in (results):\n",
    "            print(val)\n",
    "            print(movies_orig[val])\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fin4yZn9SiB2",
    "outputId": "9e540420-fd3d-4c5e-b28b-e15b47310656"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save/\n",
      "您看的电影是：[1401 'Ghosts of Mississippi (1996)' 'Drama']\n",
      "喜欢看这个电影的人是：[[3031 'M' 18 4]\n",
      " [4903 'M' 35 12]\n",
      " [4399 'F' 18 4]\n",
      " [5861 'F' 50 1]\n",
      " [5055 'F' 35 16]\n",
      " [1763 'M' 35 7]\n",
      " [3901 'M' 18 14]\n",
      " [1745 'M' 45 0]\n",
      " [4178 'F' 50 16]\n",
      " [2338 'M' 45 17]\n",
      " [5844 'F' 1 10]\n",
      " [2698 'F' 25 2]\n",
      " [2316 'M' 35 0]\n",
      " [3914 'M' 35 4]\n",
      " [1568 'F' 1 10]\n",
      " [2312 'M' 45 0]\n",
      " [4374 'M' 18 4]\n",
      " [542 'M' 18 4]\n",
      " [3780 'M' 1 0]\n",
      " [4085 'F' 25 6]]\n",
      "喜欢看这个电影的人还喜欢看：\n",
      "1162\n",
      "[1178 'Paths of Glory (1957)' 'Drama|War']\n",
      "901\n",
      "[913 'Maltese Falcon, The (1941)' 'Film-Noir|Mystery']\n",
      "735\n",
      "[745 'Close Shave, A (1995)' 'Animation|Comedy|Thriller']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{735, 901, 1162}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_other_favorite_movie(1401, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jW69MTZNUw42"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MovieRec.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:tf11]",
   "language": "python",
   "name": "conda-env-tf11-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
