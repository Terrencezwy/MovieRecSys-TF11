{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "T_5w6XOxBuuf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wxhxbt\\Anaconda3\\envs\\tf11\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\wxhxbt\\Anaconda3\\envs\\tf11\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\wxhxbt\\Anaconda3\\envs\\tf11\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\wxhxbt\\Anaconda3\\envs\\tf11\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\wxhxbt\\Anaconda3\\envs\\tf11\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\wxhxbt\\Anaconda3\\envs\\tf11\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# import warnings\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from tensorflow.python.ops import math_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vvMwnJIQCDDe"
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "import hashlib\n",
    "from bert_serving.client import BertClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "n3DzshZA621y"
   },
   "outputs": [],
   "source": [
    "# bc = BertClient(ip='192.168.86.41')\n",
    "# bc.encode(['First do it', 'then do it right', 'then do it better'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fNpGdfcE_Mp"
   },
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "gk7VRNaVCpC0"
   },
   "outputs": [],
   "source": [
    "def _unzip(save_path, _, database_name, data_path):\n",
    "    \"\"\"\n",
    "    Unzip wrapper with the same interface as _ungzip\n",
    "    :param save_path: The path of the gzip files\n",
    "    :param database_name: Name of database\n",
    "    :param data_path: Path to extract to\n",
    "    :param _: HACK - Used to have to same interface as _ungzip\n",
    "    \"\"\"\n",
    "    print('Extracting {}...'.format(database_name))\n",
    "    with zipfile.ZipFile(save_path) as zf:\n",
    "        zf.extractall(data_path)\n",
    "\n",
    "def download_extract(database_name, data_path):\n",
    "    \"\"\"\n",
    "    Download and extract database\n",
    "    :param database_name: Database name\n",
    "    \"\"\"\n",
    "    DATASET_ML1M = 'ml-1m'\n",
    "\n",
    "    if database_name == DATASET_ML1M:\n",
    "        url = 'http://files.grouplens.org/datasets/movielens/ml-1m.zip'\n",
    "        hash_code = 'c4d9eecfca2ab87c1945afe126590906'\n",
    "        extract_path = os.path.join(data_path, 'ml-1m')\n",
    "        save_path = os.path.join(data_path, 'ml-1m.zip')\n",
    "        extract_fn = _unzip\n",
    "\n",
    "    if os.path.exists(extract_path):\n",
    "        print('Found {} Data'.format(database_name))\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        with DLProgress(unit='B', unit_scale=True, miniters=1, desc='Downloading {}'.format(database_name)) as pbar:\n",
    "            urlretrieve(\n",
    "                url,\n",
    "                save_path,\n",
    "                pbar.hook)\n",
    "\n",
    "    assert hashlib.md5(open(save_path, 'rb').read()).hexdigest() == hash_code, \\\n",
    "        '{} file is corrupted.  Remove the file and try again.'.format(save_path)\n",
    "\n",
    "    os.makedirs(extract_path)\n",
    "    try:\n",
    "        extract_fn(save_path, extract_path, database_name, data_path)\n",
    "    except Exception as err:\n",
    "        shutil.rmtree(extract_path)  # Remove extraction folder if there is an error\n",
    "        raise err\n",
    "\n",
    "    print('Done.')\n",
    "    # Remove compressed data\n",
    "#     os.remove(save_path)\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    \"\"\"\n",
    "    Handle Progress Bar while Downloading\n",
    "    \"\"\"\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        \"\"\"\n",
    "        A hook function that will be called once on establishment of the network connection and\n",
    "        once after each block read thereafter.\n",
    "        :param block_num: A count of blocks transferred so far\n",
    "        :param block_size: Block size in bytes\n",
    "        :param total_size: The total size of the file. This may be -1 on older FTP servers which do not return\n",
    "                            a file size in response to a retrieval request.\n",
    "        \"\"\"\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RnVpYF28CxtL",
    "outputId": "724d4e58-62a5-4c48-d531-4b79d36d7f24"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading ml-1m: 5.92MB [00:01, 5.65MB/s]                                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ml-1m...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "data_dir = './'\n",
    "download_extract('ml-1m', data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "gmzh6S3AC-YS",
    "outputId": "698fdd22-a32f-4a2c-e4b5-f3db9d7668a7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>OccupationID</th>\n",
       "      <th>Zip-code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>56</td>\n",
       "      <td>16</td>\n",
       "      <td>70072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>55117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>02460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>55455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID Gender  Age  OccupationID Zip-code\n",
       "0       1      F    1            10    48067\n",
       "1       2      M   56            16    70072\n",
       "2       3      M   25            15    55117\n",
       "3       4      M   45             7    02460\n",
       "4       5      M   25            20    55455"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_title = ['UserID', 'Gender', 'Age', 'OccupationID', 'Zip-code']\n",
    "users = pd.read_table('./ml-1m/users.dat', sep='::', header=None, names=users_title, engine = 'python')\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "723u-UiwDpPc",
    "outputId": "e33d32c6-7ad1-4220-cebe-ff929aeba8f2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Animation|Children's|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children's|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MovieID                               Title                        Genres\n",
       "0        1                    Toy Story (1995)   Animation|Children's|Comedy\n",
       "1        2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
       "2        3             Grumpier Old Men (1995)                Comedy|Romance\n",
       "3        4            Waiting to Exhale (1995)                  Comedy|Drama\n",
       "4        5  Father of the Bride Part II (1995)                        Comedy"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_title = ['MovieID', 'Title', 'Genres']\n",
    "movies = pd.read_table('./ml-1m/movies.dat', sep='::', header=None, names=movies_title, engine = 'python')\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "DFVtdqDeDr1S",
    "outputId": "7bdd2519-6727-4036-e235-66c1abfa759f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>timestamps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  MovieID  Rating  timestamps\n",
       "0       1     1193       5   978300760\n",
       "1       1      661       3   978302109\n",
       "2       1      914       3   978301968\n",
       "3       1     3408       4   978300275\n",
       "4       1     2355       5   978824291"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_title = ['UserID','MovieID', 'Rating', 'timestamps']\n",
    "ratings = pd.read_table('./ml-1m/ratings.dat', sep='::', header=None, names=ratings_title, engine = 'python')\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AlEEMKiMDuca"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "JNOc9HRnE8Zc"
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load Dataset from File\n",
    "    \"\"\"\n",
    "    #读取User数据\n",
    "    users_title = ['UserID', 'Gender', 'Age', 'JobID', 'Zip-code']\n",
    "    users = pd.read_table('./ml-1m/users.dat', sep='::', header=None, names=users_title, engine = 'python')\n",
    "    users = users.filter(regex='UserID|Gender|Age|JobID')\n",
    "    users_orig = users.values\n",
    "    #改变User数据中性别和年龄\n",
    "    gender_map = {'F':0, 'M':1}\n",
    "    users['Gender'] = users['Gender'].map(gender_map)\n",
    "\n",
    "    age_map = {val:ii for ii,val in enumerate(set(users['Age']))}\n",
    "    users['Age'] = users['Age'].map(age_map)\n",
    "\n",
    "    #读取Movie数据集\n",
    "    movies_title = ['MovieID', 'Title', 'Genres']\n",
    "    movies = pd.read_table('./ml-1m/movies.dat', sep='::', header=None, names=movies_title, engine = 'python')\n",
    "    movies_orig = movies.values\n",
    "    #将Title中的年份去掉\n",
    "    pattern = re.compile(r'^(.*)\\((\\d+)\\)$')\n",
    "\n",
    "    title_map = {val:pattern.match(val).group(1) for ii,val in enumerate(set(movies['Title']))}\n",
    "    movies['Title'] = movies['Title'].map(title_map)\n",
    "\n",
    "    #电影类型转数字字典\n",
    "    genres_set = set()\n",
    "    for val in movies['Genres'].str.split('|'):\n",
    "        genres_set.update(val)\n",
    "\n",
    "    genres_set.add('<PAD>')\n",
    "    genres2int = {val:ii for ii, val in enumerate(genres_set)}\n",
    "\n",
    "    #将电影类型转成等长数字列表，长度是18\n",
    "    genres_map = {val:[genres2int[row] for row in val.split('|')] for ii,val in enumerate(set(movies['Genres']))}\n",
    "\n",
    "    for key in genres_map:\n",
    "        for cnt in range(max(genres2int.values()) - len(genres_map[key])):\n",
    "            genres_map[key].insert(len(genres_map[key]) + cnt,genres2int['<PAD>'])\n",
    "    \n",
    "    movies['Genres'] = movies['Genres'].map(genres_map)\n",
    "    \n",
    "    # Get the extracted movie title feature from bert\n",
    "    if os.path.exists(\"bert_encoded_title.p\"):\n",
    "        movies['Title'] = pickle.load(open('bert_encoded_title.p', mode='rb'))\n",
    "    else:\n",
    "        bc = BertClient()\n",
    "        title_map = {val:bc.encode([val])[0] for ii,val in enumerate(set(movies['Title']))}\n",
    "        movies['Title'] = movies['Title'].map(title_map)\n",
    "\n",
    "    #读取评分数据集\n",
    "    ratings_title = ['UserID','MovieID', 'ratings', 'timestamps']\n",
    "    ratings = pd.read_table('./ml-1m/ratings.dat', sep='::', header=None, names=ratings_title, engine = 'python')\n",
    "    ratings = ratings.filter(regex='UserID|MovieID|ratings')\n",
    "\n",
    "    #合并三个表\n",
    "    data = pd.merge(pd.merge(ratings, users), movies)\n",
    "    \n",
    "    #将数据分成X和y两张表\n",
    "    target_fields = ['ratings']\n",
    "    features_pd, targets_pd = data.drop(target_fields, axis=1), data[target_fields]\n",
    "    \n",
    "    features = features_pd.values\n",
    "    targets_values = targets_pd.values\n",
    "    \n",
    "    return genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "QzjGYIt1ODN2"
   },
   "outputs": [],
   "source": [
    "# pickle.dump(movies['Title'], open('bert_encoded_title.p', 'wb'))\n",
    "genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig = load_data()\n",
    "\n",
    "pickle.dump((genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig), open('preprocess2.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "BuLcPs1cOG2s",
    "outputId": "16652fc9-8ac9-4489-aa7b-bd56d0069254"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>JobID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  Gender  Age  JobID\n",
       "0       1       0    0     10\n",
       "1       2       1    5     16\n",
       "2       3       1    6     15\n",
       "3       4       1    2      7\n",
       "4       5       1    6     20"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "PKkyq_XNObyz",
    "outputId": "ad734e99-da1b-4c36-817c-a55048df4c4a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[-0.5726612, -0.3172742, -0.20371202, -0.20670...</td>\n",
       "      <td>[17, 0, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[-0.36613265, -0.865088, -0.26034635, -0.24028...</td>\n",
       "      <td>[18, 0, 7, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[-0.3607318, 0.0059039528, 0.066141166, 0.0151...</td>\n",
       "      <td>[6, 8, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[0.40832582, -0.1697424, 0.13474943, -0.193436...</td>\n",
       "      <td>[6, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[-0.42645, -0.5103114, 0.21123752, -0.30739355...</td>\n",
       "      <td>[6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MovieID                                              Title  \\\n",
       "0        1  [-0.5726612, -0.3172742, -0.20371202, -0.20670...   \n",
       "1        2  [-0.36613265, -0.865088, -0.26034635, -0.24028...   \n",
       "2        3  [-0.3607318, 0.0059039528, 0.066141166, 0.0151...   \n",
       "3        4  [0.40832582, -0.1697424, 0.13474943, -0.193436...   \n",
       "4        5  [-0.42645, -0.5103114, 0.21123752, -0.30739355...   \n",
       "\n",
       "                                              Genres  \n",
       "0  [17, 0, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,...  \n",
       "1  [18, 0, 7, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,...  \n",
       "2  [6, 8, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...  \n",
       "3  [6, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...  \n",
       "4  [6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>ratings</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>JobID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>[-0.038056023, 0.23439522, -0.35930735, -0.099...</td>\n",
       "      <td>[1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>[-0.038056023, 0.23439522, -0.35930735, -0.099...</td>\n",
       "      <td>[1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>1193</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>[-0.038056023, 0.23439522, -0.35930735, -0.099...</td>\n",
       "      <td>[1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>1193</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>[-0.038056023, 0.23439522, -0.35930735, -0.099...</td>\n",
       "      <td>[1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.038056023, 0.23439522, -0.35930735, -0.099...</td>\n",
       "      <td>[1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  MovieID  ratings  Gender  Age  JobID  \\\n",
       "0       1     1193        5       0    0     10   \n",
       "1       2     1193        5       1    5     16   \n",
       "2      12     1193        4       1    6     12   \n",
       "3      15     1193        4       1    6      7   \n",
       "4      17     1193        5       1    3      1   \n",
       "\n",
       "                                               Title  \\\n",
       "0  [-0.038056023, 0.23439522, -0.35930735, -0.099...   \n",
       "1  [-0.038056023, 0.23439522, -0.35930735, -0.099...   \n",
       "2  [-0.038056023, 0.23439522, -0.35930735, -0.099...   \n",
       "3  [-0.038056023, 0.23439522, -0.35930735, -0.099...   \n",
       "4  [-0.038056023, 0.23439522, -0.35930735, -0.099...   \n",
       "\n",
       "                                              Genres  \n",
       "0  [1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...  \n",
       "1  [1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...  \n",
       "2  [1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...  \n",
       "3  [1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...  \n",
       "4  [1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1193, 0, 0, 10,\n",
       "       array([-3.80560234e-02,  2.34395221e-01, -3.59307349e-01, -9.91238579e-02,\n",
       "        3.66178542e-01, -3.60790268e-02, -3.86045098e-01,  5.24232209e-01,\n",
       "       -7.15995848e-01,  2.62239516e-01,  4.61214483e-01, -6.93597555e-01,\n",
       "       -1.28532618e-01,  1.35980338e-01, -7.29559779e-01,  2.81656832e-01,\n",
       "       -1.19447216e-01,  1.53474420e-01,  3.53073210e-01, -2.35526860e-02,\n",
       "       -2.17893720e-01,  2.61140764e-01, -7.88218081e-01, -6.02616549e-01,\n",
       "        3.94233316e-01,  1.17122039e-01, -3.51624675e-02,  1.71095893e-01,\n",
       "       -6.76342705e-03,  7.52976313e-02, -1.88600451e-01,  7.39003658e-01,\n",
       "        5.34284711e-01, -1.55626252e-01, -7.20756710e-01, -2.04829603e-01,\n",
       "        6.61011040e-02, -2.02436328e-01, -7.01153636e-01, -3.23604494e-01,\n",
       "       -1.34631082e-01, -7.16558993e-01,  4.57473665e-01, -3.55783641e-01,\n",
       "        4.04813439e-01,  1.76159069e-01, -3.78521055e-01,  1.83217019e-01,\n",
       "        3.60047311e-01, -2.58149415e-01, -1.87774494e-01,  6.07944071e-01,\n",
       "       -3.53238493e-01, -2.81538755e-01, -1.95152014e-01,  8.22252452e-01,\n",
       "        3.62765342e-01, -3.99966203e-02,  2.74232179e-01,  2.09056169e-01,\n",
       "       -6.12906992e-01, -4.77485746e-01,  2.26669520e-01, -6.33058720e-04,\n",
       "       -6.31924570e-02,  3.17111641e-01,  3.56703132e-01,  1.29981965e-01,\n",
       "       -5.22667229e-01, -3.25943306e-02, -1.38383776e-01, -2.11110622e-01,\n",
       "        2.32832164e-01,  2.68755466e-01, -3.02859515e-01, -3.64783823e-01,\n",
       "       -3.47708523e-01,  3.50544065e-01, -4.47371662e-01, -3.91794264e-01,\n",
       "       -3.21357325e-02,  3.59203786e-01,  3.68574321e-01,  1.80333048e-01,\n",
       "        4.91972685e-01,  2.50064731e-01,  1.05279520e-01,  3.86266075e-02,\n",
       "       -5.46322465e-01,  2.12253898e-01, -6.69814706e-01, -5.75522244e-01,\n",
       "       -1.14091970e-01,  3.27664584e-01, -2.77689278e-01, -1.00901984e-01,\n",
       "       -3.69283199e-01, -2.74709523e-01,  2.74913669e-01, -1.12653255e-01,\n",
       "        8.17293897e-02,  5.94765618e-02,  5.58876276e-01,  2.85319835e-01,\n",
       "       -6.99774384e-01, -5.88964410e-02, -7.04844594e-02,  1.00990988e-01,\n",
       "        8.33897516e-02, -8.61088783e-02,  2.81650662e-01, -5.12799084e-01,\n",
       "        1.37773484e-01,  2.85668463e-01, -1.50698349e-01,  7.98101783e-01,\n",
       "        2.57755250e-01, -5.93771398e-01,  1.25199959e-01, -3.45255196e-01,\n",
       "        1.92214563e-01,  1.62930802e-01,  1.58656035e-02,  2.72110756e-02,\n",
       "       -8.92935246e-02, -1.51059180e-01,  2.19225124e-01, -2.44027928e-01,\n",
       "        6.23419397e-02, -2.02287100e-02,  2.17199206e-01,  4.99660730e-01,\n",
       "       -3.02205503e-01, -6.35718226e-01,  7.93876886e-01,  3.49509418e-01,\n",
       "       -4.37837020e-02, -1.43539552e-02, -6.28107429e-01,  6.42109215e-01,\n",
       "       -4.13213074e-01,  5.80401830e-02,  3.99984300e-01, -2.32380088e-02,\n",
       "        4.56471354e-01,  4.76275012e-02, -4.86631483e-01, -1.86758056e-01,\n",
       "        2.96919167e-01,  1.40156120e-01,  2.61933893e-01,  2.39765853e-01,\n",
       "       -4.33234751e-01,  2.51031697e-01, -6.58207297e-01, -5.51873207e-01,\n",
       "       -1.54677868e-01, -5.87981120e-02,  3.10368389e-01,  5.30860364e-01,\n",
       "        2.42506221e-01, -6.40628219e-01,  3.53004962e-01,  1.33923069e-01,\n",
       "       -4.97273594e-01,  1.51547998e-01,  3.99304703e-02,  2.34538391e-01,\n",
       "       -5.10236859e-01, -1.56672090e-01, -5.27907796e-02,  3.16390187e-01,\n",
       "        5.43463349e-01,  1.85071304e-01,  2.51211733e-01, -2.41618287e-02,\n",
       "        2.41634458e-01, -4.02718961e-01,  3.29252809e-01,  9.26128387e-01,\n",
       "       -8.99922073e-01, -3.96611452e-01, -5.16637154e-02,  1.84212804e-01,\n",
       "        2.07391620e-01,  4.13239866e-01,  8.65778476e-02, -1.55468449e-01,\n",
       "        4.80038747e-02,  1.07494451e-01, -1.72896147e-01,  1.16144575e-01,\n",
       "       -1.72842294e-01, -3.50032538e-01, -4.51604187e-01, -4.99037087e-01,\n",
       "       -3.25044878e-02, -9.64565456e-01, -1.88289192e-02, -3.67262363e-01,\n",
       "       -2.28866339e-01,  7.32505992e-02, -4.40260097e-02,  1.36061281e-01,\n",
       "       -2.89153636e-01,  6.79713368e-01, -5.17297804e-01, -4.35955346e-01,\n",
       "       -6.99602902e-01,  6.32172599e-02, -5.93000948e-01,  7.75419176e-01,\n",
       "       -5.52350953e-02,  7.04082698e-02, -1.45234670e-02,  1.17527306e-01,\n",
       "       -3.91666293e-01, -3.15418094e-01, -1.75247386e-01, -5.16176149e-02,\n",
       "       -3.71755451e-01,  3.30250919e-01, -2.14513466e-01, -5.07155895e-01,\n",
       "        6.28452450e-02,  5.51569343e-01,  4.29834068e-01, -7.67063141e-01,\n",
       "        5.03590889e-02,  4.49924134e-02, -3.95087332e-01, -2.16951251e-01,\n",
       "        2.70032465e-01,  2.49160737e-01, -2.71052182e-01,  3.57925259e-02,\n",
       "       -3.72424424e-01, -3.52563292e-01,  4.47336763e-01,  1.50154158e-01,\n",
       "       -3.48360389e-01,  1.85385495e-01,  3.14029098e-01,  7.00523615e-01,\n",
       "        4.64008898e-01, -7.11600929e-02,  1.93902940e-01, -5.11760831e-01,\n",
       "        1.11970054e-02, -1.61937803e-01,  3.23958963e-01, -5.47107309e-04,\n",
       "       -2.50806808e-01, -3.32838744e-01, -3.92050952e-01, -3.82289916e-01,\n",
       "       -2.49204919e-01,  2.73849905e-01, -1.71868339e-01, -1.30067319e-01,\n",
       "        1.49330303e-01, -1.14265956e-01,  9.31314766e-01,  2.34570522e-02,\n",
       "       -9.89965070e-03, -2.41996318e-01, -2.39012595e-02, -4.76305902e-01,\n",
       "        3.97746146e-01, -6.22294247e-01,  2.88904645e-02, -2.34676339e-02,\n",
       "        2.49147207e-01,  3.74395907e-01, -5.09732068e-02, -4.41416264e-01,\n",
       "       -2.48141617e-01, -1.00044802e-01,  7.44283956e-04, -2.79299557e-01,\n",
       "        3.67895812e-01, -1.25330929e-02, -1.11345983e+00,  5.90355508e-02,\n",
       "        7.15619504e-01,  6.12749979e-02,  2.64672548e-01,  5.92921451e-02,\n",
       "       -2.70262752e-02, -3.35393637e-01, -3.97520289e-02,  4.53910679e-01,\n",
       "        2.82404065e-01, -6.26946688e-02, -1.39028728e-01,  2.00202823e-01,\n",
       "       -1.63596794e-01, -1.85773134e-01,  3.52584481e-01, -6.66813910e-01,\n",
       "        4.19719607e-01,  2.83002704e-01, -6.60485148e-01, -2.47511029e-01,\n",
       "       -1.52679498e-03, -7.90114477e-02,  3.47757906e-01, -4.00076807e-01,\n",
       "       -6.73738623e+00, -3.78804915e-02,  2.37728402e-01, -2.74064630e-01,\n",
       "        6.71377361e-01,  1.26392946e-01, -5.48622131e-01,  7.91851059e-02,\n",
       "       -3.01671416e-01,  3.68571468e-02,  1.59849614e-01, -6.71993732e-01,\n",
       "        3.97201627e-01,  4.31710258e-02,  6.88777089e-01, -1.15410638e+00,\n",
       "       -1.87441021e-01, -5.63398898e-01, -3.27651426e-02,  4.71952837e-03,\n",
       "       -4.08377796e-01, -9.04438645e-02,  4.42534924e-01, -3.27795208e-01,\n",
       "        8.82304758e-02, -1.82002261e-01,  3.83543313e-01, -4.83490005e-02,\n",
       "       -1.39209237e-02,  7.04308599e-02, -3.77718955e-01, -2.22607285e-01,\n",
       "       -1.00557208e-01,  4.79763985e-01, -6.64033413e-01,  2.12889105e-01,\n",
       "       -1.70388624e-01,  1.15759328e-01,  3.92913818e-01,  4.73300517e-01,\n",
       "        5.15316010e-01, -6.34696543e-01, -1.84014261e-01,  4.27746862e-01,\n",
       "        5.13831019e-01,  3.42828147e-02,  8.85989610e-03, -2.12479234e-01,\n",
       "        2.19007060e-01,  7.13866055e-02,  1.34301513e-01,  2.12803885e-01,\n",
       "        5.26601791e-01, -6.73481643e-01, -1.54357851e-01, -4.31435496e-01,\n",
       "        4.68977205e-02,  4.40146059e-01,  5.53402379e-02, -4.87338632e-01,\n",
       "        2.09355742e-01, -2.39901811e-01, -4.03786331e-01,  3.72932017e-01,\n",
       "        1.81830168e-01, -1.33626610e-01, -6.94763720e-01,  3.31298798e-01,\n",
       "        6.77796677e-02,  1.69857770e-01, -3.61327454e-02, -4.33159351e-01,\n",
       "        4.05461073e-01, -1.42286181e+00, -2.61706442e-01, -1.00105237e-02,\n",
       "       -2.27307752e-01, -1.11168787e-01,  1.16963588e-01, -3.98341388e-01,\n",
       "       -3.67794693e-01, -4.52307463e-01,  2.04797074e-01, -2.69941390e-01,\n",
       "       -1.29696424e-03, -5.16899228e-01, -5.71674049e-01,  6.02145717e-02,\n",
       "       -4.94068861e-01, -2.77367443e-01, -1.69786483e-01, -3.82663235e-02,\n",
       "       -1.51044782e-02,  6.44575953e-01,  2.82698452e-01,  2.13517949e-01,\n",
       "        7.86208659e-02, -6.40054762e-01,  3.47002983e-01,  1.46864001e-02,\n",
       "        1.14651278e-01,  3.26563179e-01,  7.77489990e-02,  3.63383234e-01,\n",
       "        1.08364429e-02, -8.76530558e-02, -1.74072057e-01, -1.25220999e-01,\n",
       "       -3.42105329e-01,  5.61828613e-01,  6.13388836e-01, -7.44496107e-01,\n",
       "        4.25298542e-01, -6.17364459e-02, -9.74348903e-01, -2.84754276e-01,\n",
       "        7.48060584e-01,  5.70975363e-01,  6.64812550e-02,  1.29895568e-01,\n",
       "       -3.50123197e-01, -2.31552407e-01,  5.45577817e-02, -3.32525223e-01,\n",
       "       -1.88270971e-01,  4.60765064e-01,  9.83042270e-02, -1.31214082e-01,\n",
       "       -2.03082651e-01, -3.50457996e-01, -5.45527518e-01,  2.19974965e-01,\n",
       "       -9.83955339e-04,  9.59698036e-02, -2.49039501e-01,  1.33652523e-01,\n",
       "       -4.13402259e-01, -4.95818913e-01, -1.46406204e-01,  3.48881721e-01,\n",
       "       -5.38094435e-03,  6.16233468e-01,  6.35387659e-01,  4.44250368e-02,\n",
       "        2.15152070e-01, -3.01474184e-01,  1.23408221e-01, -3.28936368e-01,\n",
       "        2.69722760e-01,  2.84177631e-01, -3.75699371e-01,  5.14839180e-02,\n",
       "        2.84071952e-01, -1.28283754e-01,  1.14690438e-02,  1.57106370e-01,\n",
       "        7.40139961e-01, -3.03606361e-01,  6.32308796e-02, -1.13906786e-01,\n",
       "       -9.95494947e-02,  4.41013165e-02,  1.75638683e-02,  7.77007878e-01,\n",
       "       -3.35847437e-02,  4.20080423e-01,  3.28470677e-01, -1.06458943e-02,\n",
       "       -1.79430127e-01, -2.76047260e-01, -4.52770889e-02,  1.49480850e-01,\n",
       "       -1.88298687e-01,  6.80891275e-01, -3.75771046e-01,  5.17052054e-01,\n",
       "        1.23549066e-01, -4.31909859e-01, -3.26607883e-01,  1.14278591e+00,\n",
       "       -5.81989717e-03,  3.28871906e-01, -2.67047942e-01, -5.56715429e-01,\n",
       "       -3.89034227e-02, -1.33210957e-01,  1.16845652e-01, -3.05433095e-01,\n",
       "       -2.37366604e-03,  5.98162226e-02, -4.46183920e-01,  1.48821369e-01,\n",
       "       -5.18058360e-01, -5.38319230e-01, -7.75258482e-01, -2.70006001e-01,\n",
       "        1.04080118e-01,  3.46861392e-01,  1.39586002e-01,  2.36961693e-02,\n",
       "       -2.98234940e-01, -9.16976810e-01,  1.80369183e-01,  4.10365492e-01,\n",
       "       -2.65639842e-01, -9.31560546e-02,  2.11528778e-01, -2.96688974e-02,\n",
       "       -4.01320085e-02,  2.26696521e-01,  4.59364131e-02, -5.91784954e-01,\n",
       "       -4.27335322e-01, -5.66089332e-01,  4.38813120e-01,  3.84654760e-01,\n",
       "       -2.91048884e-01,  3.07529598e-01, -1.80186480e-01,  5.62208705e-02,\n",
       "       -7.29082823e-02, -1.57645300e-01,  4.01473977e-02, -2.20400527e-01,\n",
       "       -9.10411656e-01, -2.02897221e-01,  4.37490851e-01,  1.93248361e-01,\n",
       "       -3.70678484e-01, -6.15994811e-01, -9.81059000e-02, -3.72761369e-01,\n",
       "        6.29390627e-02,  1.96247727e-01, -2.46472508e-01, -2.07298160e-01,\n",
       "       -3.23954672e-01, -5.68108976e-01, -1.87617123e-01,  3.17451417e-01,\n",
       "        1.54382363e-01, -2.17958167e-01,  2.44894385e-01, -1.68456450e-01,\n",
       "       -2.79472321e-01, -1.01414204e-01, -3.94915670e-01,  1.89736754e-01,\n",
       "        9.05774981e-02, -2.57910430e-01, -1.65784270e-01,  3.24305475e-01,\n",
       "        4.01101589e-01,  2.92292416e-01, -1.00846924e-01, -2.91490734e-01,\n",
       "       -7.15399235e-02,  4.14562859e-02,  2.52123833e-01, -2.72988081e-01,\n",
       "       -9.78743434e-02,  9.29829851e-02, -1.19279400e-01,  9.25260246e-01,\n",
       "       -2.90764868e-01, -5.02155542e-01,  1.24759778e-01,  7.66309977e-01,\n",
       "       -1.67118788e-01,  1.31713878e-02, -3.85345817e-02, -3.52422982e-01,\n",
       "        1.56204969e-01, -1.63907096e-01,  3.34509164e-01,  6.33975491e-02,\n",
       "       -1.13604382e-01, -2.23974541e-01,  1.53764680e-01,  4.84611809e-01,\n",
       "        2.04845905e-01, -5.29863052e-02,  4.91557531e-02, -2.16797397e-01,\n",
       "       -4.99320120e-01,  2.86491066e-01, -9.45770442e-02,  6.82671010e-01,\n",
       "        3.05054903e-01, -4.61343341e-02,  2.50696421e-01,  1.52724087e-01,\n",
       "       -2.54513860e-01,  3.32629472e-01,  5.45174241e-01,  3.20515007e-01,\n",
       "       -2.75908411e-02,  2.79914379e-01, -4.72477913e-01, -4.03565876e-02,\n",
       "       -8.86640191e-01, -3.65066499e-01,  5.38982093e-01,  7.20875710e-02,\n",
       "       -3.82643014e-01,  3.76742631e-01, -6.61705613e-01,  1.73488818e-02,\n",
       "       -6.62837267e-01, -2.42599994e-01,  1.37624115e-01,  1.61427289e-01,\n",
       "        5.27173951e-02,  6.21869743e-01, -3.79086956e-02,  4.71770227e-01,\n",
       "        9.01969522e-02, -9.72838253e-02, -3.97653669e-01,  7.38875508e-01,\n",
       "        5.48402593e-02,  8.52455013e-03,  8.06631446e-01, -2.18659133e-01,\n",
       "        1.85380965e-01,  3.41570075e-03, -1.13304794e-01,  3.78617197e-01,\n",
       "       -3.74649674e-01, -2.39401430e-01, -4.12206709e-01,  2.64010549e-01,\n",
       "       -1.14951637e-02, -6.77231699e-02, -3.15545619e-01, -2.38231495e-01,\n",
       "        3.22855979e-01, -6.83786348e-02, -1.02534756e-01, -1.51994377e-01,\n",
       "        7.74280727e-01, -1.06957056e-01,  3.15762550e-01,  4.68655914e-01,\n",
       "        1.99717402e-01, -4.36272234e-01,  7.73158818e-02,  3.37193429e-01,\n",
       "        3.42073202e-01,  2.74434119e-01, -2.14826107e-01,  1.34838372e-01,\n",
       "        1.87823564e-01, -6.82677269e-01,  1.09768546e+00, -3.82789075e-01,\n",
       "        2.80633748e-01,  2.14505821e-01,  1.73328489e-01,  8.31253648e-01,\n",
       "        2.88154893e-02,  3.46134812e-01, -2.91186690e-01, -1.58837646e-01,\n",
       "       -3.32994550e-01,  2.92381585e-01, -1.19786654e-02, -1.39594510e-01,\n",
       "       -5.19217312e-01, -6.27164364e-01,  9.63087201e-01, -5.88389516e-01,\n",
       "        3.41242731e-01, -1.24439992e-01,  2.53172845e-01,  8.85248929e-02,\n",
       "        1.91785824e-02, -7.28484988e-02,  3.97018611e-01,  2.55401939e-01,\n",
       "       -3.44254106e-01, -5.07261693e-01, -4.28037077e-01, -4.09034193e-01,\n",
       "        1.62116438e-01,  1.95662618e-01, -4.90945093e-02, -3.56207162e-01,\n",
       "        1.00403503e-01, -3.47507656e-01, -2.76412480e-02,  1.65174097e-01,\n",
       "        1.18875086e-01,  3.04638207e-01,  3.47359061e-01, -2.12942511e-01,\n",
       "        2.74935722e-01, -1.31169870e-01, -4.81117107e-02,  3.29350308e-02,\n",
       "        7.98495591e-01,  1.77996675e-03, -3.83093178e-01, -2.36293208e-02,\n",
       "       -1.09690502e-01, -4.01202768e-01, -2.08329689e-02,  2.43161514e-01,\n",
       "       -4.59991276e-01,  7.34844655e-02,  5.55327296e-01,  5.78356862e-01,\n",
       "       -1.71484411e-01,  6.42317086e-02, -1.08415306e-01,  4.31532525e-02,\n",
       "        3.72666359e-01, -5.83562329e-02, -2.46666402e-01, -4.05326843e-01,\n",
       "       -4.50702965e-01, -2.62830377e-01,  2.12246329e-01, -1.07463375e-01,\n",
       "        1.91326320e-01, -3.60064000e-01,  2.74377555e-01,  6.03385270e-02,\n",
       "       -4.55344375e-03,  3.23883682e-01, -3.34175169e-01,  2.20362857e-01,\n",
       "        3.05965185e-01, -8.35546851e-01, -5.71692288e-01,  1.42884940e-01,\n",
       "       -9.38852802e-02,  1.20878205e-01, -3.75050791e-02, -1.23980463e-01,\n",
       "        2.07606763e-01,  1.99963376e-01,  4.16068034e-03, -2.27059215e-01,\n",
       "        2.09013186e-02, -4.02247101e-01,  2.24829957e-01,  1.62739605e-01,\n",
       "       -1.45929009e-01, -4.08451140e-01,  2.06067368e-01, -7.28329644e-03,\n",
       "        3.81950364e-02, -4.60458621e-02,  1.14600554e-01, -3.71929780e-02,\n",
       "       -1.44720361e-01,  5.89961037e-02,  1.46263748e-01, -5.22129647e-02],\n",
       "      dtype=float32),\n",
       "       list([1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5],\n",
       "       [5],\n",
       "       [4],\n",
       "       ...,\n",
       "       [1],\n",
       "       [5],\n",
       "       [4]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "QUAkoboqOtyT"
   },
   "outputs": [],
   "source": [
    "genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig = pickle.load(open('preprocess2.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZoL5khmrOwK7"
   },
   "outputs": [],
   "source": [
    "def save_params(params):\n",
    "    \"\"\"\n",
    "    Save parameters to file\n",
    "    \"\"\"\n",
    "    pickle.dump(params, open('params2.p', 'wb'))\n",
    "\n",
    "\n",
    "def load_params():\n",
    "    \"\"\"\n",
    "    Load parameters from file\n",
    "    \"\"\"\n",
    "    return pickle.load(open('params2.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "iStTncQMO9DY"
   },
   "outputs": [],
   "source": [
    "#嵌入矩阵的维度\n",
    "embed_dim = 32\n",
    "#用户ID个数\n",
    "uid_max = max(features.take(0,1)) + 1 # 6040\n",
    "#性别个数\n",
    "gender_max = max(features.take(2,1)) + 1 # 1 + 1 = 2\n",
    "#年龄类别个数\n",
    "age_max = max(features.take(3,1)) + 1 # 6 + 1 = 7\n",
    "#职业个数\n",
    "job_max = max(features.take(4,1)) + 1# 20 + 1 = 21\n",
    "\n",
    "#电影ID个数\n",
    "movie_id_max = max(features.take(1,1)) + 1 # 3952\n",
    "#电影类型个数\n",
    "movie_categories_max = max(genres2int.values()) + 1 # 18 + 1 = 19\n",
    "#电影名单词个数\n",
    "# movie_title_max = len(title_set) # 5216\n",
    "\n",
    "#对电影类型嵌入向量做加和操作的标志，考虑过使用mean做平均，但是没实现mean\n",
    "combiner = \"sum\"\n",
    "\n",
    "#电影名长度\n",
    "# sentences_size = title_count # = 15\n",
    "sentences_size = movies.Title.iloc[0].shape[0] # 768\n",
    "#文本卷积滑动窗口，分别滑动2, 3, 4, 5个单词\n",
    "window_sizes = {2, 3, 4, 5}\n",
    "#文本卷积核数量\n",
    "filter_num = 8\n",
    "\n",
    "#电影ID转下标的字典，数据集中电影ID跟下标不一致，比如第5行的数据电影ID不一定是5\n",
    "movieid2idx = {val[0]:i for i, val in enumerate(movies.values)}\n",
    "idx2movieid = {i:val[0] for i, val in enumerate(movies.values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_targets, test_targets = train_test_split(features, targets_values, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "PA4zKCV4PAK7"
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 5\n",
    "# Batch Size\n",
    "batch_size = 256\n",
    "\n",
    "dropout_keep = 0.5\n",
    "# Learning Rate\n",
    "learning_rate = 0.0001\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 20\n",
    "\n",
    "save_dir = './save2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ALEKmeH-PCYT"
   },
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    uid = tf.placeholder(tf.int32, [None, 1], name=\"uid\")\n",
    "    user_gender = tf.placeholder(tf.int32, [None, 1], name=\"user_gender\")\n",
    "    user_age = tf.placeholder(tf.int32, [None, 1], name=\"user_age\")\n",
    "    user_job = tf.placeholder(tf.int32, [None, 1], name=\"user_job\")\n",
    "    \n",
    "    movie_id = tf.placeholder(tf.int32, [None, 1], name=\"movie_id\")\n",
    "    movie_categories = tf.placeholder(tf.int32, [None, 18], name=\"movie_categories\")\n",
    "#     movie_titles = tf.placeholder(tf.int32, [None, 15], name=\"movie_titles\")\n",
    "    movie_titles = tf.placeholder(tf.float32, [None, 768], name=\"movie_titles\")\n",
    "    targets = tf.placeholder(tf.int32, [None, 1], name=\"targets\")\n",
    "    LearningRate = tf.placeholder(tf.float32, name = \"LearningRate\")\n",
    "    dropout_keep_prob = tf.placeholder(tf.float32, name = \"dropout_keep_prob\")\n",
    "    return uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, LearningRate, dropout_keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Dn3V2lZjPFN8"
   },
   "outputs": [],
   "source": [
    "def get_user_embedding(uid, user_gender, user_age, user_job):\n",
    "    with tf.name_scope(\"user_embedding\"):\n",
    "        uid_embed_matrix = tf.Variable(tf.random_uniform([uid_max, embed_dim], -1, 1), name = \"uid_embed_matrix\")\n",
    "        uid_embed_layer = tf.nn.embedding_lookup(uid_embed_matrix, uid, name = \"uid_embed_layer\")\n",
    "    \n",
    "        gender_embed_matrix = tf.Variable(tf.random_uniform([gender_max, embed_dim // 2], -1, 1), name= \"gender_embed_matrix\")\n",
    "        gender_embed_layer = tf.nn.embedding_lookup(gender_embed_matrix, user_gender, name = \"gender_embed_layer\")\n",
    "        \n",
    "        age_embed_matrix = tf.Variable(tf.random_uniform([age_max, embed_dim // 2], -1, 1), name=\"age_embed_matrix\")\n",
    "        age_embed_layer = tf.nn.embedding_lookup(age_embed_matrix, user_age, name=\"age_embed_layer\")\n",
    "        \n",
    "        job_embed_matrix = tf.Variable(tf.random_uniform([job_max, embed_dim // 2], -1, 1), name = \"job_embed_matrix\")\n",
    "        job_embed_layer = tf.nn.embedding_lookup(job_embed_matrix, user_job, name = \"job_embed_layer\")\n",
    "    return uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "GkBJ_Iu7PKcL"
   },
   "outputs": [],
   "source": [
    "def get_user_feature_layer(uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer):\n",
    "    with tf.name_scope(\"user_fc\"):\n",
    "        #第一层全连接\n",
    "        uid_fc_layer = tf.layers.dense(uid_embed_layer, embed_dim, name = \"uid_fc_layer\", activation=tf.nn.relu)\n",
    "        gender_fc_layer = tf.layers.dense(gender_embed_layer, embed_dim, name = \"gender_fc_layer\", activation=tf.nn.relu)\n",
    "        age_fc_layer = tf.layers.dense(age_embed_layer, embed_dim, name =\"age_fc_layer\", activation=tf.nn.relu)\n",
    "        job_fc_layer = tf.layers.dense(job_embed_layer, embed_dim, name = \"job_fc_layer\", activation=tf.nn.relu)\n",
    "        \n",
    "        #第二层全连接\n",
    "        user_combine_layer = tf.concat([uid_fc_layer, gender_fc_layer, age_fc_layer, job_fc_layer], 2)  #(?, 1, 128)\n",
    "        user_combine_layer = tf.contrib.layers.fully_connected(user_combine_layer, 200, tf.tanh)  #(?, 1, 200)\n",
    "    \n",
    "        user_combine_layer_flat = tf.reshape(user_combine_layer, [-1, 200])\n",
    "    return user_combine_layer, user_combine_layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "SXOMi5faPNS8"
   },
   "outputs": [],
   "source": [
    "def get_movie_id_embed_layer(movie_id):\n",
    "    with tf.name_scope(\"movie_embedding\"):\n",
    "        movie_id_embed_matrix = tf.Variable(tf.random_uniform([movie_id_max, embed_dim], -1, 1), name = \"movie_id_embed_matrix\")\n",
    "        movie_id_embed_layer = tf.nn.embedding_lookup(movie_id_embed_matrix, movie_id, name = \"movie_id_embed_layer\")\n",
    "    return movie_id_embed_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "orh9gLNoPQI0"
   },
   "outputs": [],
   "source": [
    "def get_movie_categories_layers(movie_categories):\n",
    "    with tf.name_scope(\"movie_categories_layers\"):\n",
    "        movie_categories_embed_matrix = tf.Variable(tf.random_uniform([movie_categories_max, embed_dim], -1, 1), name = \"movie_categories_embed_matrix\")\n",
    "        movie_categories_embed_layer = tf.nn.embedding_lookup(movie_categories_embed_matrix, movie_categories, name = \"movie_categories_embed_layer\")\n",
    "        if combiner == \"sum\":\n",
    "            movie_categories_embed_layer = tf.reduce_sum(movie_categories_embed_layer, axis=1, keep_dims=True)\n",
    "    #     elif combiner == \"mean\":\n",
    "\n",
    "    return movie_categories_embed_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_bert_layer(movie_titles):\n",
    "    with tf.name_scope(\"movie_title_fc\"):\n",
    "        movie_title_fc_layer = tf.layers.dense(movie_titles, embed_dim, name = \"movie_title_fc_layer\", activation=tf.nn.relu)\n",
    "        movie_title_fc_layer_flat = tf.reshape(movie_title_fc_layer, [-1, 1, embed_dim])\n",
    "    return movie_title_fc_layer_flat\n",
    "\n",
    "def get_movie_feature_layer(movie_id_embed_layer, movie_categories_embed_layer, movie_title_fc_layer):\n",
    "    with tf.name_scope(\"movie_fc\"):\n",
    "        #第一层全连接\n",
    "        movie_id_fc_layer = tf.layers.dense(movie_id_embed_layer, embed_dim, name = \"movie_id_fc_layer\", activation=tf.nn.relu)\n",
    "        movie_categories_fc_layer = tf.layers.dense(movie_categories_embed_layer, embed_dim, name = \"movie_categories_fc_layer\", activation=tf.nn.relu)\n",
    "    \n",
    "        #第二层全连接\n",
    "        movie_combine_layer = tf.concat([movie_id_fc_layer, movie_categories_fc_layer, movie_title_fc_layer], 2)  #(?, 1, 96)\n",
    "        movie_combine_layer = tf.contrib.layers.fully_connected(movie_combine_layer, 200, tf.tanh)  #(?, 1, 200)\n",
    "    \n",
    "        movie_combine_layer_flat = tf.reshape(movie_combine_layer, [-1, 200])\n",
    "    return movie_combine_layer, movie_combine_layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ffdh0S_PY7b",
    "outputId": "b9525f8a-437e-47e9-e9c2-4de91d0192bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-19-559a1ee9ce9e>:6: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    #获取输入占位符\n",
    "    uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob = get_inputs()\n",
    "    #获取User的4个嵌入向量\n",
    "    uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer = get_user_embedding(uid, user_gender, user_age, user_job)\n",
    "    #得到用户特征\n",
    "    user_combine_layer, user_combine_layer_flat = get_user_feature_layer(uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer)\n",
    "    #获取电影ID的嵌入向量\n",
    "    movie_id_embed_layer = get_movie_id_embed_layer(movie_id)\n",
    "    #获取电影类型的嵌入向量\n",
    "    movie_categories_embed_layer = get_movie_categories_layers(movie_categories)\n",
    "    #获取电影名的特征向量\n",
    "    movie_title_fc_layer = get_movie_bert_layer(movie_titles)\n",
    "    #得到电影特征\n",
    "    movie_combine_layer, movie_combine_layer_flat = get_movie_feature_layer(movie_id_embed_layer, \n",
    "                                                                                movie_categories_embed_layer, \n",
    "                                                                                movie_title_fc_layer)\n",
    "    #计算出评分，要注意两个不同的方案，inference的名字（name值）是不一样的，后面做推荐时要根据name取得tensor\n",
    "    with tf.name_scope(\"inference\"):\n",
    "        #将用户特征和电影特征作为输入，经过全连接，输出一个值的方案\n",
    "#         inference_layer = tf.concat([user_combine_layer_flat, movie_combine_layer_flat], 1)  #(?, 200)\n",
    "#         inference = tf.layers.dense(inference_layer, 1,\n",
    "#                                     kernel_initializer=tf.truncated_normal_initializer(stddev=0.01), \n",
    "#                                     kernel_regularizer=tf.nn.l2_loss, name=\"inference\")\n",
    "        #简单的将用户特征和电影特征做矩阵乘法得到一个预测评分\n",
    "        inference = tf.reduce_sum(user_combine_layer_flat * movie_combine_layer_flat, axis=1)\n",
    "        inference = tf.expand_dims(inference, axis=1)\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        # MSE损失，将计算值回归到评分\n",
    "        cost = tf.losses.mean_squared_error(targets, inference )\n",
    "        loss = tf.reduce_mean(cost)\n",
    "    # 优化损失 \n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "    gradients = optimizer.compute_gradients(loss)  #cost\n",
    "    train_op = optimizer.apply_gradients(gradients, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "AXa_tQLYPe5D"
   },
   "outputs": [],
   "source": [
    "def get_batches(Xs, ys, batch_size):\n",
    "    for start in range(0, len(Xs), batch_size):\n",
    "        end = min(start + batch_size, len(Xs))\n",
    "        yield Xs[start:end], ys[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vzmEZAKSQ5oq",
    "outputId": "58ba3dfc-7bf1-4ea7-bca2-3f8b3257a106"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to C:\\Users\\wxhxbt\\jupyter\\MovieRec\\runs\\1619149982\n",
      "\n",
      "2021-04-22T23:53:04.074266: Epoch   0 Batch    0/2813   train_loss = 36.327\n",
      "2021-04-22T23:53:04.496031: Epoch   0 Batch   20/2813   train_loss = 5.823\n",
      "2021-04-22T23:53:04.912792: Epoch   0 Batch   40/2813   train_loss = 3.465\n",
      "2021-04-22T23:53:05.335515: Epoch   0 Batch   60/2813   train_loss = 2.787\n",
      "2021-04-22T23:53:05.774813: Epoch   0 Batch   80/2813   train_loss = 2.224\n",
      "2021-04-22T23:53:06.209578: Epoch   0 Batch  100/2813   train_loss = 1.858\n",
      "2021-04-22T23:53:06.636332: Epoch   0 Batch  120/2813   train_loss = 1.890\n",
      "2021-04-22T23:53:07.078077: Epoch   0 Batch  140/2813   train_loss = 1.745\n",
      "2021-04-22T23:53:07.516810: Epoch   0 Batch  160/2813   train_loss = 1.875\n",
      "2021-04-22T23:53:07.942746: Epoch   0 Batch  180/2813   train_loss = 2.021\n",
      "2021-04-22T23:53:08.361488: Epoch   0 Batch  200/2813   train_loss = 1.478\n",
      "2021-04-22T23:53:08.785258: Epoch   0 Batch  220/2813   train_loss = 1.973\n",
      "2021-04-22T23:53:09.214011: Epoch   0 Batch  240/2813   train_loss = 1.274\n",
      "2021-04-22T23:53:09.642770: Epoch   0 Batch  260/2813   train_loss = 1.568\n",
      "2021-04-22T23:53:10.069519: Epoch   0 Batch  280/2813   train_loss = 1.589\n",
      "2021-04-22T23:53:10.500650: Epoch   0 Batch  300/2813   train_loss = 1.360\n",
      "2021-04-22T23:53:10.918424: Epoch   0 Batch  320/2813   train_loss = 1.339\n",
      "2021-04-22T23:53:11.355173: Epoch   0 Batch  340/2813   train_loss = 1.472\n",
      "2021-04-22T23:53:11.784911: Epoch   0 Batch  360/2813   train_loss = 1.537\n",
      "2021-04-22T23:53:12.219674: Epoch   0 Batch  380/2813   train_loss = 1.394\n",
      "2021-04-22T23:53:12.644431: Epoch   0 Batch  400/2813   train_loss = 1.323\n",
      "2021-04-22T23:53:13.075168: Epoch   0 Batch  420/2813   train_loss = 1.509\n",
      "2021-04-22T23:53:13.507920: Epoch   0 Batch  440/2813   train_loss = 1.333\n",
      "2021-04-22T23:53:13.942274: Epoch   0 Batch  460/2813   train_loss = 1.403\n",
      "2021-04-22T23:53:14.366009: Epoch   0 Batch  480/2813   train_loss = 1.296\n",
      "2021-04-22T23:53:14.787766: Epoch   0 Batch  500/2813   train_loss = 1.293\n",
      "2021-04-22T23:53:15.216519: Epoch   0 Batch  520/2813   train_loss = 1.412\n",
      "2021-04-22T23:53:15.639764: Epoch   0 Batch  540/2813   train_loss = 1.331\n",
      "2021-04-22T23:53:16.072505: Epoch   0 Batch  560/2813   train_loss = 1.379\n",
      "2021-04-22T23:53:16.497260: Epoch   0 Batch  580/2813   train_loss = 1.333\n",
      "2021-04-22T23:53:16.922030: Epoch   0 Batch  600/2813   train_loss = 1.350\n",
      "2021-04-22T23:53:17.346773: Epoch   0 Batch  620/2813   train_loss = 1.206\n",
      "2021-04-22T23:53:17.770529: Epoch   0 Batch  640/2813   train_loss = 1.258\n",
      "2021-04-22T23:53:18.201294: Epoch   0 Batch  660/2813   train_loss = 1.321\n",
      "2021-04-22T23:53:18.629050: Epoch   0 Batch  680/2813   train_loss = 1.187\n",
      "2021-04-22T23:53:19.059796: Epoch   0 Batch  700/2813   train_loss = 1.313\n",
      "2021-04-22T23:53:19.488553: Epoch   0 Batch  720/2813   train_loss = 1.369\n",
      "2021-04-22T23:53:19.914963: Epoch   0 Batch  740/2813   train_loss = 1.327\n",
      "2021-04-22T23:53:20.337722: Epoch   0 Batch  760/2813   train_loss = 1.231\n",
      "2021-04-22T23:53:20.759477: Epoch   0 Batch  780/2813   train_loss = 1.392\n",
      "2021-04-22T23:53:21.178795: Epoch   0 Batch  800/2813   train_loss = 1.211\n",
      "2021-04-22T23:53:21.600550: Epoch   0 Batch  820/2813   train_loss = 1.339\n",
      "2021-04-22T23:53:22.028316: Epoch   0 Batch  840/2813   train_loss = 1.354\n",
      "2021-04-22T23:53:22.459056: Epoch   0 Batch  860/2813   train_loss = 1.316\n",
      "2021-04-22T23:53:22.884825: Epoch   0 Batch  880/2813   train_loss = 1.295\n",
      "2021-04-22T23:53:23.318571: Epoch   0 Batch  900/2813   train_loss = 1.320\n",
      "2021-04-22T23:53:23.743316: Epoch   0 Batch  920/2813   train_loss = 1.078\n",
      "2021-04-22T23:53:24.164074: Epoch   0 Batch  940/2813   train_loss = 1.242\n",
      "2021-04-22T23:53:24.581848: Epoch   0 Batch  960/2813   train_loss = 1.182\n",
      "2021-04-22T23:53:25.011601: Epoch   0 Batch  980/2813   train_loss = 1.276\n",
      "2021-04-22T23:53:25.441349: Epoch   0 Batch 1000/2813   train_loss = 1.144\n",
      "2021-04-22T23:53:25.861099: Epoch   0 Batch 1020/2813   train_loss = 1.185\n",
      "2021-04-22T23:53:26.289850: Epoch   0 Batch 1040/2813   train_loss = 1.172\n",
      "2021-04-22T23:53:26.709624: Epoch   0 Batch 1060/2813   train_loss = 1.299\n",
      "2021-04-22T23:53:27.145372: Epoch   0 Batch 1080/2813   train_loss = 1.357\n",
      "2021-04-22T23:53:27.567114: Epoch   0 Batch 1100/2813   train_loss = 1.198\n",
      "2021-04-22T23:53:27.992884: Epoch   0 Batch 1120/2813   train_loss = 1.232\n",
      "2021-04-22T23:53:28.424621: Epoch   0 Batch 1140/2813   train_loss = 1.115\n",
      "2021-04-22T23:53:28.850387: Epoch   0 Batch 1160/2813   train_loss = 1.463\n",
      "2021-04-22T23:53:29.277142: Epoch   0 Batch 1180/2813   train_loss = 1.326\n",
      "2021-04-22T23:53:29.690906: Epoch   0 Batch 1200/2813   train_loss = 1.195\n",
      "2021-04-22T23:53:30.113663: Epoch   0 Batch 1220/2813   train_loss = 1.315\n",
      "2021-04-22T23:53:30.548399: Epoch   0 Batch 1240/2813   train_loss = 1.168\n",
      "2021-04-22T23:53:30.970157: Epoch   0 Batch 1260/2813   train_loss = 1.215\n",
      "2021-04-22T23:53:31.393927: Epoch   0 Batch 1280/2813   train_loss = 1.299\n",
      "2021-04-22T23:53:31.815678: Epoch   0 Batch 1300/2813   train_loss = 1.017\n",
      "2021-04-22T23:53:32.244436: Epoch   0 Batch 1320/2813   train_loss = 1.268\n",
      "2021-04-22T23:53:32.671175: Epoch   0 Batch 1340/2813   train_loss = 1.354\n",
      "2021-04-22T23:53:33.097949: Epoch   0 Batch 1360/2813   train_loss = 1.266\n",
      "2021-04-22T23:53:33.517689: Epoch   0 Batch 1380/2813   train_loss = 0.867\n",
      "2021-04-22T23:53:33.941444: Epoch   0 Batch 1400/2813   train_loss = 1.382\n",
      "2021-04-22T23:53:34.358204: Epoch   0 Batch 1420/2813   train_loss = 1.102\n",
      "2021-04-22T23:53:34.781975: Epoch   0 Batch 1440/2813   train_loss = 1.125\n",
      "2021-04-22T23:53:35.211185: Epoch   0 Batch 1460/2813   train_loss = 1.256\n",
      "2021-04-22T23:53:35.636939: Epoch   0 Batch 1480/2813   train_loss = 1.177\n",
      "2021-04-22T23:53:36.054698: Epoch   0 Batch 1500/2813   train_loss = 1.071\n",
      "2021-04-22T23:53:36.481438: Epoch   0 Batch 1520/2813   train_loss = 1.288\n",
      "2021-04-22T23:53:36.898803: Epoch   0 Batch 1540/2813   train_loss = 1.116\n",
      "2021-04-22T23:53:37.315549: Epoch   0 Batch 1560/2813   train_loss = 1.144\n",
      "2021-04-22T23:53:37.749312: Epoch   0 Batch 1580/2813   train_loss = 1.226\n",
      "2021-04-22T23:53:38.176418: Epoch   0 Batch 1600/2813   train_loss = 1.056\n",
      "2021-04-22T23:53:38.602173: Epoch   0 Batch 1620/2813   train_loss = 1.166\n",
      "2021-04-22T23:53:39.018933: Epoch   0 Batch 1640/2813   train_loss = 1.133\n",
      "2021-04-22T23:53:39.439691: Epoch   0 Batch 1660/2813   train_loss = 1.304\n",
      "2021-04-22T23:53:39.865446: Epoch   0 Batch 1680/2813   train_loss = 1.120\n",
      "2021-04-22T23:53:40.295218: Epoch   0 Batch 1700/2813   train_loss = 0.997\n",
      "2021-04-22T23:53:40.718955: Epoch   0 Batch 1720/2813   train_loss = 1.037\n",
      "2021-04-22T23:53:41.146725: Epoch   0 Batch 1740/2813   train_loss = 1.106\n",
      "2021-04-22T23:53:41.572464: Epoch   0 Batch 1760/2813   train_loss = 1.234\n",
      "2021-04-22T23:53:42.002232: Epoch   0 Batch 1780/2813   train_loss = 1.076\n",
      "2021-04-22T23:53:42.422983: Epoch   0 Batch 1800/2813   train_loss = 1.195\n",
      "2021-04-22T23:53:42.852725: Epoch   0 Batch 1820/2813   train_loss = 1.175\n",
      "2021-04-22T23:53:43.277480: Epoch   0 Batch 1840/2813   train_loss = 1.127\n",
      "2021-04-22T23:53:43.717227: Epoch   0 Batch 1860/2813   train_loss = 1.057\n",
      "2021-04-22T23:53:44.136986: Epoch   0 Batch 1880/2813   train_loss = 0.995\n",
      "2021-04-22T23:53:44.562751: Epoch   0 Batch 1900/2813   train_loss = 1.301\n",
      "2021-04-22T23:53:44.985508: Epoch   0 Batch 1920/2813   train_loss = 1.245\n",
      "2021-04-22T23:53:45.413252: Epoch   0 Batch 1940/2813   train_loss = 1.061\n",
      "2021-04-22T23:53:45.847016: Epoch   0 Batch 1960/2813   train_loss = 1.138\n",
      "2021-04-22T23:53:46.271767: Epoch   0 Batch 1980/2813   train_loss = 1.105\n",
      "2021-04-22T23:53:46.701185: Epoch   0 Batch 2000/2813   train_loss = 1.158\n",
      "2021-04-22T23:53:47.132936: Epoch   0 Batch 2020/2813   train_loss = 0.996\n",
      "2021-04-22T23:53:47.558676: Epoch   0 Batch 2040/2813   train_loss = 1.055\n",
      "2021-04-22T23:53:47.975448: Epoch   0 Batch 2060/2813   train_loss = 1.139\n",
      "2021-04-22T23:53:48.404200: Epoch   0 Batch 2080/2813   train_loss = 1.096\n",
      "2021-04-22T23:53:48.825961: Epoch   0 Batch 2100/2813   train_loss = 1.260\n",
      "2021-04-22T23:53:49.245724: Epoch   0 Batch 2120/2813   train_loss = 1.033\n",
      "2021-04-22T23:53:49.683454: Epoch   0 Batch 2140/2813   train_loss = 1.156\n",
      "2021-04-22T23:53:50.108208: Epoch   0 Batch 2160/2813   train_loss = 1.004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-22T23:53:50.548954: Epoch   0 Batch 2180/2813   train_loss = 1.101\n",
      "2021-04-22T23:53:50.977718: Epoch   0 Batch 2200/2813   train_loss = 1.121\n",
      "2021-04-22T23:53:51.402478: Epoch   0 Batch 2220/2813   train_loss = 1.216\n",
      "2021-04-22T23:53:51.823231: Epoch   0 Batch 2240/2813   train_loss = 1.178\n",
      "2021-04-22T23:53:52.243992: Epoch   0 Batch 2260/2813   train_loss = 1.158\n",
      "2021-04-22T23:53:52.668743: Epoch   0 Batch 2280/2813   train_loss = 1.220\n",
      "2021-04-22T23:53:53.109480: Epoch   0 Batch 2300/2813   train_loss = 1.048\n",
      "2021-04-22T23:53:53.539234: Epoch   0 Batch 2320/2813   train_loss = 0.989\n",
      "2021-04-22T23:53:53.996970: Epoch   0 Batch 2340/2813   train_loss = 1.150\n",
      "2021-04-22T23:53:54.420544: Epoch   0 Batch 2360/2813   train_loss = 0.954\n",
      "2021-04-22T23:53:54.845290: Epoch   0 Batch 2380/2813   train_loss = 0.954\n",
      "2021-04-22T23:53:55.291042: Epoch   0 Batch 2400/2813   train_loss = 1.175\n",
      "2021-04-22T23:53:55.716801: Epoch   0 Batch 2420/2813   train_loss = 1.205\n",
      "2021-04-22T23:53:56.150538: Epoch   0 Batch 2440/2813   train_loss = 1.025\n",
      "2021-04-22T23:53:56.578290: Epoch   0 Batch 2460/2813   train_loss = 1.000\n",
      "2021-04-22T23:53:57.009016: Epoch   0 Batch 2480/2813   train_loss = 1.106\n",
      "2021-04-22T23:53:57.437788: Epoch   0 Batch 2500/2813   train_loss = 1.070\n",
      "2021-04-22T23:53:57.861539: Epoch   0 Batch 2520/2813   train_loss = 1.108\n",
      "2021-04-22T23:53:58.289282: Epoch   0 Batch 2540/2813   train_loss = 1.160\n",
      "2021-04-22T23:53:58.711067: Epoch   0 Batch 2560/2813   train_loss = 1.173\n",
      "2021-04-22T23:53:59.138820: Epoch   0 Batch 2580/2813   train_loss = 1.289\n",
      "2021-04-22T23:53:59.562589: Epoch   0 Batch 2600/2813   train_loss = 1.117\n",
      "2021-04-22T23:54:00.000337: Epoch   0 Batch 2620/2813   train_loss = 1.068\n",
      "2021-04-22T23:54:00.421080: Epoch   0 Batch 2640/2813   train_loss = 1.050\n",
      "2021-04-22T23:54:00.843837: Epoch   0 Batch 2660/2813   train_loss = 1.174\n",
      "2021-04-22T23:54:01.278601: Epoch   0 Batch 2680/2813   train_loss = 0.965\n",
      "2021-04-22T23:54:01.717334: Epoch   0 Batch 2700/2813   train_loss = 1.075\n",
      "2021-04-22T23:54:02.141090: Epoch   0 Batch 2720/2813   train_loss = 1.104\n",
      "2021-04-22T23:54:02.569853: Epoch   0 Batch 2740/2813   train_loss = 1.119\n",
      "2021-04-22T23:54:02.995598: Epoch   0 Batch 2760/2813   train_loss = 1.037\n",
      "2021-04-22T23:54:03.418369: Epoch   0 Batch 2780/2813   train_loss = 1.071\n",
      "2021-04-22T23:54:03.838117: Epoch   0 Batch 2800/2813   train_loss = 0.877\n",
      "2021-04-22T23:54:04.220655: Epoch   0 Batch    0/312   test_loss = 1.072\n",
      "2021-04-22T23:54:04.384559: Epoch   0 Batch   20/312   test_loss = 0.949\n",
      "2021-04-22T23:54:04.549465: Epoch   0 Batch   40/312   test_loss = 1.292\n",
      "2021-04-22T23:54:04.715634: Epoch   0 Batch   60/312   test_loss = 1.153\n",
      "2021-04-22T23:54:04.883536: Epoch   0 Batch   80/312   test_loss = 1.007\n",
      "2021-04-22T23:54:05.053438: Epoch   0 Batch  100/312   test_loss = 1.025\n",
      "2021-04-22T23:54:05.219457: Epoch   0 Batch  120/312   test_loss = 1.070\n",
      "2021-04-22T23:54:05.384405: Epoch   0 Batch  140/312   test_loss = 1.087\n",
      "2021-04-22T23:54:05.551311: Epoch   0 Batch  160/312   test_loss = 0.957\n",
      "2021-04-22T23:54:05.716214: Epoch   0 Batch  180/312   test_loss = 1.001\n",
      "2021-04-22T23:54:05.881119: Epoch   0 Batch  200/312   test_loss = 1.098\n",
      "2021-04-22T23:54:06.045965: Epoch   0 Batch  220/312   test_loss = 1.154\n",
      "2021-04-22T23:54:06.211871: Epoch   0 Batch  240/312   test_loss = 1.192\n",
      "2021-04-22T23:54:06.372778: Epoch   0 Batch  260/312   test_loss = 1.005\n",
      "2021-04-22T23:54:06.540680: Epoch   0 Batch  280/312   test_loss = 1.054\n",
      "2021-04-22T23:54:06.705587: Epoch   0 Batch  300/312   test_loss = 1.308\n",
      "2021-04-22T23:54:07.212292: Epoch   1 Batch    7/2813   train_loss = 1.230\n",
      "2021-04-22T23:54:07.639057: Epoch   1 Batch   27/2813   train_loss = 1.006\n",
      "2021-04-22T23:54:08.070815: Epoch   1 Batch   47/2813   train_loss = 0.979\n",
      "2021-04-22T23:54:08.503566: Epoch   1 Batch   67/2813   train_loss = 1.018\n",
      "2021-04-22T23:54:08.924308: Epoch   1 Batch   87/2813   train_loss = 1.099\n",
      "2021-04-22T23:54:09.358067: Epoch   1 Batch  107/2813   train_loss = 1.092\n",
      "2021-04-22T23:54:09.782828: Epoch   1 Batch  127/2813   train_loss = 1.124\n",
      "2021-04-22T23:54:10.209581: Epoch   1 Batch  147/2813   train_loss = 0.969\n",
      "2021-04-22T23:54:10.634555: Epoch   1 Batch  167/2813   train_loss = 0.986\n",
      "2021-04-22T23:54:11.060310: Epoch   1 Batch  187/2813   train_loss = 1.039\n",
      "2021-04-22T23:54:11.485066: Epoch   1 Batch  207/2813   train_loss = 1.103\n",
      "2021-04-22T23:54:11.910825: Epoch   1 Batch  227/2813   train_loss = 0.939\n",
      "2021-04-22T23:54:12.340572: Epoch   1 Batch  247/2813   train_loss = 1.002\n",
      "2021-04-22T23:54:12.770325: Epoch   1 Batch  267/2813   train_loss = 1.112\n",
      "2021-04-22T23:54:13.197065: Epoch   1 Batch  287/2813   train_loss = 0.910\n",
      "2021-04-22T23:54:13.736757: Epoch   1 Batch  307/2813   train_loss = 0.955\n",
      "2021-04-22T23:54:15.205908: Epoch   1 Batch  327/2813   train_loss = 1.080\n",
      "2021-04-22T23:54:15.623668: Epoch   1 Batch  347/2813   train_loss = 1.110\n",
      "2021-04-22T23:54:16.051421: Epoch   1 Batch  367/2813   train_loss = 1.061\n",
      "2021-04-22T23:54:16.473179: Epoch   1 Batch  387/2813   train_loss = 0.924\n",
      "2021-04-22T23:54:16.902941: Epoch   1 Batch  407/2813   train_loss = 1.068\n",
      "2021-04-22T23:54:17.328700: Epoch   1 Batch  427/2813   train_loss = 0.937\n",
      "2021-04-22T23:54:17.757454: Epoch   1 Batch  447/2813   train_loss = 0.992\n",
      "2021-04-22T23:54:18.182176: Epoch   1 Batch  467/2813   train_loss = 0.886\n",
      "2021-04-22T23:54:18.610918: Epoch   1 Batch  487/2813   train_loss = 1.040\n",
      "2021-04-22T23:54:19.034678: Epoch   1 Batch  507/2813   train_loss = 0.950\n",
      "2021-04-22T23:54:19.459430: Epoch   1 Batch  527/2813   train_loss = 1.110\n",
      "2021-04-22T23:54:19.882190: Epoch   1 Batch  547/2813   train_loss = 1.078\n",
      "2021-04-22T23:54:20.309945: Epoch   1 Batch  567/2813   train_loss = 0.938\n",
      "2021-04-22T23:54:20.745695: Epoch   1 Batch  587/2813   train_loss = 1.128\n",
      "2021-04-22T23:54:21.176440: Epoch   1 Batch  607/2813   train_loss = 1.092\n",
      "2021-04-22T23:54:21.604185: Epoch   1 Batch  627/2813   train_loss = 0.999\n",
      "2021-04-22T23:54:22.031938: Epoch   1 Batch  647/2813   train_loss = 0.942\n",
      "2021-04-22T23:54:22.454708: Epoch   1 Batch  667/2813   train_loss = 1.098\n",
      "2021-04-22T23:54:22.879459: Epoch   1 Batch  687/2813   train_loss = 0.850\n",
      "2021-04-22T23:54:23.304219: Epoch   1 Batch  707/2813   train_loss = 0.945\n",
      "2021-04-22T23:54:23.736966: Epoch   1 Batch  727/2813   train_loss = 0.877\n",
      "2021-04-22T23:54:24.169707: Epoch   1 Batch  747/2813   train_loss = 1.232\n",
      "2021-04-22T23:54:24.847354: Epoch   1 Batch  767/2813   train_loss = 0.977\n",
      "2021-04-22T23:54:26.175563: Epoch   1 Batch  787/2813   train_loss = 1.006\n",
      "2021-04-22T23:54:26.592322: Epoch   1 Batch  807/2813   train_loss = 1.143\n",
      "2021-04-22T23:54:27.014069: Epoch   1 Batch  827/2813   train_loss = 1.201\n",
      "2021-04-22T23:54:27.438835: Epoch   1 Batch  847/2813   train_loss = 1.022\n",
      "2021-04-22T23:54:27.861581: Epoch   1 Batch  867/2813   train_loss = 0.906\n",
      "2021-04-22T23:54:28.288345: Epoch   1 Batch  887/2813   train_loss = 0.828\n",
      "2021-04-22T23:54:28.719088: Epoch   1 Batch  907/2813   train_loss = 0.970\n",
      "2021-04-22T23:54:29.145846: Epoch   1 Batch  927/2813   train_loss = 1.140\n",
      "2021-04-22T23:54:30.631988: Epoch   1 Batch  947/2813   train_loss = 0.996\n",
      "2021-04-22T23:54:31.160700: Epoch   1 Batch  967/2813   train_loss = 1.119\n",
      "2021-04-22T23:54:31.584012: Epoch   1 Batch  987/2813   train_loss = 0.939\n",
      "2021-04-22T23:54:32.006000: Epoch   1 Batch 1007/2813   train_loss = 0.966\n",
      "2021-04-22T23:54:32.432750: Epoch   1 Batch 1027/2813   train_loss = 0.990\n",
      "2021-04-22T23:54:32.858503: Epoch   1 Batch 1047/2813   train_loss = 1.116\n",
      "2021-04-22T23:54:33.294238: Epoch   1 Batch 1067/2813   train_loss = 0.948\n",
      "2021-04-22T23:54:33.719009: Epoch   1 Batch 1087/2813   train_loss = 0.951\n",
      "2021-04-22T23:54:34.142763: Epoch   1 Batch 1107/2813   train_loss = 1.016\n",
      "2021-04-22T23:54:34.564522: Epoch   1 Batch 1127/2813   train_loss = 1.077\n",
      "2021-04-22T23:54:34.995260: Epoch   1 Batch 1147/2813   train_loss = 1.040\n",
      "2021-04-22T23:54:35.424013: Epoch   1 Batch 1167/2813   train_loss = 1.002\n",
      "2021-04-22T23:54:37.000142: Epoch   1 Batch 1187/2813   train_loss = 0.986\n",
      "2021-04-22T23:54:37.435868: Epoch   1 Batch 1207/2813   train_loss = 0.974\n",
      "2021-04-22T23:54:37.853622: Epoch   1 Batch 1227/2813   train_loss = 1.154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-22T23:54:38.282376: Epoch   1 Batch 1247/2813   train_loss = 0.903\n",
      "2021-04-22T23:54:38.708135: Epoch   1 Batch 1267/2813   train_loss = 0.942\n",
      "2021-04-22T23:54:39.138887: Epoch   1 Batch 1287/2813   train_loss = 0.919\n",
      "2021-04-22T23:54:39.568640: Epoch   1 Batch 1307/2813   train_loss = 0.997\n",
      "2021-04-22T23:54:39.997380: Epoch   1 Batch 1327/2813   train_loss = 1.202\n",
      "2021-04-22T23:54:40.426132: Epoch   1 Batch 1347/2813   train_loss = 0.917\n",
      "2021-04-22T23:54:40.848888: Epoch   1 Batch 1367/2813   train_loss = 1.228\n",
      "2021-04-22T23:54:41.275642: Epoch   1 Batch 1387/2813   train_loss = 0.980\n",
      "2021-04-22T23:54:41.705400: Epoch   1 Batch 1407/2813   train_loss = 1.107\n",
      "2021-04-22T23:54:42.128162: Epoch   1 Batch 1427/2813   train_loss = 0.993\n",
      "2021-04-22T23:54:42.557918: Epoch   1 Batch 1447/2813   train_loss = 0.957\n",
      "2021-04-22T23:54:42.990655: Epoch   1 Batch 1467/2813   train_loss = 1.030\n",
      "2021-04-22T23:54:43.859159: Epoch   1 Batch 1487/2813   train_loss = 0.905\n",
      "2021-04-22T23:54:45.017501: Epoch   1 Batch 1507/2813   train_loss = 1.011\n",
      "2021-04-22T23:54:45.439258: Epoch   1 Batch 1527/2813   train_loss = 1.200\n",
      "2021-04-22T23:54:45.860017: Epoch   1 Batch 1547/2813   train_loss = 1.059\n",
      "2021-04-22T23:54:46.289836: Epoch   1 Batch 1567/2813   train_loss = 0.933\n",
      "2021-04-22T23:54:46.717607: Epoch   1 Batch 1587/2813   train_loss = 1.022\n",
      "2021-04-22T23:54:47.154336: Epoch   1 Batch 1607/2813   train_loss = 0.980\n",
      "2021-04-22T23:54:47.592085: Epoch   1 Batch 1627/2813   train_loss = 0.966\n",
      "2021-04-22T23:54:48.160760: Epoch   1 Batch 1647/2813   train_loss = 1.028\n",
      "2021-04-22T23:54:49.599930: Epoch   1 Batch 1667/2813   train_loss = 0.763\n",
      "2021-04-22T23:54:50.027683: Epoch   1 Batch 1687/2813   train_loss = 0.950\n",
      "2021-04-22T23:54:50.452438: Epoch   1 Batch 1707/2813   train_loss = 1.016\n",
      "2021-04-22T23:54:50.883190: Epoch   1 Batch 1727/2813   train_loss = 0.985\n",
      "2021-04-22T23:54:51.306946: Epoch   1 Batch 1747/2813   train_loss = 0.916\n",
      "2021-04-22T23:54:51.759701: Epoch   1 Batch 1767/2813   train_loss = 0.944\n",
      "2021-04-22T23:54:52.184440: Epoch   1 Batch 1787/2813   train_loss = 1.095\n",
      "2021-04-22T23:54:52.610195: Epoch   1 Batch 1807/2813   train_loss = 1.134\n",
      "2021-04-22T23:54:54.149311: Epoch   1 Batch 1827/2813   train_loss = 1.046\n",
      "2021-04-22T23:54:54.617054: Epoch   1 Batch 1847/2813   train_loss = 1.115\n",
      "2021-04-22T23:54:55.044795: Epoch   1 Batch 1867/2813   train_loss = 0.925\n",
      "2021-04-22T23:54:55.467550: Epoch   1 Batch 1887/2813   train_loss = 1.161\n",
      "2021-04-22T23:54:55.887307: Epoch   1 Batch 1907/2813   train_loss = 1.057\n",
      "2021-04-22T23:54:56.318061: Epoch   1 Batch 1927/2813   train_loss = 0.897\n",
      "2021-04-22T23:54:56.750825: Epoch   1 Batch 1947/2813   train_loss = 1.140\n",
      "2021-04-22T23:54:57.184561: Epoch   1 Batch 1967/2813   train_loss = 0.919\n",
      "2021-04-22T23:54:58.420853: Epoch   1 Batch 1987/2813   train_loss = 0.969\n",
      "2021-04-22T23:54:59.213407: Epoch   1 Batch 2007/2813   train_loss = 0.851\n",
      "2021-04-22T23:54:59.632151: Epoch   1 Batch 2027/2813   train_loss = 1.085\n",
      "2021-04-22T23:55:00.062914: Epoch   1 Batch 2047/2813   train_loss = 0.843\n",
      "2021-04-22T23:55:00.491657: Epoch   1 Batch 2067/2813   train_loss = 1.205\n",
      "2021-04-22T23:55:00.926406: Epoch   1 Batch 2087/2813   train_loss = 1.050\n",
      "2021-04-22T23:55:01.349163: Epoch   1 Batch 2107/2813   train_loss = 0.908\n",
      "2021-04-22T23:55:01.769935: Epoch   1 Batch 2127/2813   train_loss = 0.998\n",
      "2021-04-22T23:55:02.666407: Epoch   1 Batch 2147/2813   train_loss = 0.844\n",
      "2021-04-22T23:55:03.775766: Epoch   1 Batch 2167/2813   train_loss = 0.941\n",
      "2021-04-22T23:55:04.202535: Epoch   1 Batch 2187/2813   train_loss = 0.930\n",
      "2021-04-22T23:55:04.628287: Epoch   1 Batch 2207/2813   train_loss = 1.022\n",
      "2021-04-22T23:55:05.050032: Epoch   1 Batch 2227/2813   train_loss = 1.120\n",
      "2021-04-22T23:55:05.476801: Epoch   1 Batch 2247/2813   train_loss = 0.968\n",
      "2021-04-22T23:55:05.905540: Epoch   1 Batch 2267/2813   train_loss = 0.940\n",
      "2021-04-22T23:55:06.333308: Epoch   1 Batch 2287/2813   train_loss = 0.996\n",
      "2021-04-22T23:55:06.939947: Epoch   1 Batch 2307/2813   train_loss = 1.011\n",
      "2021-04-22T23:55:08.359126: Epoch   1 Batch 2327/2813   train_loss = 0.900\n",
      "2021-04-22T23:55:08.779884: Epoch   1 Batch 2347/2813   train_loss = 0.990\n",
      "2021-04-22T23:55:09.207637: Epoch   1 Batch 2367/2813   train_loss = 0.954\n",
      "2021-04-22T23:55:09.642387: Epoch   1 Batch 2387/2813   train_loss = 1.045\n",
      "2021-04-22T23:55:10.061146: Epoch   1 Batch 2407/2813   train_loss = 0.911\n",
      "2021-04-22T23:55:10.495895: Epoch   1 Batch 2427/2813   train_loss = 1.101\n",
      "2021-04-22T23:55:10.913073: Epoch   1 Batch 2447/2813   train_loss = 0.920\n",
      "2021-04-22T23:55:11.338843: Epoch   1 Batch 2467/2813   train_loss = 0.923\n",
      "2021-04-22T23:55:12.830973: Epoch   1 Batch 2487/2813   train_loss = 1.110\n",
      "2021-04-22T23:55:13.353668: Epoch   1 Batch 2507/2813   train_loss = 1.060\n",
      "2021-04-22T23:55:13.780422: Epoch   1 Batch 2527/2813   train_loss = 0.845\n",
      "2021-04-22T23:55:14.205192: Epoch   1 Batch 2547/2813   train_loss = 1.041\n",
      "2021-04-22T23:55:14.634944: Epoch   1 Batch 2567/2813   train_loss = 0.948\n",
      "2021-04-22T23:55:15.074691: Epoch   1 Batch 2587/2813   train_loss = 1.008\n",
      "2021-04-22T23:55:15.503444: Epoch   1 Batch 2607/2813   train_loss = 1.053\n",
      "2021-04-22T23:55:15.931198: Epoch   1 Batch 2627/2813   train_loss = 1.123\n",
      "2021-04-22T23:55:17.132494: Epoch   1 Batch 2647/2813   train_loss = 1.005\n",
      "2021-04-22T23:55:17.917054: Epoch   1 Batch 2667/2813   train_loss = 0.875\n",
      "2021-04-22T23:55:18.333816: Epoch   1 Batch 2687/2813   train_loss = 0.880\n",
      "2021-04-22T23:55:18.758558: Epoch   1 Batch 2707/2813   train_loss = 1.017\n",
      "2021-04-22T23:55:19.186323: Epoch   1 Batch 2727/2813   train_loss = 1.023\n",
      "2021-04-22T23:55:19.607082: Epoch   1 Batch 2747/2813   train_loss = 1.004\n",
      "2021-04-22T23:55:20.029833: Epoch   1 Batch 2767/2813   train_loss = 0.992\n",
      "2021-04-22T23:55:20.453581: Epoch   1 Batch 2787/2813   train_loss = 1.099\n",
      "2021-04-22T23:55:21.240128: Epoch   1 Batch 2807/2813   train_loss = 1.083\n",
      "2021-04-22T23:55:21.939761: Epoch   1 Batch    8/312   test_loss = 0.935\n",
      "2021-04-22T23:55:22.393906: Epoch   1 Batch   28/312   test_loss = 1.166\n",
      "2021-04-22T23:55:22.556810: Epoch   1 Batch   48/312   test_loss = 1.069\n",
      "2021-04-22T23:55:22.722714: Epoch   1 Batch   68/312   test_loss = 0.989\n",
      "2021-04-22T23:55:22.886620: Epoch   1 Batch   88/312   test_loss = 1.090\n",
      "2021-04-22T23:55:23.055524: Epoch   1 Batch  108/312   test_loss = 1.038\n",
      "2021-04-22T23:55:23.218429: Epoch   1 Batch  128/312   test_loss = 0.795\n",
      "2021-04-22T23:55:23.386343: Epoch   1 Batch  148/312   test_loss = 0.944\n",
      "2021-04-22T23:55:23.550238: Epoch   1 Batch  168/312   test_loss = 1.039\n",
      "2021-04-22T23:55:23.716143: Epoch   1 Batch  188/312   test_loss = 0.855\n",
      "2021-04-22T23:55:23.880048: Epoch   1 Batch  208/312   test_loss = 1.050\n",
      "2021-04-22T23:55:24.045447: Epoch   1 Batch  228/312   test_loss = 0.954\n",
      "2021-04-22T23:55:24.209355: Epoch   1 Batch  248/312   test_loss = 0.869\n",
      "2021-04-22T23:55:24.375249: Epoch   1 Batch  268/312   test_loss = 1.020\n",
      "2021-04-22T23:55:24.540152: Epoch   1 Batch  288/312   test_loss = 0.864\n",
      "2021-04-22T23:55:24.710064: Epoch   1 Batch  308/312   test_loss = 1.033\n",
      "2021-04-22T23:55:25.301714: Epoch   2 Batch   14/2813   train_loss = 0.875\n",
      "2021-04-22T23:55:25.723471: Epoch   2 Batch   34/2813   train_loss = 0.935\n",
      "2021-04-22T23:55:26.153238: Epoch   2 Batch   54/2813   train_loss = 0.988\n",
      "2021-04-22T23:55:26.573986: Epoch   2 Batch   74/2813   train_loss = 1.119\n",
      "2021-04-22T23:55:27.000102: Epoch   2 Batch   94/2813   train_loss = 0.917\n",
      "2021-04-22T23:55:28.595174: Epoch   2 Batch  114/2813   train_loss = 1.018\n",
      "2021-04-22T23:55:29.021943: Epoch   2 Batch  134/2813   train_loss = 0.872\n",
      "2021-04-22T23:55:29.447660: Epoch   2 Batch  154/2813   train_loss = 1.101\n",
      "2021-04-22T23:55:29.876415: Epoch   2 Batch  174/2813   train_loss = 0.972\n",
      "2021-04-22T23:55:30.295172: Epoch   2 Batch  194/2813   train_loss = 0.948\n",
      "2021-04-22T23:55:30.711947: Epoch   2 Batch  214/2813   train_loss = 1.071\n",
      "2021-04-22T23:55:31.142699: Epoch   2 Batch  234/2813   train_loss = 0.923\n",
      "2021-04-22T23:55:31.572452: Epoch   2 Batch  254/2813   train_loss = 0.869\n",
      "2021-04-22T23:55:32.804690: Epoch   2 Batch  274/2813   train_loss = 1.066\n",
      "2021-04-22T23:55:33.579242: Epoch   2 Batch  294/2813   train_loss = 0.948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-22T23:55:34.009993: Epoch   2 Batch  314/2813   train_loss = 0.958\n",
      "2021-04-22T23:55:34.435763: Epoch   2 Batch  334/2813   train_loss = 0.886\n",
      "2021-04-22T23:55:34.856507: Epoch   2 Batch  354/2813   train_loss = 1.100\n",
      "2021-04-22T23:55:35.277278: Epoch   2 Batch  374/2813   train_loss = 1.008\n",
      "2021-04-22T23:55:35.699035: Epoch   2 Batch  394/2813   train_loss = 0.973\n",
      "2021-04-22T23:55:36.122777: Epoch   2 Batch  414/2813   train_loss = 0.939\n",
      "2021-04-22T23:55:36.555528: Epoch   2 Batch  434/2813   train_loss = 0.949\n",
      "2021-04-22T23:55:36.983221: Epoch   2 Batch  454/2813   train_loss = 0.952\n",
      "2021-04-22T23:55:37.418978: Epoch   2 Batch  474/2813   train_loss = 0.970\n",
      "2021-04-22T23:55:37.843725: Epoch   2 Batch  494/2813   train_loss = 0.876\n",
      "2021-04-22T23:55:39.152971: Epoch   2 Batch  514/2813   train_loss = 0.975\n",
      "2021-04-22T23:55:39.851567: Epoch   2 Batch  534/2813   train_loss = 1.154\n",
      "2021-04-22T23:55:40.266329: Epoch   2 Batch  554/2813   train_loss = 0.924\n",
      "2021-04-22T23:55:40.688095: Epoch   2 Batch  574/2813   train_loss = 0.958\n",
      "2021-04-22T23:55:41.116852: Epoch   2 Batch  594/2813   train_loss = 0.952\n",
      "2021-04-22T23:55:41.536386: Epoch   2 Batch  614/2813   train_loss = 1.096\n",
      "2021-04-22T23:55:41.959143: Epoch   2 Batch  634/2813   train_loss = 1.022\n",
      "2021-04-22T23:55:42.377902: Epoch   2 Batch  654/2813   train_loss = 1.027\n",
      "2021-04-22T23:55:43.218420: Epoch   2 Batch  674/2813   train_loss = 0.952\n",
      "2021-04-22T23:55:44.390757: Epoch   2 Batch  694/2813   train_loss = 1.078\n",
      "2021-04-22T23:55:44.820510: Epoch   2 Batch  714/2813   train_loss = 0.794\n",
      "2021-04-22T23:55:45.245795: Epoch   2 Batch  734/2813   train_loss = 0.989\n",
      "2021-04-22T23:55:45.668441: Epoch   2 Batch  754/2813   train_loss = 0.880\n",
      "2021-04-22T23:55:46.100178: Epoch   2 Batch  774/2813   train_loss = 0.947\n",
      "2021-04-22T23:55:46.531929: Epoch   2 Batch  794/2813   train_loss = 0.924\n",
      "2021-04-22T23:55:46.961696: Epoch   2 Batch  814/2813   train_loss = 1.037\n",
      "2021-04-22T23:55:47.391435: Epoch   2 Batch  834/2813   train_loss = 0.837\n",
      "2021-04-22T23:55:47.818203: Epoch   2 Batch  854/2813   train_loss = 0.972\n",
      "2021-04-22T23:55:48.246956: Epoch   2 Batch  874/2813   train_loss = 0.882\n",
      "2021-04-22T23:55:48.681707: Epoch   2 Batch  894/2813   train_loss = 0.902\n",
      "2021-04-22T23:55:49.754076: Epoch   2 Batch  914/2813   train_loss = 0.985\n",
      "2021-04-22T23:55:50.725515: Epoch   2 Batch  934/2813   train_loss = 1.039\n",
      "2021-04-22T23:55:51.142289: Epoch   2 Batch  954/2813   train_loss = 0.959\n",
      "2021-04-22T23:55:51.563033: Epoch   2 Batch  974/2813   train_loss = 0.834\n",
      "2021-04-22T23:55:51.992787: Epoch   2 Batch  994/2813   train_loss = 0.955\n",
      "2021-04-22T23:55:52.418541: Epoch   2 Batch 1014/2813   train_loss = 1.051\n",
      "2021-04-22T23:55:52.841311: Epoch   2 Batch 1034/2813   train_loss = 1.002\n",
      "2021-04-22T23:55:53.270063: Epoch   2 Batch 1054/2813   train_loss = 1.062\n",
      "2021-04-22T23:55:53.899725: Epoch   2 Batch 1074/2813   train_loss = 0.809\n",
      "2021-04-22T23:55:55.267898: Epoch   2 Batch 1094/2813   train_loss = 0.799\n",
      "2021-04-22T23:55:55.687656: Epoch   2 Batch 1114/2813   train_loss = 0.825\n",
      "2021-04-22T23:55:56.109424: Epoch   2 Batch 1134/2813   train_loss = 0.885\n",
      "2021-04-22T23:55:56.535180: Epoch   2 Batch 1154/2813   train_loss = 0.909\n",
      "2021-04-22T23:55:56.958925: Epoch   2 Batch 1174/2813   train_loss = 0.899\n",
      "2021-04-22T23:55:57.380683: Epoch   2 Batch 1194/2813   train_loss = 0.748\n",
      "2021-04-22T23:55:57.813448: Epoch   2 Batch 1214/2813   train_loss = 1.048\n",
      "2021-04-22T23:55:58.234190: Epoch   2 Batch 1234/2813   train_loss = 0.895\n",
      "2021-04-22T23:55:59.657410: Epoch   2 Batch 1254/2813   train_loss = 0.922\n",
      "2021-04-22T23:56:00.245047: Epoch   2 Batch 1274/2813   train_loss = 0.987\n",
      "2021-04-22T23:56:00.676794: Epoch   2 Batch 1294/2813   train_loss = 0.797\n",
      "2021-04-22T23:56:01.106537: Epoch   2 Batch 1314/2813   train_loss = 0.941\n",
      "2021-04-22T23:56:01.532292: Epoch   2 Batch 1334/2813   train_loss = 0.917\n",
      "2021-04-22T23:56:01.959060: Epoch   2 Batch 1354/2813   train_loss = 0.833\n",
      "2021-04-22T23:56:02.386815: Epoch   2 Batch 1374/2813   train_loss = 1.103\n",
      "2021-04-22T23:56:02.809566: Epoch   2 Batch 1394/2813   train_loss = 0.966\n",
      "2021-04-22T23:56:03.946903: Epoch   2 Batch 1414/2813   train_loss = 0.886\n",
      "2021-04-22T23:56:04.823396: Epoch   2 Batch 1434/2813   train_loss = 0.856\n",
      "2021-04-22T23:56:05.241155: Epoch   2 Batch 1454/2813   train_loss = 0.743\n",
      "2021-04-22T23:56:05.665921: Epoch   2 Batch 1474/2813   train_loss = 0.941\n",
      "2021-04-22T23:56:06.094679: Epoch   2 Batch 1494/2813   train_loss = 1.022\n",
      "2021-04-22T23:56:06.516421: Epoch   2 Batch 1514/2813   train_loss = 0.939\n",
      "2021-04-22T23:56:06.947187: Epoch   2 Batch 1534/2813   train_loss = 0.934\n",
      "2021-04-22T23:56:07.373939: Epoch   2 Batch 1554/2813   train_loss = 0.927\n",
      "2021-04-22T23:56:08.136490: Epoch   2 Batch 1574/2813   train_loss = 1.046\n",
      "2021-04-22T23:56:09.390674: Epoch   2 Batch 1594/2813   train_loss = 0.779\n",
      "2021-04-22T23:56:09.817443: Epoch   2 Batch 1614/2813   train_loss = 1.056\n",
      "2021-04-22T23:56:10.247182: Epoch   2 Batch 1634/2813   train_loss = 1.043\n",
      "2021-04-22T23:56:10.670939: Epoch   2 Batch 1654/2813   train_loss = 0.794\n",
      "2021-04-22T23:56:11.103703: Epoch   2 Batch 1674/2813   train_loss = 0.951\n",
      "2021-04-22T23:56:11.531452: Epoch   2 Batch 1694/2813   train_loss = 0.836\n",
      "2021-04-22T23:56:11.952201: Epoch   2 Batch 1714/2813   train_loss = 0.961\n",
      "2021-04-22T23:56:12.376955: Epoch   2 Batch 1734/2813   train_loss = 0.907\n",
      "2021-04-22T23:56:13.965040: Epoch   2 Batch 1754/2813   train_loss = 0.842\n",
      "2021-04-22T23:56:14.396794: Epoch   2 Batch 1774/2813   train_loss = 0.917\n",
      "2021-04-22T23:56:14.820562: Epoch   2 Batch 1794/2813   train_loss = 0.935\n",
      "2021-04-22T23:56:15.244305: Epoch   2 Batch 1814/2813   train_loss = 0.933\n",
      "2021-04-22T23:56:15.674072: Epoch   2 Batch 1834/2813   train_loss = 0.949\n",
      "2021-04-22T23:56:16.108805: Epoch   2 Batch 1854/2813   train_loss = 0.955\n",
      "2021-04-22T23:56:16.536560: Epoch   2 Batch 1874/2813   train_loss = 1.010\n",
      "2021-04-22T23:56:16.958317: Epoch   2 Batch 1894/2813   train_loss = 0.865\n",
      "2021-04-22T23:56:18.301550: Epoch   2 Batch 1914/2813   train_loss = 1.033\n",
      "2021-04-22T23:56:18.960165: Epoch   2 Batch 1934/2813   train_loss = 0.844\n",
      "2021-04-22T23:56:19.382935: Epoch   2 Batch 1954/2813   train_loss = 0.870\n",
      "2021-04-22T23:56:19.806686: Epoch   2 Batch 1974/2813   train_loss = 0.931\n",
      "2021-04-22T23:56:20.350373: Epoch   2 Batch 1994/2813   train_loss = 0.984\n",
      "2021-04-22T23:56:21.823516: Epoch   2 Batch 2014/2813   train_loss = 1.001\n",
      "2021-04-22T23:56:22.243398: Epoch   2 Batch 2034/2813   train_loss = 0.954\n",
      "2021-04-22T23:56:22.662555: Epoch   2 Batch 2054/2813   train_loss = 0.991\n",
      "2021-04-22T23:56:23.087310: Epoch   2 Batch 2074/2813   train_loss = 0.924\n",
      "2021-04-22T23:56:23.526058: Epoch   2 Batch 2094/2813   train_loss = 0.934\n",
      "2021-04-22T23:56:24.001784: Epoch   2 Batch 2114/2813   train_loss = 0.872\n",
      "2021-04-22T23:56:24.463519: Epoch   2 Batch 2134/2813   train_loss = 1.049\n",
      "2021-04-22T23:56:25.050182: Epoch   2 Batch 2154/2813   train_loss = 1.033\n",
      "2021-04-22T23:56:26.598289: Epoch   2 Batch 2174/2813   train_loss = 0.997\n",
      "2021-04-22T23:56:27.039034: Epoch   2 Batch 2194/2813   train_loss = 1.043\n",
      "2021-04-22T23:56:27.494774: Epoch   2 Batch 2214/2813   train_loss = 1.022\n",
      "2021-04-22T23:56:27.931535: Epoch   2 Batch 2234/2813   train_loss = 0.894\n",
      "2021-04-22T23:56:29.360701: Epoch   2 Batch 2254/2813   train_loss = 0.836\n",
      "2021-04-22T23:56:29.961352: Epoch   2 Batch 2274/2813   train_loss = 0.928\n",
      "2021-04-22T23:56:30.372115: Epoch   2 Batch 2294/2813   train_loss = 0.889\n",
      "2021-04-22T23:56:30.795883: Epoch   2 Batch 2314/2813   train_loss = 1.046\n",
      "2021-04-22T23:56:31.305581: Epoch   2 Batch 2334/2813   train_loss = 1.018\n",
      "2021-04-22T23:56:32.810711: Epoch   2 Batch 2354/2813   train_loss = 1.083\n",
      "2021-04-22T23:56:33.229470: Epoch   2 Batch 2374/2813   train_loss = 0.999\n",
      "2021-04-22T23:56:33.643231: Epoch   2 Batch 2394/2813   train_loss = 0.820\n",
      "2021-04-22T23:56:34.064988: Epoch   2 Batch 2414/2813   train_loss = 0.868\n",
      "2021-04-22T23:56:34.869528: Epoch   2 Batch 2434/2813   train_loss = 1.006\n",
      "2021-04-22T23:56:36.076831: Epoch   2 Batch 2454/2813   train_loss = 0.996\n",
      "2021-04-22T23:56:36.494590: Epoch   2 Batch 2474/2813   train_loss = 0.934\n",
      "2021-04-22T23:56:36.922162: Epoch   2 Batch 2494/2813   train_loss = 0.951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-22T23:56:37.347912: Epoch   2 Batch 2514/2813   train_loss = 0.939\n",
      "2021-04-22T23:56:38.511242: Epoch   2 Batch 2534/2813   train_loss = 1.067\n",
      "2021-04-22T23:56:39.360753: Epoch   2 Batch 2554/2813   train_loss = 0.801\n",
      "2021-04-22T23:56:39.777518: Epoch   2 Batch 2574/2813   train_loss = 0.839\n",
      "2021-04-22T23:56:40.201261: Epoch   2 Batch 2594/2813   train_loss = 0.831\n",
      "2021-04-22T23:56:40.626328: Epoch   2 Batch 2614/2813   train_loss = 0.903\n",
      "2021-04-22T23:56:42.120459: Epoch   2 Batch 2634/2813   train_loss = 0.913\n",
      "2021-04-22T23:56:42.661156: Epoch   2 Batch 2654/2813   train_loss = 0.926\n",
      "2021-04-22T23:56:43.108890: Epoch   2 Batch 2674/2813   train_loss = 0.818\n",
      "2021-04-22T23:56:43.561627: Epoch   2 Batch 2694/2813   train_loss = 1.006\n",
      "2021-04-22T23:56:44.532073: Epoch   2 Batch 2714/2813   train_loss = 0.899\n",
      "2021-04-22T23:56:45.652426: Epoch   2 Batch 2734/2813   train_loss = 0.864\n",
      "2021-04-22T23:56:46.114159: Epoch   2 Batch 2754/2813   train_loss = 0.941\n",
      "2021-04-22T23:56:46.567900: Epoch   2 Batch 2774/2813   train_loss = 0.801\n",
      "2021-04-22T23:56:47.090290: Epoch   2 Batch 2794/2813   train_loss = 0.918\n",
      "2021-04-22T23:56:48.740339: Epoch   2 Batch   16/312   test_loss = 0.793\n",
      "2021-04-22T23:56:48.909242: Epoch   2 Batch   36/312   test_loss = 1.037\n",
      "2021-04-22T23:56:49.092146: Epoch   2 Batch   56/312   test_loss = 0.895\n",
      "2021-04-22T23:56:49.257040: Epoch   2 Batch   76/312   test_loss = 0.860\n",
      "2021-04-22T23:56:49.425943: Epoch   2 Batch   96/312   test_loss = 0.885\n",
      "2021-04-22T23:56:49.591847: Epoch   2 Batch  116/312   test_loss = 0.819\n",
      "2021-04-22T23:56:49.757751: Epoch   2 Batch  136/312   test_loss = 1.076\n",
      "2021-04-22T23:56:49.922656: Epoch   2 Batch  156/312   test_loss = 1.170\n",
      "2021-04-22T23:56:50.087805: Epoch   2 Batch  176/312   test_loss = 0.826\n",
      "2021-04-22T23:56:50.254707: Epoch   2 Batch  196/312   test_loss = 0.881\n",
      "2021-04-22T23:56:50.424610: Epoch   2 Batch  216/312   test_loss = 0.778\n",
      "2021-04-22T23:56:50.605506: Epoch   2 Batch  236/312   test_loss = 0.910\n",
      "2021-04-22T23:56:50.776407: Epoch   2 Batch  256/312   test_loss = 0.985\n",
      "2021-04-22T23:56:50.940312: Epoch   2 Batch  276/312   test_loss = 1.041\n",
      "2021-04-22T23:56:51.106217: Epoch   2 Batch  296/312   test_loss = 0.810\n",
      "2021-04-22T23:56:51.523987: Epoch   3 Batch    1/2813   train_loss = 0.883\n",
      "2021-04-22T23:56:51.948746: Epoch   3 Batch   21/2813   train_loss = 0.925\n",
      "2021-04-22T23:56:52.378896: Epoch   3 Batch   41/2813   train_loss = 0.848\n",
      "2021-04-22T23:56:52.809661: Epoch   3 Batch   61/2813   train_loss = 0.891\n",
      "2021-04-22T23:56:53.254403: Epoch   3 Batch   81/2813   train_loss = 0.844\n",
      "2021-04-22T23:56:54.687565: Epoch   3 Batch  101/2813   train_loss = 1.024\n",
      "2021-04-22T23:56:55.262635: Epoch   3 Batch  121/2813   train_loss = 0.875\n",
      "2021-04-22T23:56:55.678395: Epoch   3 Batch  141/2813   train_loss = 0.827\n",
      "2021-04-22T23:56:56.095156: Epoch   3 Batch  161/2813   train_loss = 0.971\n",
      "2021-04-22T23:56:56.732794: Epoch   3 Batch  181/2813   train_loss = 0.867\n",
      "2021-04-22T23:56:58.124999: Epoch   3 Batch  201/2813   train_loss = 0.867\n",
      "2021-04-22T23:56:58.549754: Epoch   3 Batch  221/2813   train_loss = 0.785\n",
      "2021-04-22T23:56:58.966758: Epoch   3 Batch  241/2813   train_loss = 0.926\n",
      "2021-04-22T23:56:59.387512: Epoch   3 Batch  261/2813   train_loss = 0.751\n",
      "2021-04-22T23:56:59.810258: Epoch   3 Batch  281/2813   train_loss = 1.014\n",
      "2021-04-22T23:57:00.238012: Epoch   3 Batch  301/2813   train_loss = 0.801\n",
      "2021-04-22T23:57:00.666784: Epoch   3 Batch  321/2813   train_loss = 0.894\n",
      "2021-04-22T23:57:01.112523: Epoch   3 Batch  341/2813   train_loss = 0.912\n",
      "2021-04-22T23:57:02.552717: Epoch   3 Batch  361/2813   train_loss = 0.818\n",
      "2021-04-22T23:57:03.159387: Epoch   3 Batch  381/2813   train_loss = 0.901\n",
      "2021-04-22T23:57:03.582060: Epoch   3 Batch  401/2813   train_loss = 0.868\n",
      "2021-04-22T23:57:04.004808: Epoch   3 Batch  421/2813   train_loss = 0.835\n",
      "2021-04-22T23:57:04.433559: Epoch   3 Batch  441/2813   train_loss = 1.037\n",
      "2021-04-22T23:57:04.860328: Epoch   3 Batch  461/2813   train_loss = 0.847\n",
      "2021-04-22T23:57:05.281072: Epoch   3 Batch  481/2813   train_loss = 0.907\n",
      "2021-04-22T23:57:05.706841: Epoch   3 Batch  501/2813   train_loss = 0.890\n",
      "2021-04-22T23:57:06.855167: Epoch   3 Batch  521/2813   train_loss = 0.863\n",
      "2021-04-22T23:57:07.718681: Epoch   3 Batch  541/2813   train_loss = 0.827\n",
      "2021-04-22T23:57:08.145422: Epoch   3 Batch  561/2813   train_loss = 1.091\n",
      "2021-04-22T23:57:08.573395: Epoch   3 Batch  581/2813   train_loss = 0.996\n",
      "2021-04-22T23:57:08.992138: Epoch   3 Batch  601/2813   train_loss = 1.075\n",
      "2021-04-22T23:57:10.598214: Epoch   3 Batch  621/2813   train_loss = 0.854\n",
      "2021-04-22T23:57:11.015973: Epoch   3 Batch  641/2813   train_loss = 0.986\n",
      "2021-04-22T23:57:11.441728: Epoch   3 Batch  661/2813   train_loss = 0.865\n",
      "2021-04-22T23:57:11.872494: Epoch   3 Batch  681/2813   train_loss = 0.943\n",
      "2021-04-22T23:57:12.752012: Epoch   3 Batch  701/2813   train_loss = 0.852\n",
      "2021-04-22T23:57:13.895328: Epoch   3 Batch  721/2813   train_loss = 0.952\n",
      "2021-04-22T23:57:14.317088: Epoch   3 Batch  741/2813   train_loss = 0.827\n",
      "2021-04-22T23:57:14.738831: Epoch   3 Batch  761/2813   train_loss = 0.854\n",
      "2021-04-22T23:57:15.164599: Epoch   3 Batch  781/2813   train_loss = 1.008\n",
      "2021-04-22T23:57:15.587351: Epoch   3 Batch  801/2813   train_loss = 0.774\n",
      "2021-04-22T23:57:16.018093: Epoch   3 Batch  821/2813   train_loss = 0.957\n",
      "2021-04-22T23:57:16.451854: Epoch   3 Batch  841/2813   train_loss = 0.885\n",
      "2021-04-22T23:57:16.876495: Epoch   3 Batch  861/2813   train_loss = 0.978\n",
      "2021-04-22T23:57:18.478556: Epoch   3 Batch  881/2813   train_loss = 0.880\n",
      "2021-04-22T23:57:18.899326: Epoch   3 Batch  901/2813   train_loss = 1.118\n",
      "2021-04-22T23:57:19.329064: Epoch   3 Batch  921/2813   train_loss = 0.921\n",
      "2021-04-22T23:57:19.747824: Epoch   3 Batch  941/2813   train_loss = 0.901\n",
      "2021-04-22T23:57:20.579354: Epoch   3 Batch  961/2813   train_loss = 1.004\n",
      "2021-04-22T23:57:21.766662: Epoch   3 Batch  981/2813   train_loss = 0.812\n",
      "2021-04-22T23:57:22.193842: Epoch   3 Batch 1001/2813   train_loss = 0.844\n",
      "2021-04-22T23:57:22.640594: Epoch   3 Batch 1021/2813   train_loss = 0.908\n",
      "2021-04-22T23:57:23.083331: Epoch   3 Batch 1041/2813   train_loss = 1.004\n",
      "2021-04-22T23:57:24.640435: Epoch   3 Batch 1061/2813   train_loss = 0.820\n",
      "2021-04-22T23:57:25.166130: Epoch   3 Batch 1081/2813   train_loss = 1.061\n",
      "2021-04-22T23:57:25.601879: Epoch   3 Batch 1101/2813   train_loss = 0.912\n",
      "2021-04-22T23:57:26.018639: Epoch   3 Batch 1121/2813   train_loss = 0.912\n",
      "2021-04-22T23:57:26.976090: Epoch   3 Batch 1141/2813   train_loss = 0.880\n",
      "2021-04-22T23:57:28.109965: Epoch   3 Batch 1161/2813   train_loss = 0.770\n",
      "2021-04-22T23:57:28.545713: Epoch   3 Batch 1181/2813   train_loss = 0.944\n",
      "2021-04-22T23:57:28.974453: Epoch   3 Batch 1201/2813   train_loss = 1.001\n",
      "2021-04-22T23:57:29.418197: Epoch   3 Batch 1221/2813   train_loss = 1.033\n",
      "2021-04-22T23:57:30.873365: Epoch   3 Batch 1241/2813   train_loss = 0.890\n",
      "2021-04-22T23:57:31.454038: Epoch   3 Batch 1261/2813   train_loss = 0.848\n",
      "2021-04-22T23:57:31.870798: Epoch   3 Batch 1281/2813   train_loss = 0.782\n",
      "2021-04-22T23:57:32.304536: Epoch   3 Batch 1301/2813   train_loss = 0.922\n",
      "2021-04-22T23:57:32.893233: Epoch   3 Batch 1321/2813   train_loss = 1.130\n",
      "2021-04-22T23:57:34.320374: Epoch   3 Batch 1341/2813   train_loss = 0.932\n",
      "2021-04-22T23:57:34.742146: Epoch   3 Batch 1361/2813   train_loss = 0.856\n",
      "2021-04-22T23:57:35.166885: Epoch   3 Batch 1381/2813   train_loss = 0.926\n",
      "2021-04-22T23:57:35.594641: Epoch   3 Batch 1401/2813   train_loss = 1.066\n",
      "2021-04-22T23:57:36.575080: Epoch   3 Batch 1421/2813   train_loss = 0.896\n",
      "2021-04-22T23:57:37.622473: Epoch   3 Batch 1441/2813   train_loss = 0.754\n",
      "2021-04-22T23:57:38.053238: Epoch   3 Batch 1461/2813   train_loss = 0.915\n",
      "2021-04-22T23:57:38.479979: Epoch   3 Batch 1481/2813   train_loss = 0.818\n",
      "2021-04-22T23:57:38.910730: Epoch   3 Batch 1501/2813   train_loss = 0.887\n",
      "2021-04-22T23:57:40.273954: Epoch   3 Batch 1521/2813   train_loss = 0.898\n",
      "2021-04-22T23:57:40.903592: Epoch   3 Batch 1541/2813   train_loss = 0.985\n",
      "2021-04-22T23:57:41.328350: Epoch   3 Batch 1561/2813   train_loss = 0.799\n",
      "2021-04-22T23:57:41.748096: Epoch   3 Batch 1581/2813   train_loss = 0.938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-22T23:57:42.170422: Epoch   3 Batch 1601/2813   train_loss = 0.810\n",
      "2021-04-22T23:57:43.757459: Epoch   3 Batch 1621/2813   train_loss = 0.848\n",
      "2021-04-22T23:57:44.191494: Epoch   3 Batch 1641/2813   train_loss = 0.916\n",
      "2021-04-22T23:57:44.612236: Epoch   3 Batch 1661/2813   train_loss = 0.899\n",
      "2021-04-22T23:57:45.033994: Epoch   3 Batch 1681/2813   train_loss = 1.164\n",
      "2021-04-22T23:57:45.873514: Epoch   3 Batch 1701/2813   train_loss = 0.960\n",
      "2021-04-22T23:57:47.051763: Epoch   3 Batch 1721/2813   train_loss = 1.003\n",
      "2021-04-22T23:57:47.477516: Epoch   3 Batch 1741/2813   train_loss = 1.046\n",
      "2021-04-22T23:57:47.897264: Epoch   3 Batch 1761/2813   train_loss = 0.974\n",
      "2021-04-22T23:57:48.324029: Epoch   3 Batch 1781/2813   train_loss = 0.869\n",
      "2021-04-22T23:57:49.502343: Epoch   3 Batch 1801/2813   train_loss = 0.878\n",
      "2021-04-22T23:57:50.345248: Epoch   3 Batch 1821/2813   train_loss = 0.786\n",
      "2021-04-22T23:57:50.773000: Epoch   3 Batch 1841/2813   train_loss = 0.955\n",
      "2021-04-22T23:57:51.198755: Epoch   3 Batch 1861/2813   train_loss = 0.852\n",
      "2021-04-22T23:57:51.622510: Epoch   3 Batch 1881/2813   train_loss = 0.865\n",
      "2021-04-22T23:57:53.193892: Epoch   3 Batch 1901/2813   train_loss = 0.871\n",
      "2021-04-22T23:57:53.639633: Epoch   3 Batch 1921/2813   train_loss = 0.838\n",
      "2021-04-22T23:57:54.061391: Epoch   3 Batch 1941/2813   train_loss = 0.887\n",
      "2021-04-22T23:57:54.482148: Epoch   3 Batch 1961/2813   train_loss = 0.900\n",
      "2021-04-22T23:57:55.166756: Epoch   3 Batch 1981/2813   train_loss = 0.805\n",
      "2021-04-22T23:57:56.501986: Epoch   3 Batch 2001/2813   train_loss = 0.821\n",
      "2021-04-22T23:57:56.929181: Epoch   3 Batch 2021/2813   train_loss = 0.947\n",
      "2021-04-22T23:57:57.350928: Epoch   3 Batch 2041/2813   train_loss = 0.922\n",
      "2021-04-22T23:57:57.770697: Epoch   3 Batch 2061/2813   train_loss = 0.933\n",
      "2021-04-22T23:57:58.843071: Epoch   3 Batch 2081/2813   train_loss = 0.793\n",
      "2021-04-22T23:57:59.800516: Epoch   3 Batch 2101/2813   train_loss = 0.989\n",
      "2021-04-22T23:58:00.221275: Epoch   3 Batch 2121/2813   train_loss = 0.885\n",
      "2021-04-22T23:58:00.640050: Epoch   3 Batch 2141/2813   train_loss = 0.989\n",
      "2021-04-22T23:58:01.078781: Epoch   3 Batch 2161/2813   train_loss = 0.899\n",
      "2021-04-22T23:58:02.539941: Epoch   3 Batch 2181/2813   train_loss = 0.829\n",
      "2021-04-22T23:58:03.106628: Epoch   3 Batch 2201/2813   train_loss = 0.791\n",
      "2021-04-22T23:58:03.531379: Epoch   3 Batch 2221/2813   train_loss = 0.893\n",
      "2021-04-22T23:58:03.958138: Epoch   3 Batch 2241/2813   train_loss = 0.842\n",
      "2021-04-22T23:58:04.385876: Epoch   3 Batch 2261/2813   train_loss = 0.918\n",
      "2021-04-22T23:58:04.817638: Epoch   3 Batch 2281/2813   train_loss = 0.893\n",
      "2021-04-22T23:58:05.253100: Epoch   3 Batch 2301/2813   train_loss = 1.003\n",
      "2021-04-22T23:58:05.675872: Epoch   3 Batch 2321/2813   train_loss = 0.807\n",
      "2021-04-22T23:58:06.716295: Epoch   3 Batch 2341/2813   train_loss = 0.940\n",
      "2021-04-22T23:58:07.698706: Epoch   3 Batch 2361/2813   train_loss = 0.960\n",
      "2021-04-22T23:58:08.123447: Epoch   3 Batch 2381/2813   train_loss = 0.868\n",
      "2021-04-22T23:58:08.551215: Epoch   3 Batch 2401/2813   train_loss = 0.882\n",
      "2021-04-22T23:58:08.978955: Epoch   3 Batch 2421/2813   train_loss = 0.898\n",
      "2021-04-22T23:58:09.406724: Epoch   3 Batch 2441/2813   train_loss = 0.871\n",
      "2021-04-22T23:58:09.838474: Epoch   3 Batch 2461/2813   train_loss = 0.777\n",
      "2021-04-22T23:58:10.264653: Epoch   3 Batch 2481/2813   train_loss = 0.838\n",
      "2021-04-22T23:58:10.984780: Epoch   3 Batch 2501/2813   train_loss = 0.829\n",
      "2021-04-22T23:58:12.273999: Epoch   3 Batch 2521/2813   train_loss = 1.064\n",
      "2021-04-22T23:58:12.695771: Epoch   3 Batch 2541/2813   train_loss = 0.780\n",
      "2021-04-22T23:58:13.122527: Epoch   3 Batch 2561/2813   train_loss = 0.843\n",
      "2021-04-22T23:58:13.547149: Epoch   3 Batch 2581/2813   train_loss = 0.934\n",
      "2021-04-22T23:58:14.687494: Epoch   3 Batch 2601/2813   train_loss = 0.841\n",
      "2021-04-22T23:58:15.554008: Epoch   3 Batch 2621/2813   train_loss = 0.851\n",
      "2021-04-22T23:58:15.973767: Epoch   3 Batch 2641/2813   train_loss = 0.800\n",
      "2021-04-22T23:58:16.403519: Epoch   3 Batch 2661/2813   train_loss = 1.016\n",
      "2021-04-22T23:58:16.825265: Epoch   3 Batch 2681/2813   train_loss = 0.853\n",
      "2021-04-22T23:58:18.360381: Epoch   3 Batch 2701/2813   train_loss = 0.860\n",
      "2021-04-22T23:58:18.833105: Epoch   3 Batch 2721/2813   train_loss = 1.044\n",
      "2021-04-22T23:58:19.254877: Epoch   3 Batch 2741/2813   train_loss = 0.827\n",
      "2021-04-22T23:58:19.675630: Epoch   3 Batch 2761/2813   train_loss = 0.873\n",
      "2021-04-22T23:58:20.313256: Epoch   3 Batch 2781/2813   train_loss = 0.923\n",
      "2021-04-22T23:58:21.681465: Epoch   3 Batch 2801/2813   train_loss = 0.887\n",
      "2021-04-22T23:58:21.966911: Epoch   3 Batch    4/312   test_loss = 0.795\n",
      "2021-04-22T23:58:22.131807: Epoch   3 Batch   24/312   test_loss = 0.967\n",
      "2021-04-22T23:58:22.295722: Epoch   3 Batch   44/312   test_loss = 0.931\n",
      "2021-04-22T23:58:22.459618: Epoch   3 Batch   64/312   test_loss = 0.860\n",
      "2021-04-22T23:58:22.624524: Epoch   3 Batch   84/312   test_loss = 0.945\n",
      "2021-04-22T23:58:22.787441: Epoch   3 Batch  104/312   test_loss = 0.973\n",
      "2021-04-22T23:58:22.952335: Epoch   3 Batch  124/312   test_loss = 1.032\n",
      "2021-04-22T23:58:23.115241: Epoch   3 Batch  144/312   test_loss = 0.940\n",
      "2021-04-22T23:58:23.283144: Epoch   3 Batch  164/312   test_loss = 0.894\n",
      "2021-04-22T23:58:23.447049: Epoch   3 Batch  184/312   test_loss = 0.973\n",
      "2021-04-22T23:58:23.611881: Epoch   3 Batch  204/312   test_loss = 0.978\n",
      "2021-04-22T23:58:23.774787: Epoch   3 Batch  224/312   test_loss = 0.953\n",
      "2021-04-22T23:58:23.940702: Epoch   3 Batch  244/312   test_loss = 0.963\n",
      "2021-04-22T23:58:24.102427: Epoch   3 Batch  264/312   test_loss = 1.121\n",
      "2021-04-22T23:58:24.269331: Epoch   3 Batch  284/312   test_loss = 0.865\n",
      "2021-04-22T23:58:24.434236: Epoch   3 Batch  304/312   test_loss = 0.871\n",
      "2021-04-22T23:58:24.927966: Epoch   4 Batch    8/2813   train_loss = 0.945\n",
      "2021-04-22T23:58:25.347885: Epoch   4 Batch   28/2813   train_loss = 0.840\n",
      "2021-04-22T23:58:25.762471: Epoch   4 Batch   48/2813   train_loss = 0.988\n",
      "2021-04-22T23:58:26.191211: Epoch   4 Batch   68/2813   train_loss = 0.753\n",
      "2021-04-22T23:58:27.380526: Epoch   4 Batch   88/2813   train_loss = 0.924\n",
      "2021-04-22T23:58:28.196305: Epoch   4 Batch  108/2813   train_loss = 0.971\n",
      "2021-04-22T23:58:28.614139: Epoch   4 Batch  128/2813   train_loss = 0.909\n",
      "2021-04-22T23:58:29.042900: Epoch   4 Batch  148/2813   train_loss = 0.970\n",
      "2021-04-22T23:58:29.466643: Epoch   4 Batch  168/2813   train_loss = 0.965\n",
      "2021-04-22T23:58:31.049768: Epoch   4 Batch  188/2813   train_loss = 0.891\n",
      "2021-04-22T23:58:31.481496: Epoch   4 Batch  208/2813   train_loss = 0.925\n",
      "2021-04-22T23:58:31.906602: Epoch   4 Batch  228/2813   train_loss = 0.915\n",
      "2021-04-22T23:58:32.326361: Epoch   4 Batch  248/2813   train_loss = 0.858\n",
      "2021-04-22T23:58:33.131885: Epoch   4 Batch  268/2813   train_loss = 1.010\n",
      "2021-04-22T23:58:34.363173: Epoch   4 Batch  288/2813   train_loss = 0.828\n",
      "2021-04-22T23:58:34.778934: Epoch   4 Batch  308/2813   train_loss = 0.763\n",
      "2021-04-22T23:58:35.205879: Epoch   4 Batch  328/2813   train_loss = 0.867\n",
      "2021-04-22T23:58:35.630635: Epoch   4 Batch  348/2813   train_loss = 0.723\n",
      "2021-04-22T23:58:36.780540: Epoch   4 Batch  368/2813   train_loss = 0.983\n",
      "2021-04-22T23:58:37.644042: Epoch   4 Batch  388/2813   train_loss = 1.039\n",
      "2021-04-22T23:58:38.063799: Epoch   4 Batch  408/2813   train_loss = 0.896\n",
      "2021-04-22T23:58:38.490568: Epoch   4 Batch  428/2813   train_loss = 0.869\n",
      "2021-04-22T23:58:38.918324: Epoch   4 Batch  448/2813   train_loss = 0.841\n",
      "2021-04-22T23:58:40.441468: Epoch   4 Batch  468/2813   train_loss = 0.919\n",
      "2021-04-22T23:58:40.953150: Epoch   4 Batch  488/2813   train_loss = 0.820\n",
      "2021-04-22T23:58:41.377892: Epoch   4 Batch  508/2813   train_loss = 0.850\n",
      "2021-04-22T23:58:41.797203: Epoch   4 Batch  528/2813   train_loss = 0.910\n",
      "2021-04-22T23:58:42.216207: Epoch   4 Batch  548/2813   train_loss = 0.749\n",
      "2021-04-22T23:58:42.645973: Epoch   4 Batch  568/2813   train_loss = 0.775\n",
      "2021-04-22T23:58:43.082718: Epoch   4 Batch  588/2813   train_loss = 0.962\n",
      "2021-04-22T23:58:43.519170: Epoch   4 Batch  608/2813   train_loss = 0.926\n",
      "2021-04-22T23:58:44.550570: Epoch   4 Batch  628/2813   train_loss = 0.930\n",
      "2021-04-22T23:58:45.533001: Epoch   4 Batch  648/2813   train_loss = 0.883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-22T23:58:45.957770: Epoch   4 Batch  668/2813   train_loss = 0.706\n",
      "2021-04-22T23:58:46.381512: Epoch   4 Batch  688/2813   train_loss = 0.895\n",
      "2021-04-22T23:58:46.810561: Epoch   4 Batch  708/2813   train_loss = 0.950\n",
      "2021-04-22T23:58:48.300711: Epoch   4 Batch  728/2813   train_loss = 0.871\n",
      "2021-04-22T23:58:48.822718: Epoch   4 Batch  748/2813   train_loss = 0.969\n",
      "2021-04-22T23:58:49.242475: Epoch   4 Batch  768/2813   train_loss = 0.963\n",
      "2021-04-22T23:58:49.663233: Epoch   4 Batch  788/2813   train_loss = 0.826\n",
      "2021-04-22T23:58:50.311847: Epoch   4 Batch  808/2813   train_loss = 0.961\n",
      "2021-04-22T23:58:51.683724: Epoch   4 Batch  828/2813   train_loss = 0.844\n",
      "2021-04-22T23:58:52.098485: Epoch   4 Batch  848/2813   train_loss = 0.823\n",
      "2021-04-22T23:58:52.531472: Epoch   4 Batch  868/2813   train_loss = 0.895\n",
      "2021-04-22T23:58:52.955224: Epoch   4 Batch  888/2813   train_loss = 0.851\n",
      "2021-04-22T23:58:53.972630: Epoch   4 Batch  908/2813   train_loss = 0.881\n",
      "2021-04-22T23:58:54.973067: Epoch   4 Batch  928/2813   train_loss = 1.009\n",
      "2021-04-22T23:58:55.395818: Epoch   4 Batch  948/2813   train_loss = 0.869\n",
      "2021-04-22T23:58:55.822577: Epoch   4 Batch  968/2813   train_loss = 0.916\n",
      "2021-04-22T23:58:56.258328: Epoch   4 Batch  988/2813   train_loss = 0.801\n",
      "2021-04-22T23:58:56.688079: Epoch   4 Batch 1008/2813   train_loss = 0.831\n",
      "2021-04-22T23:58:57.116751: Epoch   4 Batch 1028/2813   train_loss = 0.796\n",
      "2021-04-22T23:58:57.555498: Epoch   4 Batch 1048/2813   train_loss = 0.936\n",
      "2021-04-22T23:58:58.161137: Epoch   4 Batch 1068/2813   train_loss = 0.932\n",
      "2021-04-22T23:58:59.571322: Epoch   4 Batch 1088/2813   train_loss = 0.740\n",
      "2021-04-22T23:58:59.987769: Epoch   4 Batch 1108/2813   train_loss = 0.824\n",
      "2021-04-22T23:59:00.414539: Epoch   4 Batch 1128/2813   train_loss = 0.948\n",
      "2021-04-22T23:59:00.833286: Epoch   4 Batch 1148/2813   train_loss = 0.787\n",
      "2021-04-22T23:59:01.835711: Epoch   4 Batch 1168/2813   train_loss = 0.838\n",
      "2021-04-22T23:59:02.908160: Epoch   4 Batch 1188/2813   train_loss = 0.894\n",
      "2021-04-22T23:59:03.337105: Epoch   4 Batch 1208/2813   train_loss = 0.880\n",
      "2021-04-22T23:59:03.757862: Epoch   4 Batch 1228/2813   train_loss = 0.806\n",
      "2021-04-22T23:59:04.185618: Epoch   4 Batch 1248/2813   train_loss = 1.063\n",
      "2021-04-22T23:59:05.773690: Epoch   4 Batch 1268/2813   train_loss = 0.817\n",
      "2021-04-22T23:59:06.206452: Epoch   4 Batch 1288/2813   train_loss = 1.000\n",
      "2021-04-22T23:59:06.629756: Epoch   4 Batch 1308/2813   train_loss = 1.001\n",
      "2021-04-22T23:59:07.056515: Epoch   4 Batch 1328/2813   train_loss = 0.964\n",
      "2021-04-22T23:59:07.816065: Epoch   4 Batch 1348/2813   train_loss = 0.774\n",
      "2021-04-22T23:59:09.091328: Epoch   4 Batch 1368/2813   train_loss = 0.918\n",
      "2021-04-22T23:59:09.524617: Epoch   4 Batch 1388/2813   train_loss = 0.825\n",
      "2021-04-22T23:59:09.938382: Epoch   4 Batch 1408/2813   train_loss = 0.881\n",
      "2021-04-22T23:59:10.362137: Epoch   4 Batch 1428/2813   train_loss = 0.929\n",
      "2021-04-22T23:59:11.579439: Epoch   4 Batch 1448/2813   train_loss = 0.806\n",
      "2021-04-22T23:59:12.430943: Epoch   4 Batch 1468/2813   train_loss = 0.881\n",
      "2021-04-22T23:59:12.875689: Epoch   4 Batch 1488/2813   train_loss = 0.914\n",
      "2021-04-22T23:59:13.323430: Epoch   4 Batch 1508/2813   train_loss = 0.915\n",
      "2021-04-22T23:59:13.861125: Epoch   4 Batch 1528/2813   train_loss = 0.867\n",
      "2021-04-22T23:59:15.350263: Epoch   4 Batch 1548/2813   train_loss = 1.122\n",
      "2021-04-22T23:59:15.780018: Epoch   4 Batch 1568/2813   train_loss = 0.823\n",
      "2021-04-22T23:59:16.209782: Epoch   4 Batch 1588/2813   train_loss = 0.762\n",
      "2021-04-22T23:59:16.636522: Epoch   4 Batch 1608/2813   train_loss = 0.745\n",
      "2021-04-22T23:59:17.710943: Epoch   4 Batch 1628/2813   train_loss = 0.777\n",
      "2021-04-22T23:59:18.724321: Epoch   4 Batch 1648/2813   train_loss = 0.763\n",
      "2021-04-22T23:59:19.150093: Epoch   4 Batch 1668/2813   train_loss = 0.847\n",
      "2021-04-22T23:59:19.572845: Epoch   4 Batch 1688/2813   train_loss = 0.818\n",
      "2021-04-22T23:59:19.991591: Epoch   4 Batch 1708/2813   train_loss = 0.768\n",
      "2021-04-22T23:59:21.513752: Epoch   4 Batch 1728/2813   train_loss = 0.939\n",
      "2021-04-22T23:59:22.027418: Epoch   4 Batch 1748/2813   train_loss = 0.990\n",
      "2021-04-22T23:59:22.443178: Epoch   4 Batch 1768/2813   train_loss = 0.863\n",
      "2021-04-22T23:59:22.863950: Epoch   4 Batch 1788/2813   train_loss = 0.883\n",
      "2021-04-22T23:59:23.534552: Epoch   4 Batch 1808/2813   train_loss = 0.897\n",
      "2021-04-22T23:59:24.944737: Epoch   4 Batch 1828/2813   train_loss = 0.838\n",
      "2021-04-22T23:59:25.369507: Epoch   4 Batch 1848/2813   train_loss = 0.894\n",
      "2021-04-22T23:59:25.797256: Epoch   4 Batch 1868/2813   train_loss = 0.912\n",
      "2021-04-22T23:59:26.222291: Epoch   4 Batch 1888/2813   train_loss = 0.753\n",
      "2021-04-22T23:59:27.462564: Epoch   4 Batch 1908/2813   train_loss = 0.856\n",
      "2021-04-22T23:59:28.256105: Epoch   4 Batch 1928/2813   train_loss = 0.710\n",
      "2021-04-22T23:59:28.673805: Epoch   4 Batch 1948/2813   train_loss = 0.759\n",
      "2021-04-22T23:59:29.096550: Epoch   4 Batch 1968/2813   train_loss = 0.772\n",
      "2021-04-22T23:59:29.517322: Epoch   4 Batch 1988/2813   train_loss = 0.930\n",
      "2021-04-22T23:59:31.074413: Epoch   4 Batch 2008/2813   train_loss = 0.741\n",
      "2021-04-22T23:59:31.534942: Epoch   4 Batch 2028/2813   train_loss = 0.908\n",
      "2021-04-22T23:59:31.958712: Epoch   4 Batch 2048/2813   train_loss = 0.887\n",
      "2021-04-22T23:59:32.381469: Epoch   4 Batch 2068/2813   train_loss = 0.920\n",
      "2021-04-22T23:59:33.082053: Epoch   4 Batch 2088/2813   train_loss = 0.843\n",
      "2021-04-22T23:59:34.394296: Epoch   4 Batch 2108/2813   train_loss = 0.890\n",
      "2021-04-22T23:59:34.812055: Epoch   4 Batch 2128/2813   train_loss = 0.770\n",
      "2021-04-22T23:59:35.229828: Epoch   4 Batch 2148/2813   train_loss = 0.757\n",
      "2021-04-22T23:59:35.653584: Epoch   4 Batch 2168/2813   train_loss = 0.847\n",
      "2021-04-22T23:59:36.603025: Epoch   4 Batch 2188/2813   train_loss = 0.860\n",
      "2021-04-22T23:59:37.659417: Epoch   4 Batch 2208/2813   train_loss = 0.968\n",
      "2021-04-22T23:59:38.085170: Epoch   4 Batch 2228/2813   train_loss = 0.943\n",
      "2021-04-22T23:59:38.507936: Epoch   4 Batch 2248/2813   train_loss = 0.796\n",
      "2021-04-22T23:59:38.934682: Epoch   4 Batch 2268/2813   train_loss = 0.808\n",
      "2021-04-22T23:59:40.234937: Epoch   4 Batch 2288/2813   train_loss = 0.886\n",
      "2021-04-22T23:59:40.945537: Epoch   4 Batch 2308/2813   train_loss = 0.980\n",
      "2021-04-22T23:59:41.364283: Epoch   4 Batch 2328/2813   train_loss = 1.126\n",
      "2021-04-22T23:59:41.788039: Epoch   4 Batch 2348/2813   train_loss = 0.895\n",
      "2021-04-22T23:59:42.209810: Epoch   4 Batch 2368/2813   train_loss = 0.723\n",
      "2021-04-22T23:59:43.796881: Epoch   4 Batch 2388/2813   train_loss = 0.820\n",
      "2021-04-22T23:59:44.217639: Epoch   4 Batch 2408/2813   train_loss = 0.888\n",
      "2021-04-22T23:59:44.646392: Epoch   4 Batch 2428/2813   train_loss = 1.014\n",
      "2021-04-22T23:59:45.067736: Epoch   4 Batch 2448/2813   train_loss = 0.776\n",
      "2021-04-22T23:59:45.800317: Epoch   4 Batch 2468/2813   train_loss = 0.871\n",
      "2021-04-22T23:59:47.095588: Epoch   4 Batch 2488/2813   train_loss = 0.872\n",
      "2021-04-22T23:59:47.519336: Epoch   4 Batch 2508/2813   train_loss = 0.908\n",
      "2021-04-22T23:59:47.934101: Epoch   4 Batch 2528/2813   train_loss = 0.855\n",
      "2021-04-22T23:59:48.356857: Epoch   4 Batch 2548/2813   train_loss = 0.958\n",
      "2021-04-22T23:59:49.474202: Epoch   4 Batch 2568/2813   train_loss = 0.789\n",
      "2021-04-22T23:59:50.386688: Epoch   4 Batch 2588/2813   train_loss = 0.722\n",
      "2021-04-22T23:59:50.808431: Epoch   4 Batch 2608/2813   train_loss = 0.929\n",
      "2021-04-22T23:59:51.226190: Epoch   4 Batch 2628/2813   train_loss = 0.895\n",
      "2021-04-22T23:59:51.656956: Epoch   4 Batch 2648/2813   train_loss = 0.948\n",
      "2021-04-22T23:59:52.079713: Epoch   4 Batch 2668/2813   train_loss = 0.848\n",
      "2021-04-22T23:59:52.616390: Epoch   4 Batch 2688/2813   train_loss = 0.895\n",
      "2021-04-22T23:59:53.044956: Epoch   4 Batch 2708/2813   train_loss = 0.815\n",
      "2021-04-22T23:59:53.474694: Epoch   4 Batch 2728/2813   train_loss = 0.902\n",
      "2021-04-22T23:59:53.914392: Epoch   4 Batch 2748/2813   train_loss = 0.868\n",
      "2021-04-22T23:59:54.341146: Epoch   4 Batch 2768/2813   train_loss = 0.861\n",
      "2021-04-22T23:59:54.773897: Epoch   4 Batch 2788/2813   train_loss = 0.783\n",
      "2021-04-22T23:59:56.121108: Epoch   4 Batch 2808/2813   train_loss = 0.699\n",
      "2021-04-22T23:59:56.560853: Epoch   4 Batch   12/312   test_loss = 0.849\n",
      "2021-04-22T23:59:56.726527: Epoch   4 Batch   32/312   test_loss = 0.831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-22T23:59:56.894172: Epoch   4 Batch   52/312   test_loss = 0.900\n",
      "2021-04-22T23:59:57.059078: Epoch   4 Batch   72/312   test_loss = 0.910\n",
      "2021-04-22T23:59:57.225610: Epoch   4 Batch   92/312   test_loss = 0.797\n",
      "2021-04-22T23:59:57.388507: Epoch   4 Batch  112/312   test_loss = 0.895\n",
      "2021-04-22T23:59:57.556410: Epoch   4 Batch  132/312   test_loss = 0.759\n",
      "2021-04-22T23:59:57.721313: Epoch   4 Batch  152/312   test_loss = 0.883\n",
      "2021-04-22T23:59:57.886219: Epoch   4 Batch  172/312   test_loss = 0.921\n",
      "2021-04-22T23:59:58.053125: Epoch   4 Batch  192/312   test_loss = 0.889\n",
      "2021-04-22T23:59:58.220027: Epoch   4 Batch  212/312   test_loss = 0.791\n",
      "2021-04-22T23:59:58.385932: Epoch   4 Batch  232/312   test_loss = 0.840\n",
      "2021-04-22T23:59:58.555833: Epoch   4 Batch  252/312   test_loss = 0.823\n",
      "2021-04-22T23:59:58.739737: Epoch   4 Batch  272/312   test_loss = 0.930\n",
      "2021-04-22T23:59:58.905646: Epoch   4 Batch  292/312   test_loss = 0.746\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "losses = {'train':[], 'test':[]}\n",
    "\n",
    "with tf.Session(graph=train_graph, config=config) as sess:\n",
    "    \n",
    "    #搜集数据给tensorBoard用\n",
    "    # Keep track of gradient values and sparsity\n",
    "    grad_summaries = []\n",
    "    for g, v in gradients:\n",
    "        if g is not None:\n",
    "            grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name.replace(':', '_')), g)\n",
    "            sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name.replace(':', '_')), tf.nn.zero_fraction(g))\n",
    "            grad_summaries.append(grad_hist_summary)\n",
    "            grad_summaries.append(sparsity_summary)\n",
    "    grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "        \n",
    "    # Output directory for models and summaries\n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "    print(\"Writing to {}\\n\".format(out_dir))\n",
    "     \n",
    "    # Summaries for loss and accuracy\n",
    "    loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "    # Train Summaries\n",
    "    train_summary_op = tf.summary.merge([loss_summary, grad_summaries_merged])\n",
    "    train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "    train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "    # Inference summaries\n",
    "    inference_summary_op = tf.summary.merge([loss_summary])\n",
    "    inference_summary_dir = os.path.join(out_dir, \"summaries\", \"inference\")\n",
    "    inference_summary_writer = tf.summary.FileWriter(inference_summary_dir, sess.graph)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    for epoch_i in range(num_epochs):\n",
    "        \n",
    "        #cross validation\n",
    "#         train_X,test_X, train_y, test_y = train_test_split(features,  \n",
    "#                                                            targets_values,  \n",
    "#                                                            test_size = 0.2,  \n",
    "#                                                            random_state = 0)\n",
    "        train_X,test_X, train_y, test_y = train_test_split(train_data,  \n",
    "                                                           train_targets,  \n",
    "                                                           test_size = 0.1,  \n",
    "                                                           random_state = None) \n",
    "        \n",
    "        train_batches = get_batches(train_X, train_y, batch_size)\n",
    "        test_batches = get_batches(test_X, test_y, batch_size)\n",
    "    \n",
    "        #训练的迭代，保存训练损失\n",
    "        for batch_i in range(len(train_X) // batch_size):\n",
    "            x, y = next(train_batches)\n",
    "\n",
    "            categories = np.zeros([batch_size, 18])\n",
    "            for i in range(batch_size):\n",
    "                categories[i] = x.take(6,1)[i]\n",
    "\n",
    "            titles = np.zeros([batch_size, sentences_size])\n",
    "            for i in range(batch_size):\n",
    "                titles[i] = x.take(5,1)[i]\n",
    "            \n",
    "            feed = {\n",
    "                uid: np.reshape(x.take(0,1), [batch_size, 1]),\n",
    "                user_gender: np.reshape(x.take(2,1), [batch_size, 1]),\n",
    "                user_age: np.reshape(x.take(3,1), [batch_size, 1]),\n",
    "                user_job: np.reshape(x.take(4,1), [batch_size, 1]),\n",
    "                movie_id: np.reshape(x.take(1,1), [batch_size, 1]),\n",
    "                movie_categories: categories,  #x.take(6,1)\n",
    "                movie_titles: titles,  #x.take(5,1)\n",
    "                targets: np.reshape(y, [batch_size, 1]),\n",
    "                dropout_keep_prob: dropout_keep, #dropout_keep\n",
    "                lr: learning_rate}\n",
    "            \n",
    "            step, train_loss, summaries, _ = sess.run([global_step, loss, train_summary_op, train_op], feed)  #cost\n",
    "            \n",
    "            losses['train'].append(train_loss)\n",
    "            train_summary_writer.add_summary(summaries, step)  #\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * (len(train_X) // batch_size) + batch_i) % show_every_n_batches == 0:\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print('{}: Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    time_str,\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    (len(train_X) // batch_size),\n",
    "                    train_loss))\n",
    "                \n",
    "        #使用测试数据的迭代\n",
    "        for batch_i  in range(len(test_X) // batch_size):\n",
    "            x, y = next(test_batches)\n",
    "            \n",
    "            categories = np.zeros([batch_size, 18])\n",
    "            for i in range(batch_size):\n",
    "                categories[i] = x.take(6,1)[i]\n",
    "\n",
    "            titles = np.zeros([batch_size, sentences_size])\n",
    "            for i in range(batch_size):\n",
    "                titles[i] = x.take(5,1)[i]\n",
    "\n",
    "            feed = {\n",
    "                uid: np.reshape(x.take(0,1), [batch_size, 1]),\n",
    "                user_gender: np.reshape(x.take(2,1), [batch_size, 1]),\n",
    "                user_age: np.reshape(x.take(3,1), [batch_size, 1]),\n",
    "                user_job: np.reshape(x.take(4,1), [batch_size, 1]),\n",
    "                movie_id: np.reshape(x.take(1,1), [batch_size, 1]),\n",
    "                movie_categories: categories,  #x.take(6,1)\n",
    "                movie_titles: titles,  #x.take(5,1)\n",
    "                targets: np.reshape(y, [batch_size, 1]),\n",
    "                dropout_keep_prob: 1,\n",
    "                lr: learning_rate}\n",
    "            \n",
    "            step, test_loss, summaries = sess.run([global_step, loss, inference_summary_op], feed)  #cost\n",
    "\n",
    "            #保存测试损失\n",
    "            losses['test'].append(test_loss)\n",
    "            inference_summary_writer.add_summary(summaries, step)  #\n",
    "\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            if (epoch_i * (len(test_X) // batch_size) + batch_i) % show_every_n_batches == 0:\n",
    "                print('{}: Epoch {:>3} Batch {:>4}/{}   test_loss = {:.3f}'.format(\n",
    "                    time_str,\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    (len(test_X) // batch_size),\n",
    "                    test_loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver.save(sess, save_dir)  #, global_step=epoch_i\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "bOCKZe59dU10"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.60521394, 0.61942226)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load_dir\n",
    "# !rm -rf ml-1m\n",
    "min(losses['train']), min(losses['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "I_jjFeUdRnQp",
    "outputId": "baa09280-1ad5-4212-a38a-65a8bd8211de"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses['train'], label='Training loss')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "xKssIf8nRquY",
    "outputId": "43d8d72d-90bd-4875-a670-67d2cfa9e214"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses['test'], label='Test loss')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "jZy763jFQ-dZ"
   },
   "outputs": [],
   "source": [
    "save_params((save_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jZy763jFQ-dZ"
   },
   "outputs": [],
   "source": [
    "load_dir = load_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "0P-VRGn-Rvgg"
   },
   "outputs": [],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "\n",
    "    uid = loaded_graph.get_tensor_by_name(\"uid:0\")\n",
    "    user_gender = loaded_graph.get_tensor_by_name(\"user_gender:0\")\n",
    "    user_age = loaded_graph.get_tensor_by_name(\"user_age:0\")\n",
    "    user_job = loaded_graph.get_tensor_by_name(\"user_job:0\")\n",
    "    movie_id = loaded_graph.get_tensor_by_name(\"movie_id:0\")\n",
    "    movie_categories = loaded_graph.get_tensor_by_name(\"movie_categories:0\")\n",
    "    movie_titles = loaded_graph.get_tensor_by_name(\"movie_titles:0\")\n",
    "    targets = loaded_graph.get_tensor_by_name(\"targets:0\")\n",
    "    dropout_keep_prob = loaded_graph.get_tensor_by_name(\"dropout_keep_prob:0\")\n",
    "    lr = loaded_graph.get_tensor_by_name(\"LearningRate:0\")\n",
    "    #两种不同计算预测评分的方案使用不同的name获取tensor inference\n",
    "#     inference = loaded_graph.get_tensor_by_name(\"inference/inference/BiasAdd:0\")\n",
    "    inference = loaded_graph.get_tensor_by_name(\"inference/ExpandDims:0\") # 之前是MatMul:0 因为inference代码修改了 这里也要修改 感谢网友 @清歌 指出问题\n",
    "    movie_combine_layer_flat = loaded_graph.get_tensor_by_name(\"movie_fc/Reshape:0\")\n",
    "    user_combine_layer_flat = loaded_graph.get_tensor_by_name(\"user_fc/Reshape:0\")\n",
    "    return uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, inference, movie_combine_layer_flat, user_combine_layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Vh-Xl0c-RzK3"
   },
   "outputs": [],
   "source": [
    "def rating_movie(user_id_val, movie_id_val):\n",
    "    loaded_graph = tf.Graph()  #\n",
    "    with tf.Session(graph=loaded_graph) as sess:  #\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "    \n",
    "        # Get Tensors from loaded model\n",
    "        uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, inference,_, __ = get_tensors(loaded_graph)  #loaded_graph\n",
    "    \n",
    "        categories = np.zeros([1, 18])\n",
    "        categories[0] = movies.values[movieid2idx[movie_id_val]][2]\n",
    "    \n",
    "        titles = np.zeros([1, sentences_size])\n",
    "        titles[0] = movies.values[movieid2idx[movie_id_val]][1]\n",
    "    \n",
    "        feed = {\n",
    "              uid: np.reshape(users.values[user_id_val-1][0], [1, 1]),\n",
    "              user_gender: np.reshape(users.values[user_id_val-1][1], [1, 1]),\n",
    "              user_age: np.reshape(users.values[user_id_val-1][2], [1, 1]),\n",
    "              user_job: np.reshape(users.values[user_id_val-1][3], [1, 1]),\n",
    "              movie_id: np.reshape(movies.values[movieid2idx[movie_id_val]][0], [1, 1]),\n",
    "              movie_categories: categories,  #x.take(6,1)\n",
    "              movie_titles: titles,  #x.take(5,1)\n",
    "              dropout_keep_prob: 1}\n",
    "    \n",
    "        # Get Prediction\n",
    "        inference_val = sess.run([inference], feed)  \n",
    "    \n",
    "    return (inference_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZRE6fPkLR2xP",
    "outputId": "ec3e7de8-155a-40f3-92f2-2489a5f4c6ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save2/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[3.4785843]], dtype=float32)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_movie(234, 1401)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3K-cKqlDR453",
    "outputId": "e2134d50-731e-494a-e69b-83965404c728"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save2/\n"
     ]
    }
   ],
   "source": [
    "loaded_graph = tf.Graph()  #\n",
    "movie_matrics = []\n",
    "with tf.Session(graph=loaded_graph) as sess:  #\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, _, movie_combine_layer_flat, __ = get_tensors(loaded_graph)  #loaded_graph\n",
    "\n",
    "    for item in movies.values:\n",
    "        categories = np.zeros([1, 18])\n",
    "        categories[0] = item.take(2)\n",
    "\n",
    "        titles = np.zeros([1, sentences_size])\n",
    "        titles[0] = item.take(1)\n",
    "\n",
    "        feed = {\n",
    "            movie_id: np.reshape(item.take(0), [1, 1]),\n",
    "            movie_categories: categories,  #x.take(6,1)\n",
    "            movie_titles: titles,  #x.take(5,1)\n",
    "            dropout_keep_prob: 1}\n",
    "\n",
    "        movie_combine_layer_flat_val = sess.run([movie_combine_layer_flat], feed)  \n",
    "        movie_matrics.append(movie_combine_layer_flat_val)\n",
    "\n",
    "pickle.dump((np.array(movie_matrics).reshape(-1, 200)), open('movie_matrics.p', 'wb'))\n",
    "movie_matrics = pickle.load(open('movie_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3883, 200)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_matrics = pickle.load(open('movie_matrics.p', mode='rb'))\n",
    "movie_matrics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bmsXiepvSAK_",
    "outputId": "75291bfd-abc0-431f-9138-7917b77131ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save2/\n"
     ]
    }
   ],
   "source": [
    "loaded_graph = tf.Graph()  #\n",
    "users_matrics = []\n",
    "with tf.Session(graph=loaded_graph) as sess:  #\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, _, __,user_combine_layer_flat = get_tensors(loaded_graph)  #loaded_graph\n",
    "\n",
    "    for item in users.values:\n",
    "\n",
    "        feed = {\n",
    "            uid: np.reshape(item.take(0), [1, 1]),\n",
    "            user_gender: np.reshape(item.take(1), [1, 1]),\n",
    "            user_age: np.reshape(item.take(2), [1, 1]),\n",
    "            user_job: np.reshape(item.take(3), [1, 1]),\n",
    "            dropout_keep_prob: 1}\n",
    "\n",
    "        user_combine_layer_flat_val = sess.run([user_combine_layer_flat], feed)  \n",
    "        users_matrics.append(user_combine_layer_flat_val)\n",
    "\n",
    "pickle.dump((np.array(users_matrics).reshape(-1, 200)), open('users_matrics.p', 'wb'))\n",
    "users_matrics = pickle.load(open('users_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6040, 200)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_matrics = pickle.load(open('users_matrics.p', mode='rb'))\n",
    "users_matrics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 200042/200042 [16:57<00:00, 196.60it/s]\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model():\n",
    "    loaded_graph = tf.Graph()\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        predict_rating = []\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "\n",
    "        uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, inference,_, __ = get_tensors(loaded_graph)\n",
    "\n",
    "        for i in tqdm(range(test_data.shape[0])):\n",
    "            categories = np.zeros([1, 18])\n",
    "            categories[0] = test_data[i][6]\n",
    "\n",
    "            titles = np.zeros([1, sentences_size])\n",
    "            titles[0] = test_data[i][5]\n",
    "\n",
    "            feed = {\n",
    "                uid: np.reshape(test_data[i][0], [1, 1]),\n",
    "                user_gender: np.reshape(test_data[i][2], [1, 1]),\n",
    "                user_age: np.reshape(test_data[i][3], [1, 1]),\n",
    "                user_job: np.reshape(test_data[i][4], [1, 1]),\n",
    "                movie_id: np.reshape(test_data[i][1], [1, 1]),\n",
    "                movie_categories: categories,  #x.take(6,1)\n",
    "                movie_titles: titles,  #x.take(5,1)\n",
    "                targets: np.reshape(test_targets[i], [1, 1]),\n",
    "                dropout_keep_prob: 1,\n",
    "                lr: learning_rate}\n",
    "\n",
    "            inference_val = sess.run([inference], feed)\n",
    "            predict_rating.append(inference_val)\n",
    "        return predict_rating\n",
    "    \n",
    "predict_rating = evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump((np.squeeze(np.array(predict_rating))), open('eva_ratings.p', 'wb'))\n",
    "predict_rating = pickle.load(open('eva_ratings.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 41.29%\n"
     ]
    }
   ],
   "source": [
    "predict_rating = pickle.load(open('eva_ratings.p', mode='rb'))\n",
    "print(f\"Accuracy: {round((test_targets.squeeze() == np.round(predict_rating, 0).astype(int)).sum() * 100 / predict_rating.shape[0], 2)}%\")\n",
    "# print(f\"Acc V2: {round((np.abs(test_targets.squeeze() - predict_rating) < 1).sum() * 100 / predict_rating.shape[0], 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5959795 3.5959795\n"
     ]
    }
   ],
   "source": [
    "def get_rating_matrix(users_matrics, movie_matrics):\n",
    "    rating_mat = np.matmul(users_matrics, np.transpose(movie_matrics))\n",
    "    ######################important##################################\n",
    "    usr_id, mov_id = test_data[2][0], test_data[2][1]\n",
    "    print(rating_mat[usr_id-1, movieid2idx[mov_id]], predict_rating[2])\n",
    "    return rating_mat\n",
    "\n",
    "rating_mat = get_rating_matrix(users_matrics, movie_matrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6040, 3883)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 88.42%\n",
      "Recall: 95.09%\n"
     ]
    }
   ],
   "source": [
    "def compute_precision():\n",
    "    # Only runs in test dataset\n",
    "    columns=['U_ID', 'M_ID', 'U_Gender', 'U_Age', 'U_Job', 'M_Title', 'M_Genre']\n",
    "    test_df = pd.DataFrame(test_data[:, :2], columns=['U_ID', 'M_ID'])\n",
    "    test_df['Rating'] = test_targets\n",
    "    test_df['Predict_Rating'] = predict_rating\n",
    "    \n",
    "    positive_df = test_df[test_df['Rating'] > 3]\n",
    "    precision = (positive_df['Predict_Rating'] > 3).sum() / (test_df['Predict_Rating'].round(0) > 3).sum()\n",
    "    recall = (positive_df['Predict_Rating'] > 3).sum() / positive_df.shape[0]\n",
    "    print('Precision: {:0.2f}%\\nRecall: {:0.2f}%'.format(precision*100, recall*100))\n",
    "    return precision, recall\n",
    "\n",
    "pre, rec = compute_precision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 80.85%\n",
      "Recall: 81.49%\n"
     ]
    }
   ],
   "source": [
    "def compute_precision_v2(rank=20):\n",
    "    rate_table = ratings.copy()\n",
    "    rate_table['Predict_Rating'] = rate_table.apply(lambda x: rating_mat[x.UserID-1, movieid2idx[x.MovieID]], axis=1)\n",
    "\n",
    "    cnt = 0\n",
    "    gb = rate_table.groupby('UserID')\n",
    "    for k in gb.groups.keys():\n",
    "        group = gb.get_group(k).copy()\n",
    "        group.sort_values(by=['Predict_Rating'], ascending=False, inplace=True)\n",
    "        group = group.iloc[:rank]\n",
    "        cnt += (group['ratings'] > 3).sum()\n",
    "        \n",
    "    precision = cnt / (len(users) * rank)\n",
    "    recall = cnt / (rate_table['ratings'] > 3).sum()\n",
    "    user_median_num_rate = rate_table.groupby('UserID').agg({'MovieID':'count'}).median()[0]\n",
    "    recall_rebalance = recall * user_median_num_rate / rank\n",
    "    print('Precision: {:0.2f}%\\nRecall: {:0.2f}%'.format(precision*100, recall_rebalance*100))\n",
    "    return precision, recall_rebalance, rate_table\n",
    "\n",
    "pre, rec, comb_pre_ratings = compute_precision_v2(rank=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>ratings</th>\n",
       "      <th>Predict_Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>4.420122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>3.611515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>4.466289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>4.265350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>3.997452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  MovieID  ratings  Predict_Rating\n",
       "0       1     1193        5        4.420122\n",
       "1       1      661        3        3.611515\n",
       "2       1      914        3        4.466289\n",
       "3       1     3408        4        4.265350\n",
       "4       1     2355        5        3.997452"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb_pre_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n"
     ]
    }
   ],
   "source": [
    "user_median_num_rate = comb_pre_ratings.groupby('UserID').agg({'MovieID':'count'}).median()[0]\n",
    "print(int(user_median_num_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive_ratings = ratings[ratings['ratings'] > 3].copy()\n",
    "\n",
    "# positive_ratings['Hash'] = (positive_ratings['UserID'] - 1).astype(str) + ',' + positive_ratings['MovieID'].map(movieid2idx).astype(str)\n",
    "\n",
    "# positive_ratings.head()\n",
    "\n",
    "# sorted_id = (-rating_mat).argsort(axis=1)\n",
    "# print(np.take_along_axis(rating_mat, sorted_id, axis=1))\n",
    "\n",
    "# sorted_id = sorted_id[:, :10]\n",
    "\n",
    "# cnt = 0\n",
    "# p_rating_arr = positive_ratings.Hash.values\n",
    "# for i in tqdm(range(sorted_id.shape[0])):\n",
    "#     for j in range(sorted_id.shape[1]):\n",
    "#         cor = str(i)+','+str(sorted_id[i, j])\n",
    "#         if cor in p_rating_arr:\n",
    "#             cnt += 1\n",
    "# print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "2z7lM5j0SSKn"
   },
   "outputs": [],
   "source": [
    "def recommend_same_type_movie(movie_id_val, top_k = 20):\n",
    "    \n",
    "    loaded_graph = tf.Graph()  #\n",
    "    with tf.Session(graph=loaded_graph) as sess:  #\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "        \n",
    "        norm_movie_matrics = tf.sqrt(tf.reduce_sum(tf.square(movie_matrics), 1, keep_dims=True))\n",
    "        normalized_movie_matrics = movie_matrics / norm_movie_matrics\n",
    "\n",
    "        #推荐同类型的电影\n",
    "        probs_embeddings = (movie_matrics[movieid2idx[movie_id_val]]).reshape([1, 200])\n",
    "        probs_similarity = tf.matmul(probs_embeddings, tf.transpose(normalized_movie_matrics))\n",
    "        sim = (probs_similarity.eval())\n",
    "    #     results = (-sim[0]).argsort()[0:top_k]\n",
    "    #     print(results)\n",
    "        \n",
    "        print(\"您看的电影是：{}\".format(movies_orig[movieid2idx[movie_id_val]]))\n",
    "        print(\"以下是给您的推荐：\")\n",
    "        p = np.squeeze(sim)\n",
    "        p[np.argsort(p)[:-top_k]] = 0\n",
    "        p = p / np.sum(p)\n",
    "        results = set()\n",
    "        while len(results) != 5:\n",
    "            c = np.random.choice(3883, 1, p=p)[0]\n",
    "            results.add(c)\n",
    "        for val in (results):\n",
    "            print(val)\n",
    "            print(movies_orig[val])\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8VbLBuRpSV2o",
    "outputId": "10df2820-f32e-46db-b82b-5ef4e0cf2da8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save2/\n",
      "您看的电影是：[1401 'Ghosts of Mississippi (1996)' 'Drama']\n",
      "以下是给您的推荐：\n",
      "1380\n",
      "[1401 'Ghosts of Mississippi (1996)' 'Drama']\n",
      "3796\n",
      "[3866 'Sunset Strip (2000)' 'Comedy']\n",
      "340\n",
      "[344 'Ace Ventura: Pet Detective (1994)' 'Comedy']\n",
      "1048\n",
      "[1062 'Sunchaser, The (1996)' 'Drama']\n",
      "479\n",
      "[483 'King of the Hill (1993)' 'Drama']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{340, 479, 1048, 1380, 3796}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_same_type_movie(1401, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "A2WVuM7lSXv3"
   },
   "outputs": [],
   "source": [
    "def recommend_your_favorite_movie(user_id_val, top_k = 10):\n",
    "\n",
    "    loaded_graph = tf.Graph()  #\n",
    "    with tf.Session(graph=loaded_graph) as sess:  #\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "\n",
    "        #推荐您喜欢的电影\n",
    "        probs_embeddings = (users_matrics[user_id_val-1]).reshape([1, 200])\n",
    "\n",
    "        probs_similarity = tf.matmul(probs_embeddings, tf.transpose(movie_matrics))\n",
    "        sim = (probs_similarity.eval())\n",
    "    #     print(sim.shape)\n",
    "    #     results = (-sim[0]).argsort()[0:top_k]\n",
    "    #     print(results)\n",
    "        \n",
    "    #     sim_norm = probs_norm_similarity.eval()\n",
    "    #     print((-sim_norm[0]).argsort()[0:top_k])\n",
    "    \n",
    "        print(\"以下是给您的推荐：\")\n",
    "        p = np.squeeze(sim)\n",
    "        p[np.argsort(p)[:-top_k]] = 0\n",
    "        p = p / np.sum(p)\n",
    "        results = set()\n",
    "        while len(results) != 5:\n",
    "            c = np.random.choice(3883, 1, p=p)[0]\n",
    "            results.add(c)\n",
    "        for val in (results):\n",
    "            print(val)\n",
    "            print(movies_orig[val])\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zEkK6ksMSb_Y",
    "outputId": "ae1ad720-7b77-4f87-80eb-6753a5c67d0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save2/\n",
      "以下是给您的推荐：\n",
      "257\n",
      "[260 'Star Wars: Episode IV - A New Hope (1977)'\n",
      " 'Action|Adventure|Fantasy|Sci-Fi']\n",
      "574\n",
      "[578 'Hour of the Pig, The (1993)' 'Drama|Mystery']\n",
      "1961\n",
      "[2030 'East Palace West Palace (Dong gong xi gong) (1997)' 'Drama']\n",
      "3448\n",
      "[3517 'Bells, The (1926)' 'Crime|Drama']\n",
      "1950\n",
      "[2019\n",
      " 'Seven Samurai (The Magnificent Seven) (Shichinin no samurai) (1954)'\n",
      " 'Action|Drama']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{257, 574, 1950, 1961, 3448}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_your_favorite_movie(234, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "TpXoYGVrSenn"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def recommend_other_favorite_movie(movie_id_val, top_k = 20):\n",
    "    loaded_graph = tf.Graph()  #\n",
    "    with tf.Session(graph=loaded_graph) as sess:  #\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "\n",
    "        probs_movie_embeddings = (movie_matrics[movieid2idx[movie_id_val]]).reshape([1, 200])\n",
    "        probs_user_favorite_similarity = tf.matmul(probs_movie_embeddings, tf.transpose(users_matrics))\n",
    "        favorite_user_id = np.argsort(probs_user_favorite_similarity.eval())[0][-top_k:]\n",
    "    #     print(normalized_users_matrics.eval().shape)\n",
    "    #     print(probs_user_favorite_similarity.eval()[0][favorite_user_id])\n",
    "    #     print(favorite_user_id.shape)\n",
    "    \n",
    "        print(\"您看的电影是：{}\".format(movies_orig[movieid2idx[movie_id_val]]))\n",
    "        \n",
    "        print(\"喜欢看这个电影的人是：{}\".format(users_orig[favorite_user_id-1]))\n",
    "        probs_users_embeddings = (users_matrics[favorite_user_id-1]).reshape([-1, 200])\n",
    "        probs_similarity = tf.matmul(probs_users_embeddings, tf.transpose(movie_matrics))\n",
    "        sim = (probs_similarity.eval())\n",
    "    #     results = (-sim[0]).argsort()[0:top_k]\n",
    "    #     print(results)\n",
    "    \n",
    "    #     print(sim.shape)\n",
    "    #     print(np.argmax(sim, 1))\n",
    "        p = np.argmax(sim, 1)\n",
    "        print(\"喜欢看这个电影的人还喜欢看：\")\n",
    "\n",
    "        results = set()\n",
    "        while len(results) != 3:\n",
    "            c = p[random.randrange(top_k)]\n",
    "            results.add(c)\n",
    "        for val in (results):\n",
    "            print(val)\n",
    "            print(movies_orig[val])\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fin4yZn9SiB2",
    "outputId": "9e540420-fd3d-4c5e-b28b-e15b47310656"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save2/\n",
      "您看的电影是：[1401 'Ghosts of Mississippi (1996)' 'Drama']\n",
      "喜欢看这个电影的人是：[[1958 'F' 25 1]\n",
      " [1805 'M' 18 4]\n",
      " [52 'M' 18 4]\n",
      " [2608 'F' 25 1]\n",
      " [4504 'F' 25 0]\n",
      " [4249 'F' 18 17]\n",
      " [3754 'M' 56 7]\n",
      " [1590 'M' 35 1]\n",
      " [2082 'M' 1 19]\n",
      " [1131 'M' 56 13]\n",
      " [4085 'F' 25 6]\n",
      " [5458 'F' 18 2]\n",
      " [1081 'M' 18 4]\n",
      " [5567 'M' 50 3]\n",
      " [100 'M' 35 17]\n",
      " [5669 'M' 56 1]\n",
      " [2737 'M' 25 3]\n",
      " [4800 'M' 18 4]\n",
      " [2154 'M' 25 12]\n",
      " [1763 'M' 35 7]]\n",
      "喜欢看这个电影的人还喜欢看：\n",
      "1961\n",
      "[2030 'East Palace West Palace (Dong gong xi gong) (1997)' 'Drama']\n",
      "1435\n",
      "[1462 'Unforgotten: Twenty-Five Years After Willowbrook (1996)'\n",
      " 'Documentary']\n",
      "574\n",
      "[578 'Hour of the Pig, The (1993)' 'Drama|Mystery']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{574, 1435, 1961}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_other_favorite_movie(1401, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "jW69MTZNUw42"
   },
   "outputs": [],
   "source": [
    "# bc = BertClient()\n",
    "# bc.encode(['First do it'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.Title.iloc[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = tf.convert_to_tensor(movies.Title.iloc[0], np.float32)\n",
    "# x = tf.reshape(x, [1, -1])\n",
    "# x.dtype\n",
    "\n",
    "# # a = tf.layers.dense(x, 32, name = \"movie_title_fc_layer\", activation=tf.nn.relu)\n",
    "# # a.get_shape()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MovieRec.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:tf11]",
   "language": "python",
   "name": "conda-env-tf11-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
